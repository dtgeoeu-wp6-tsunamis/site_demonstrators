{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f6fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.matlib as npm\n",
    "import utm\n",
    "# import ast\n",
    "import copy\n",
    "import math\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import distributions\n",
    "# from operator import itemgetter\n",
    "# from numba import jit\n",
    "import cartopy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import json\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"\n",
    "        Calculate the great circle distance between two points\n",
    "        on the earth (specified in decimal degrees)\n",
    "        \"\"\"\n",
    "        # convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "        # haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        r = 6371 # Radius of earth in kilometers.\n",
    "        return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6446bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_setup():\n",
    "    proj = cartopy.crs.PlateCarree()\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    ax = plt.axes(projection=cartopy.crs.Mercator())\n",
    "\n",
    "    # coastline = cartopy.feature.GSHHSFeature(scale='low', levels=[1])\n",
    "    coastline = cartopy.feature.GSHHSFeature(scale='high', levels=[1])\n",
    "    ax.add_feature(coastline, edgecolor='#000000', facecolor='#cccccc', linewidth=1)\n",
    "    ax.add_feature(cartopy.feature.BORDERS.with_scale('50m'))\n",
    "    ax.add_feature(cartopy.feature.STATES.with_scale('50m'))\n",
    "    ax.add_feature(cartopy.feature.OCEAN.with_scale('50m'))\n",
    "    gl = ax.gridlines(crs=proj, draw_labels=True, linewidth=1,\n",
    "                        color=\"#ffffff\", alpha=0.5, linestyle='-')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.bottom_labels = True\n",
    "    gl.left_labels = True\n",
    "    gl.xformatter = cartopy.mpl.gridliner.LONGITUDE_FORMATTER\n",
    "    gl.yformatter = cartopy.mpl.gridliner.LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 14}\n",
    "    gl.ylabel_style = {'size': 14}\n",
    "\n",
    "    return ax, proj\n",
    "\n",
    "\n",
    "def plot_pois(pois_coords, ind, event_dict):\n",
    "\n",
    "    ev_lon = event_dict['lon']\n",
    "    ev_lat = event_dict['lat']\n",
    "\n",
    "    ax, proj = map_setup()\n",
    "    \n",
    "    ax.plot(pois_coords[ind,0], pois_coords[ind,1], markerfacecolor=\"#0ba518\", marker=\"o\",\n",
    "                linewidth=0, markeredgecolor=\"#000000\",\n",
    "                transform=proj)#, zorder=10)\n",
    "\n",
    "    ax.plot(ev_lon, ev_lat, linewidth=0, marker='*', markersize=10,\n",
    "            markerfacecolor='red', markeredgecolor='#000000',\n",
    "            transform=proj)\n",
    "    \n",
    "    # ax.set_extent([19., 30.5, 35.5, 41.5], crs=proj)\n",
    "\n",
    "    ax.set_xlabel(r'Longitude ($^\\circ$)', fontsize=14)\n",
    "    ax.set_ylabel(r'Latitude ($^\\circ$)', fontsize=14)\n",
    "\n",
    "def plot_barycenters(bs_coords, prob, event_dict):\n",
    "\n",
    "    # fig,axs = plt.subplots(2,1,figsize=(20,20),subplot_kw={'projection': cartopy.crs.Mercator()})\n",
    "    # fig.set_tight_layout(True)\n",
    "    ev_lon = event_dict['lon']\n",
    "    ev_lat = event_dict['lat']\n",
    "\n",
    "    ax, proj = map_setup()\n",
    " \n",
    "    lonlat_list=[list(x) for x in set(tuple(x) for x in bs_coords)]\n",
    "    nloc=len(lonlat_list)\n",
    " \n",
    "    sumprob=[]\n",
    "    for iloc in range(nloc):\n",
    "        locxy=lonlat_list[iloc]\n",
    "        isel = [i for i, x in enumerate(list(bs_coords)) if np.allclose(x, locxy)]\n",
    "        # isel = [i for i, x in enumerate(list(bs_coords)) if x == locxy]\n",
    "        #print(len(isel))\n",
    "        tmplist=[prob[i] for i in isel]\n",
    "        sumprob.append(sum(tmplist))\n",
    "    lon = [el[0] for el in lonlat_list]\n",
    "    lat = [el[1] for el in lonlat_list]\n",
    " \n",
    "    # lon = bs_coords[:,0] \n",
    "    # lat = bs_coords[:,1]\n",
    "    cmap = ax.scatter(lon,lat,s=30, transform=proj,edgecolors='k',c=sumprob,cmap=plt.cm.plasma)#, label=labels[ic], c=colors[ic])\n",
    "    cbar = plt.colorbar(cmap,ax=ax,extend='both',extendfrac='auto',aspect=30,format='%.0e',pad=0.02,\n",
    "                 label='Cumulated Probability (for each barycenter)',shrink=0.8)\n",
    " \n",
    "    ax.plot(ev_lon, ev_lat, linewidth=0, marker='*', markersize=10,\n",
    "            markerfacecolor='red', markeredgecolor='#000000',\n",
    "            transform=proj)\n",
    "     \n",
    "    ax.set_extent([-80, -67.5, -38, -30], crs=proj)\n",
    "\n",
    "    cbar.locator = ticker.MaxNLocator(nbins=10)\n",
    "    cbar.update_ticks()\n",
    "\n",
    "def plot_sealevel(sealevel_dict, arrival_times=None):\n",
    "\n",
    "    nstations = len(sealevel_dict.keys())\n",
    "    ncol = 3\n",
    "    nrows = int(np.ceil(nstations / ncol))\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncol, figsize=(20, 3 * nrows)) #, sharex=True)\n",
    "    axs = axs.flatten()\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    for i, (name, inner_dict) in enumerate(sealevel_dict.items()):\n",
    "        sealevel_df = inner_dict['data']\n",
    "        sec = sealevel_df['sec'].values\n",
    "        sea_level = sealevel_df['sea_level'].values\n",
    "\n",
    "        axs[i].plot(sec, sea_level) #, label=name)\n",
    "        if arrival_times is not None:\n",
    "            axs[i].axvline(x=arrival_times[i], color='red', linestyle='--', label='Arrival Time')\n",
    "        axs[i].set_xlabel('Sec')\n",
    "        axs[i].set_ylabel('Sea Level (m)')\n",
    "        axs[i].set_title(name)\n",
    "        # axs[i].set_title('Sea Level Time Series')\n",
    "        # axs[i].legend()\n",
    "\n",
    "    # Hide empty plots if less than nrows*ncols\n",
    "    for j in range(i+1, len(axs)):\n",
    "        axs[j].set_visible(False)\n",
    "\n",
    "def plot_histogram_misfit(data):\n",
    "    \n",
    "    plt.hist(data, bins=30, alpha=0.5, edgecolor='black')\n",
    "    plt.xlabel('Misfit Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of misfit values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_pre_load.py file\n",
    "\n",
    "def load_intensity_thresholds(data_folder):\n",
    "    \"\"\"\n",
    "    READ THRESHOLDS\n",
    "    \"\"\"\n",
    "    \n",
    "    intensity_thresholds = os.path.join(data_folder, 'intensity_thresholds.npy')\n",
    "\n",
    "    ith = np.load(intensity_thresholds, allow_pickle=True)\n",
    "    intensity_measure = ith.item().keys()\n",
    "    thresholds = ith.item()[list(intensity_measure)[0]]\n",
    "\n",
    "    return thresholds, intensity_measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afcca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_scaling_laws.py file\n",
    "def scalinglaw_WC(**kwargs):\n",
    "    '''\n",
    "    Scaling law from Wells&Coppersmith (1994)\n",
    "    '''\n",
    "\n",
    "    mag        = kwargs.get('mag', None)\n",
    "    type_scala = kwargs.get('type_scala', None)\n",
    "\n",
    "    if (type_scala == 'M2L'):\n",
    "        a =-2.440\n",
    "        b =0.590\n",
    "        y = 10.**(a+b*mag)\n",
    "\n",
    "    elif (type_scala == 'M2W'):\n",
    "        a=-1.010\n",
    "        b=0.320\n",
    "        y = 10.**(a+b*mag)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Scaling law in scalinglaw_WC not recognized. Exit!\")\n",
    "        #print(\"Scaling law in scalinglaw_WC not recognized. Exit!\")\n",
    "        #sys.exit()\n",
    "\n",
    "    return y\n",
    "\n",
    "def scalinglaw_Murotani(**kwargs):\n",
    "    '''\n",
    "    Scaling law from Murotani et al (2013)\n",
    "    '''\n",
    "\n",
    "    mag        = kwargs.get('mag', None)\n",
    "    type_scala = kwargs.get('type_scala', None)\n",
    "\n",
    "    a    = -3.806\n",
    "    b    = 1.000\n",
    "    Area = 10**(a+b*mag)\n",
    "\n",
    "    if (type_scala == 'M2L'):\n",
    "        y = math.sqrt(2.5*Area)/2.5\n",
    "\n",
    "    elif (type_scala == 'M2W'):\n",
    "        y = math.sqrt(2.5*Area)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Scaling law in scalinglaw_Murotani not recognized. Exit!\")\n",
    "        #print(\"Scaling law in scalinglaw_Murotani not recognized. Exit!\")\n",
    "        #sys.exit()\n",
    "\n",
    "    return y\n",
    "\n",
    "def mag_to_l_BS(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 1000.0 * scalinglaw_WC(mag=mag, type_scala='M2L')\n",
    "\n",
    "    return out\n",
    "\n",
    "def mag_to_w_BS(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 1000.0 * scalinglaw_WC(mag=mag, type_scala='M2W')\n",
    "\n",
    "    return out\n",
    "\n",
    "def mag_to_l_PS(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 1000.0 * scalinglaw_Murotani(mag=mag, type_scala='M2W')\n",
    "\n",
    "    return out\n",
    "\n",
    "def correct_BS_horizontal_position(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 0.5 * mag_to_l_BS(mag=mag)\n",
    "\n",
    "    return out\n",
    "\n",
    "def correct_PS_horizontal_position(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 0.5 * mag_to_l_PS(mag=mag)\n",
    "\n",
    "    return out\n",
    " \n",
    "def correct_BS_vertical_position(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = math.sin(math.pi/4)*0.5 * mag_to_w_BS(mag=mag)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_mix_utilities.py file\n",
    "\n",
    "def NormMultiDvec(**kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    # Here mu and sigma, already inserted into ee dictionary\n",
    "    # Coordinates in utm\n",
    "    mu = tmpmu =PosMean_3D = [EarlyEst.lonUTM,EarlyEst.latUTM,EarlyEst.Dep*1.E3]\n",
    "    Sigma = tmpCOV = EarlyEst.PosCovMat_3D = [EarlyEst.PosSigmaXX EarlyEst.PosSigmaXY EarlyEst.PosSigmaXZ; ...\n",
    "                         EarlyEst.PosSigmaXY EarlyEst.PosSigmaYY EarlyEst.PosSigmaYZ; ...\n",
    "                         EarlyEst.PosSigmaXZ EarlyEst.PosSigmaYZ EarlyEst.PosSigmaZZ];\n",
    "    mu =     np.array([ee['lon'], ee['lat'], ee['depth']*1000.0])\n",
    "    sigma =  np.array([[ee['cov_matrix']['XX'], ee['cov_matrix']['XY'], ee['cov_matrix']['XZ']], \\\n",
    "                       [ee['cov_matrix']['XY'], ee['cov_matrix']['YY'], ee['cov_matrix']['YZ']], \\\n",
    "                       [ee['cov_matrix']['XZ'], ee['cov_matrix']['YZ'], ee['cov_matrix']['ZZ']]])\n",
    "    \"\"\"\n",
    "\n",
    "    x     = kwargs.get('x', None)\n",
    "    mu    = kwargs.get('mu', None)\n",
    "    sigma = kwargs.get('sigma', None)\n",
    "\n",
    "    n = len(mu)\n",
    "\n",
    "    #mu = np.reshape(mu,(3,1))\n",
    "    mu = np.reshape(mu,(n,1))\n",
    "    t1  = (2 * math.pi)**(-1*len(mu)/2)\n",
    "    t2  = 1 / math.sqrt(np.linalg.det(sigma))\n",
    "    #c1  = npm.repmat(mu, 1, np.shape(mu)[0])\n",
    "    c1  = npm.repmat(mu, 1, len(x))\n",
    "    c11 = (x - c1.transpose()).transpose()\n",
    "    c12 = x - c1.transpose()\n",
    "\n",
    "    d  = np.linalg.lstsq(sigma, c11, rcond=None)[0]\n",
    "    e = np.dot(c12, d)\n",
    "    f = np.multiply(-0.5,np.diag(e))\n",
    "    g = np.exp(f)\n",
    "    h = t1 * t2 * g\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3634bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_load_event.py file\n",
    "def int_quake_cat2dict(json_object):\n",
    "\n",
    "    d = dict()\n",
    "\n",
    "    # Event Ids\n",
    "    try:\n",
    "        origin_id =  str(json_object['features'][0]['properties']['originid'])\n",
    "    except:\n",
    "        origin_id =  str(json_object['features'][0]['properties']['originId'])\n",
    "\n",
    "    event_id      =  str(json_object['features'][0]['properties']['eventId'])\n",
    "    # author        =  str(json_object['features'][0]['properties']['author'])\n",
    "    # version       =  str(json_object['features'][0]['properties']['version'])\n",
    "\n",
    "    # Epicenter informations\n",
    "    lon           =  float(json_object['features'][0]['geometry']['coordinates'][0])\n",
    "    lat           =  float(json_object['features'][0]['geometry']['coordinates'][1])\n",
    "    depth         =  float(json_object['features'][0]['geometry']['coordinates'][2])\n",
    "    OT            =  str(json_object['features'][0]['properties']['time'])\n",
    "    ev_type       =  str(json_object['features'][0]['properties']['type'])\n",
    "    mag_type      =  str(json_object['features'][0]['properties']['magType'])\n",
    "    place          =  str(json_object['features'][0]['properties']['place'])\n",
    "\n",
    "    #utm conversion\n",
    "    ee_utm = utm.from_latlon(lat, lon)\n",
    "\n",
    "    # Specific Mag percentiles and covariant cov_matrix\n",
    "    mag_percentiles = json_object['features'][0]['properties']['mag_percentiles']\n",
    "    cov_matrix      = json_object['features'][0]['properties']['cov_matrix']\n",
    "    \n",
    "    pos_Sigma = cov_matrix.copy() #json_string['features'][0]['properties']['cov_matrix']\n",
    "\n",
    "    cov_matrix['XX'] = float(cov_matrix['XX'])\n",
    "    cov_matrix['XY'] = float(cov_matrix['XY'])\n",
    "    cov_matrix['XZ'] = float(cov_matrix['XZ'])\n",
    "    cov_matrix['YY'] = float(cov_matrix['YY'])\n",
    "    cov_matrix['YZ'] = float(cov_matrix['YZ'])\n",
    "    cov_matrix['ZZ'] = float(cov_matrix['ZZ'])\n",
    "\n",
    "    pos_Sigma['XX']  = float(pos_Sigma['XX']) * 1e6\n",
    "    pos_Sigma['XY']  = float(pos_Sigma['XY']) * 1e6\n",
    "    pos_Sigma['XZ']  = float(pos_Sigma['XZ']) * 1e6\n",
    "    pos_Sigma['YY']  = float(pos_Sigma['YY']) * 1e6\n",
    "    pos_Sigma['YZ']  = float(pos_Sigma['YZ']) * 1e6\n",
    "    pos_Sigma['ZZ']  = float(pos_Sigma['ZZ']) * 1e6\n",
    "\n",
    "    mag_percentiles['p16']  = float(mag_percentiles['p16'])\n",
    "    mag_percentiles['p50']  = float(mag_percentiles['p50'])\n",
    "    mag_percentiles['p84']  = float(mag_percentiles['p84'])\n",
    "    mag_sigma = 0.5 * (mag_percentiles['p84'] - mag_percentiles['p16'])\n",
    "\n",
    "    d['eventid']        = event_id\n",
    "    d['originid']       = origin_id\n",
    "    d['lat']            = lat\n",
    "    d['lon']            = lon\n",
    "    d['depth']          = depth\n",
    "    d['ot']             = OT\n",
    "    d['mag']            = mag_percentiles['p50']\n",
    "    d['mag_percentiles'] = mag_percentiles\n",
    "    d['MagSigma']        = mag_sigma\n",
    "    d['type']           = ev_type\n",
    "    d['mag_type']       = mag_type\n",
    "    d['ee_utm']         = ee_utm\n",
    "    d['place']          = place\n",
    "\n",
    "    # d['version']        = \"%03d\" % (float(version))\n",
    "    # d['author']         = author\n",
    "    # d['area']           = area\n",
    "    # d['area_geo']       = area_geo\n",
    "    # d['mag']            = mag\n",
    "    # d['mag_values']     = mag_values\n",
    "    # d['mag_counts']     = mag_counts\n",
    "    # d['ct']             = creation_time\n",
    "    # d['ot_year']        = origin_year\n",
    "    # d['ot_month']       = origin_month\n",
    "    # d['ot_day']         = origin_day\n",
    "\n",
    "    d['cov_matrix']      = cov_matrix\n",
    "    d['pos_Sigma']       = pos_Sigma\n",
    "\n",
    "    d['ee_PosCovMat_2d'] = np.array([[cov_matrix['XX'], cov_matrix['XY']], \\\n",
    "                                     [cov_matrix['XY'], cov_matrix['YY']]])\n",
    "    d['PosMean_2d']      = np.array([d['ee_utm'][0], \\\n",
    "                                     d['ee_utm'][1]])\n",
    "    d['PosCovMat_3d']    = np.array([[cov_matrix['XX'], cov_matrix['XY'], cov_matrix['XZ']], \\\n",
    "                                     [cov_matrix['XY'], cov_matrix['YY'], cov_matrix['YZ']], \\\n",
    "                                     [cov_matrix['XZ'], cov_matrix['YZ'], cov_matrix['ZZ']]])\n",
    "    d['PosCovMat_3dm']    = d['PosCovMat_3d']*1000000\n",
    "    d['PosMean_3d']      = np.array([d['ee_utm'][0], \\\n",
    "                                     d['ee_utm'][1], \\\n",
    "                                     d['depth'] * 1000.0])\n",
    "\n",
    "    # d['root_name']       = str(d['ot_year']) + str(d['ot_month']) + str(d['ot_day']) + '_' + \\\n",
    "    #                        d['area']\n",
    "    #                        #str(d['eventid']) + '_' + str(d['version']) + '_' + d['area']\n",
    "\n",
    "    return d\n",
    "\n",
    "def compute_position_sigma_lat_lon(event_parameters):\n",
    "    \"\"\"\n",
    "    REFERENCE LAT = YY\n",
    "    REFERENCE LON = XX\n",
    "    \"\"\"\n",
    "\n",
    "    bs_mag_max          = 8.1\n",
    "\n",
    "    sigma               = event_parameters['sigma']\n",
    "    event_mag           = event_parameters['mag_percentiles']['p50']\n",
    "    event_mag_max       = event_parameters['mag_percentiles']['p50'] + \\\n",
    "                          event_parameters['MagSigma'] * sigma\n",
    "    event_mag_sigma     = event_parameters['MagSigma']\n",
    "\n",
    "    event_cov_xx        = event_parameters['pos_Sigma']['XX']\n",
    "    event_cov_xy        = event_parameters['pos_Sigma']['XY']\n",
    "    event_cov_yy        = event_parameters['pos_Sigma']['YY']\n",
    "\n",
    "\n",
    "    mag_to_correct      = min(bs_mag_max, event_mag_max)\n",
    "\n",
    "    delta_position_BS_h = correct_BS_horizontal_position(mag=mag_to_correct)\n",
    "    delta_position_PS_h = correct_PS_horizontal_position(mag=event_mag + sigma * event_mag_sigma)\n",
    "\n",
    "    position_BS_sigma_yy  = math.sqrt(abs(event_cov_yy)) + delta_position_BS_h\n",
    "    position_BS_sigma_xx  = math.sqrt(abs(event_cov_xx)) + delta_position_BS_h\n",
    "    position_BS_sigma_xy  = math.sqrt(abs(event_cov_xy)) + delta_position_BS_h\n",
    "\n",
    "    event_parameters['position_BS_sigma_yy'] = position_BS_sigma_yy\n",
    "    event_parameters['position_BS_sigma_xx'] = position_BS_sigma_xx\n",
    "    event_parameters['position_BS_sigma_xy'] = position_BS_sigma_xy\n",
    "\n",
    "    position_PS_sigma_yy  = math.sqrt(abs(event_cov_yy)) + delta_position_PS_h\n",
    "    position_PS_sigma_xx  = math.sqrt(abs(event_cov_xx)) + delta_position_PS_h\n",
    "    position_PS_sigma_xy  = math.sqrt(abs(event_cov_xy)) + delta_position_PS_h\n",
    "\n",
    "    event_parameters['position_PS_sigma_yy'] = position_PS_sigma_yy\n",
    "    event_parameters['position_PS_sigma_xx'] = position_PS_sigma_xx\n",
    "    event_parameters['position_PS_sigma_xy'] = position_PS_sigma_xy\n",
    "\n",
    "    return event_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_define_ensemble_global.py file\n",
    "def select_magnitude(event_parameters, magnitude_values, sigma):\n",
    "    \"\"\"\n",
    "    Selecting magnitude values to create the ensemble.\n",
    "    If magnitude distribution is provided in json file then the values \n",
    "    in the key [mag_values] are used. \n",
    "    Otherwise, magnitude ranges are defined from the uncertainty provided \n",
    "    in the key parameter [mag_percentiles]\n",
    "    \n",
    "    \"\"\"\n",
    "    print('--> Selecting magnitude values')\n",
    "\n",
    "    min_mag  = event_parameters['mag'] - event_parameters['MagSigma'] * sigma\n",
    "    max_mag  = event_parameters['mag'] + event_parameters['MagSigma'] * sigma\n",
    "\n",
    "    magnitudes = magnitude_values[(magnitude_values >= min_mag) & (magnitude_values <= max_mag)]\n",
    "    idx = np.where((magnitude_values >= min_mag) & (magnitude_values <= max_mag))\n",
    "    if magnitudes.size == 0:\n",
    "        idx = np.array((np.abs(magnitude_values-max_mag)).argmin())\n",
    "        magnitudes = np.array([magnitude_values[idx]])\n",
    "\n",
    "    print('    Number of magnitude values = ' + str(len(magnitudes)))\n",
    "    print('    Magnitudes selected: ' + str(magnitudes))\n",
    "\n",
    "    return magnitudes, idx\n",
    "\n",
    "def discretized_position(ellipse, step):\n",
    "\n",
    "    print('--> Discretizing positions')\n",
    "\n",
    "    utm_x_min = np.amin(ellipse[:,1])\n",
    "    utm_x_max = np.amax(ellipse[:,1])\n",
    "    utm_y_min = np.amin(ellipse[:,0])\n",
    "    utm_y_max = np.amax(ellipse[:,0])\n",
    "    utm_x = np.arange(utm_x_min, utm_x_max + step, step)\n",
    "    utm_y = np.arange(utm_y_min, utm_y_max + step, step)\n",
    "    \n",
    "    print('    Number of points along x= ' + str(len(utm_x)))\n",
    "    print('    Number of points along y= ' + str(len(utm_y)))\n",
    "\n",
    "    return utm_x, utm_y\n",
    "\n",
    "def discretized_depth(z, step):\n",
    "\n",
    "    print('--> Discretizing depths')\n",
    "\n",
    "    min_depth = np.amin(z)\n",
    "    max_depth = np.amax(z)\n",
    "\n",
    "    depth = np.arange(min_depth, max_depth + step, step)\n",
    "    print('    Number of points along z= ' + str(len(depth)))\n",
    "    \n",
    "    return depth\n",
    "\n",
    "def create_grid3d(event_parameters, ensemble): \n",
    "    \"\"\"\n",
    "    Creation of grid 3d of position in utm (x, y, z) and conversion to geographic coordinates (lon, lat).\n",
    "    In ensemble['grid_3d'] points are ordered as follows: x1,y1,z1; x1,y2,z1; ...; x2,y1,z1;...\n",
    "    \"\"\"\n",
    "\n",
    "    print('--> Creating grid 3d')\n",
    "\n",
    "    zone_number = event_parameters['ee_utm'][2]\n",
    "    zone_letter = event_parameters['ee_utm'][3]\n",
    "\n",
    "    position_x = ensemble['position_utm_x']\n",
    "    position_y = ensemble['position_utm_y']\n",
    "    depth      = ensemble['depth']\n",
    "\n",
    "    xx_3d, yy_3d, zz_3d = np.meshgrid(position_x, position_y, depth, indexing='xy')\n",
    "    xx_3d               = xx_3d.flatten('F')\n",
    "    yy_3d               = yy_3d.flatten('F')\n",
    "    zz_3d               = zz_3d.flatten('F')\n",
    "    grid_3d             = np.array([xx_3d, yy_3d, zz_3d]).transpose()\n",
    "\n",
    "    # geo_coord = np.array(utm.to_latlon(grid_3d[:,0], grid_3d[:,1], zone_number, zone_letter))\n",
    "    easting = copy.deepcopy(grid_3d[:,0])\n",
    "    northing = copy.deepcopy(grid_3d[:,1])\n",
    "    geo_coord = np.array(utm.to_latlon(easting, northing, zone_number, zone_letter))\n",
    "\n",
    "    # print(geo_coord.shape, grid_3d.shape)\n",
    "    print('    Number of grid points = ' + str(grid_3d.shape[0]))\n",
    "    \n",
    "    return grid_3d, geo_coord[1,:], geo_coord[0,:]\n",
    "\n",
    "def discretized_mechanism(fm, stk_step, stk_sigma, dip_step, dip_sigma, rake_step, rake_sigma):\n",
    "    \n",
    "    print('--> Discretizing strike, dip, and rake values')\n",
    "\n",
    "    #discretizing strike angle\n",
    "    stk_tmp = fm['np1']['strike']\n",
    "    stk1 = np.arange(stk_tmp - stk_sigma, stk_tmp + stk_sigma + 1, stk_step)\n",
    "    ind1 = np.argwhere(stk1 < 0)\n",
    "    stk1[ind1] = 180. - stk1[ind1]\n",
    "    stk_tmp = fm['np2']['strike']\n",
    "    stk2 = np.arange(stk_tmp - stk_sigma, stk_tmp + stk_sigma + 1, stk_step)\n",
    "    ind2 = np.argwhere(stk2 < 0)\n",
    "    stk2[ind2] = 180. - stk2[ind2]\n",
    "\n",
    "    #discretizing dip angle\n",
    "    #TODO BE CAREFUL TO NEGATIVE VALUES OF DIP\n",
    "    dip_tmp = fm['np1']['dip']\n",
    "    dip1 = np.arange(dip_tmp - dip_sigma, dip_tmp + dip_sigma + 1, dip_step)\n",
    "    dip_tmp = fm['np2']['dip']\n",
    "    dip2 = np.arange(dip_tmp - dip_sigma, dip_tmp + dip_sigma + 1, dip_step)\n",
    "\n",
    "    #discretizing rake angle\n",
    "    rake_tmp = fm['np1']['rake']\n",
    "    rake1 = np.arange(rake_tmp - rake_sigma, rake_tmp + rake_sigma + 1, rake_step)\n",
    "    rake_tmp = fm['np2']['rake']\n",
    "    rake2 = np.arange(rake_tmp - rake_sigma, rake_tmp + rake_sigma + 1, rake_step)\n",
    "\n",
    "    fm1 = np.array(np.meshgrid(stk1, dip1, rake1)).T.reshape(-1,3)\n",
    "    fm2 = np.array(np.meshgrid(stk2, dip2, rake2)).T.reshape(-1,3)\n",
    "    focal_mechanism = np.concatenate((fm1, fm2))\n",
    "    print('    Number of angle combinations = ' + str(focal_mechanism.shape[0]))\n",
    "\n",
    "    return focal_mechanism\n",
    "\n",
    "def compute_fault_size(magnitude):\n",
    "    \"\"\"\n",
    "    Fault dimensions (length, width and area) are converted from km to m.\n",
    "    \"\"\"\n",
    "    #TODO understand which scaling law to use in global\n",
    "\n",
    "    print('--> Computing fault size')\n",
    "\n",
    "    length = scalinglaw_WC(mag=magnitude, type_scala='M2L')\n",
    "    width = scalinglaw_WC(mag=magnitude, type_scala='M2W')\n",
    "    area = length * width\n",
    "    fault_size = np.vstack((length*1.e3, width*1.e3, area*1.e6)).T\n",
    "   \n",
    "    return fault_size\n",
    "\n",
    "def compute_slip(magnitude, fault_size, mu):\n",
    "    \"\"\"\n",
    "    Calculation of slip from magnitude by scalar seismic moment.\n",
    "\n",
    "    Scalar seismic moment: M0 = 10**(1.5*(magnitudo+10.7) \n",
    "                           Kanamori formula 1977 in dyne⋅cm (10−7 N⋅m)\n",
    "    Slip on fault        : D(m) = M0(Pa*m3) / area(m2)*mu(Pa)\n",
    "\n",
    "    \"\"\"\n",
    "    print('--> Computing slip')\n",
    "\n",
    "    area = fault_size[:,0]*fault_size[:,1]\n",
    "    scalar_moment = 10.**(1.5*(magnitude+10.7)) * 1.e-7\n",
    "    \n",
    "    return scalar_moment/(area*mu)\n",
    "\n",
    "def compute_mag_probability(idx, event_parameters, mag_discretization):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    ev_mag_sigma = event_parameters['MagSigma']\n",
    "    ev_mag       = event_parameters['mag']\n",
    "\n",
    "    print('--> Computing magnitude cumulative distribution')\n",
    "\n",
    "    a = mag_discretization[0:-1]\n",
    "    b = mag_discretization[1:]\n",
    "    c = np.add(a, b) * 0.5\n",
    "\n",
    "    lower = np.insert(c, 0, -np.inf)\n",
    "    upper = np.insert(c, c.size, np.inf)\n",
    "\n",
    "    lower_probility_norm  = norm.cdf(lower, ev_mag, ev_mag_sigma)\n",
    "    upper_probility_norm  = norm.cdf(upper, ev_mag, ev_mag_sigma)\n",
    "\n",
    "    magnitude_probability = np.subtract(upper_probility_norm, lower_probility_norm)\n",
    "\n",
    "    magnitude_probability = magnitude_probability[idx]\n",
    "\n",
    "    return magnitude_probability\n",
    "\n",
    "def compute_pos_probability(event_parameters, ensemble):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    print('--> Computing position probabilities')\n",
    "\n",
    "    magnitudes = ensemble['magnitude']\n",
    "    grid_3d    = ensemble['grid_3d']\n",
    "\n",
    "    mu = event_parameters['PosMean_3d']\n",
    "\n",
    "    n_points = grid_3d.shape[0]\n",
    "    n_mag = len(magnitudes)\n",
    "    position_probability = np.zeros((n_mag, n_points))\n",
    "    for imag,mag in enumerate(magnitudes):\n",
    "        # Compute vertical half_width with respect the magnitude\n",
    "        v_hwidth = correct_BS_vertical_position(mag = mag)\n",
    "        h_hwidth = correct_BS_horizontal_position(mag = mag)\n",
    "\n",
    "        co = copy.deepcopy(event_parameters['PosCovMat_3dm'])\n",
    "        # Correct  Covariance matrix\n",
    "        co[0,0] = co[0,0] + h_hwidth**2\n",
    "        co[1,1] = co[1,1] + h_hwidth**2\n",
    "        co[2,2] = co[2,2] + v_hwidth**2\n",
    "        tmp_prob_points = NormMultiDvec(x = grid_3d, mu = mu, sigma = co)\n",
    "        normfact = np.sum(tmp_prob_points)\n",
    "        #TODO normalization for each mag (or normalize once outside of the loop?)\n",
    "        position_probability[imag] = tmp_prob_points / normfact   \n",
    "   \n",
    "    return position_probability\n",
    "\n",
    "def compute_mech_probability(ensemble):\n",
    "\n",
    "    print('--> Computing focal mechanism probabilities')\n",
    "    \n",
    "    # TODO EQUIPROBABILITY FOR ALL MECHANISMS\n",
    "    n_scenarios, _ = ensemble['focal_mechanism'].shape\n",
    "    mechanism_probabilities = np.ones((n_scenarios))/n_scenarios              \n",
    "\n",
    "    return mechanism_probabilities\n",
    "\n",
    "def scenarios_parameters_and_probabilities(ensemble_probs, ensemble):\n",
    "    \"\"\"\n",
    "    ordered list of scenarios parameters for t-hysea simulations:\n",
    "       index, mag, lon, lat, depth, strike, dip, rake, length, area, slip\n",
    "       nparams: number of parameters (assigned manually as 11)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print('--> Computing the total probability for each scenario')\n",
    "\n",
    "    n_mag = len(ensemble['magnitude'])\n",
    "    n_points = ensemble['grid_3d'].shape[0]\n",
    "    n_foc_mech = ensemble['focal_mechanism'].shape[0]\n",
    "    #print(n_mag, n_points, n_foc_mech)\n",
    "    nscen = n_mag * n_points * n_foc_mech\n",
    "    print('Total number of scenarios = ' + str(nscen))\n",
    "\n",
    "    nparams = 12\n",
    "    scen_params = np.zeros((nscen, nparams))\n",
    "    scen_probs = np.zeros((nscen,3))\n",
    "    \n",
    "    iscen = 0\n",
    "    for imag in range(n_mag):\n",
    "        for ipoint in range(n_points):\n",
    "            for ifoc in range(n_foc_mech):\n",
    "                scen_params[iscen,0] = iscen + 1\n",
    "                scen_params[iscen,1] = 9999\n",
    "                scen_params[iscen,2] = ensemble['magnitude'][imag]\n",
    "                scen_params[iscen,3] = ensemble['position_geo_lon'][ipoint]\n",
    "                scen_params[iscen,4] = ensemble['position_geo_lat'][ipoint]\n",
    "                scen_params[iscen,5] = ensemble['grid_3d'][ipoint,2] / 1.e3    # m --> km\n",
    "                scen_params[iscen,6] = ensemble['focal_mechanism'][ifoc,0]\n",
    "                scen_params[iscen,7] = ensemble['focal_mechanism'][ifoc,1]\n",
    "                scen_params[iscen,8] = ensemble['focal_mechanism'][ifoc,2]\n",
    "                scen_params[iscen,9] = ensemble['fault_size'][imag,0] / 1.e3   # m --> km\n",
    "                scen_params[iscen,10] = ensemble['fault_size'][imag,2] / 1.e6  # m2 --> km2\n",
    "                scen_params[iscen,11] = ensemble['slip'][imag]\n",
    "\n",
    "                scen_probs[iscen,0] = ensemble_probs['magnitude'][imag]\n",
    "                scen_probs[iscen,1] = ensemble_probs['position'][imag,ipoint]\n",
    "                scen_probs[iscen,2] = ensemble_probs['focal_mechanism'][ifoc]\n",
    "\n",
    "                iscen += 1\n",
    "\n",
    "    scenario_parameters = scen_params\n",
    "    scenario_probability_pre_norm = scen_probs.prod(axis=1)\n",
    "\n",
    "    # print('--> Normalizing scenario probabilities')\n",
    "    #both prob_mag and prob_pos are normalized, normfact=1, maybe not needed!\n",
    "    normfact = np.sum(scenario_probability_pre_norm) \n",
    "    scenario_probability = scenario_probability_pre_norm / normfact\n",
    "    \n",
    "    # print(np.sort(scenario_probability)) \n",
    "    \n",
    "    return scenario_parameters, scenario_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_ellipsoid.py file\n",
    "\n",
    "def build_location_ellipsoid_objects(**kwargs):\n",
    "    \"\"\"\n",
    "    From ellipsedata.m\n",
    "    % Copyright (c) 2014, Hugo Gabriel Eyherabide, Department of Mathematics\n",
    "    % and Statistics, Department of Computer Science and Helsinki Institute\n",
    "    % for Information Technology, University of Helsinki, Finland.\n",
    "    % All rights reserved.\n",
    "\n",
    "    !!!! Difference with the original matlab function !!!!\n",
    "    sigma in this python function is a float\n",
    "    sigma in matlab is a vector\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ee                 = kwargs.get('event', 'None')\n",
    "    sigma              = kwargs.get('sigma', 'None')\n",
    "    seismicity_type    = kwargs.get('seismicity_type', 'None')\n",
    "   \n",
    "    # Number of points to set the 2d ellipse\n",
    "    nr_points = 1000\n",
    "    \n",
    "    sigma = float(sigma)\n",
    "\n",
    "    # 2d Covariant matrix, eigenvalues and eignevectors\n",
    "    if(seismicity_type == 'BS'):\n",
    "       cov_matrix   = np.array([ [ee['position_BS_sigma_yy']**2, 0], [0, ee['position_BS_sigma_xx']**2] ])\n",
    "    elif(seismicity_type == 'PS'):\n",
    "       cov_matrix   = np.array([ [ee['position_PS_sigma_yy']**2, 0], [0, ee['position_PS_sigma_xx']**2] ])\n",
    "    else:\n",
    "       raise Exception('No seismicity type found. Exit')\n",
    "       #sys.exit('No seismicity type found. Exit')\n",
    "\n",
    "    # Center of the ellipse\n",
    "    center = (ee['ee_utm'][1],ee['ee_utm'][0])\n",
    "\n",
    "    PV, PD = np.linalg.eigh(cov_matrix)\n",
    "    PV = np.sqrt(np.diag(PV))\n",
    "\n",
    "    # Build points of ellipse\n",
    "    theta = np.linspace(0,2*np.pi,nr_points)\n",
    "    elpt  = np.dot(np.transpose(np.array([np.cos(theta), np.sin(theta)])) , PV)\n",
    "    elpt  = np.dot(elpt, np.transpose(PD))\n",
    "\n",
    "    # Add uncertainty\n",
    "    elpt = elpt * sigma\n",
    "\n",
    "    # shift to the center\n",
    "    elpt    = np.transpose(elpt)\n",
    "    elpt[0] = elpt[0] + center[0]\n",
    "    elpt[1] = elpt[1] + center[1]\n",
    "    elpt    = np.transpose(elpt)\n",
    "\n",
    "    return elpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9771e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_lambda_bsps_load.py\n",
    "def load_lambda_BSPS(sigma, ee_d):\n",
    "\n",
    "    d = dict()\n",
    "\n",
    "    ## Variables for lambda\n",
    "    d['lambdaBSPS']                                = {}\n",
    "    d['lambdaBSPS']['hypo_utm']                    = np.array([ee_d['ee_utm'][0], \\\n",
    "                                                               ee_d['ee_utm'][1], \\\n",
    "                                                               ee_d['depth'] ])\n",
    "    d['lambdaBSPS']['utmzone_hypo']                = ee_d['ee_utm'][2]\n",
    "    d['lambdaBSPS']['NormCov']                     = ee_d['PosCovMat_3d']\n",
    "    d['lambdaBSPS']['confid_lev']                  = norm.cdf(sigma) - norm.cdf(-1 * sigma)\n",
    "    d['lambdaBSPS']['dchi2']                       = distributions.chi2.ppf(d['lambdaBSPS']['confid_lev'], 3)\n",
    "    d['lambdaBSPS']['SD']                          = math.sqrt(d['lambdaBSPS']['dchi2'])\n",
    "    # d['lambdaBSPS']['mesh']                        = get_meshes(ee_d, data_folder)\n",
    "    d['lambdaBSPS']['covariance_epicenter_volume'] = get_cov_volume(ee_d['PosCovMat_3d'], d['lambdaBSPS']['SD'])\n",
    "    d['lambdaBSPS']['npts_mw']                     = get_npts_mw(d['lambdaBSPS']['covariance_epicenter_volume'])\n",
    "    d['lambdaBSPS']['gaussian_ellipsoid']          = get_gaussian_ellipsoid_3d(ee_d, ee_d['PosCovMat_3d'], d['lambdaBSPS']['SD'], d['lambdaBSPS']['npts_mw'])\n",
    "    d['lambdaBSPS']                                = get_gaussian_ellipsoid_tetraedons(d['lambdaBSPS'], ee_d)\n",
    "\n",
    "    return d['lambdaBSPS']\n",
    "\n",
    "def get_cov_volume(cov_matrix, std):\n",
    "\n",
    "    w, v = np.linalg.eig(cov_matrix)\n",
    "    l_major = std*np.sqrt(w[0]) * 1000.0\n",
    "    l_inter = std*np.sqrt(w[1]) * 1000.0\n",
    "    l_minor = std*np.sqrt(w[2]) * 1000.0\n",
    "\n",
    "    volume = (4./3.) * np.pi * l_major * l_inter * l_minor\n",
    "\n",
    "    return volume\n",
    "\n",
    "def get_npts_mw(volume):\n",
    "    \"\"\"\n",
    "    Calculate the number of points to define de ellipsoide.\n",
    "    Fitting function a*x**b found by F.Romano\n",
    "    \"\"\"\n",
    "\n",
    "    a         = 0.6211\n",
    "    b         = 0.4358\n",
    "    n_tetra   = 23382 \n",
    "    vol_tetra = 3.9990e+15\n",
    "\n",
    "    npts_mw   = np.ceil(a*(volume*n_tetra/vol_tetra)**b).astype(int)\n",
    "    npts_mw   = max(10, npts_mw)\n",
    "\n",
    "    return npts_mw\n",
    "\n",
    "def get_gaussian_ellipsoid_3d(ee, cov, std, npts): \n",
    "\n",
    "    center = [ee['ee_utm'][0], ee['ee_utm'][1], ee['depth']*-1000.0]\n",
    "\n",
    "    cov = cov*1e6\n",
    "    w, v = np.linalg.eigh(cov)\n",
    "    if np.any(w < 0):\n",
    "        print('Warning: negative eigenvalues')\n",
    "        w = max(w,0)\n",
    "    w = std * np.sqrt(w)    #get std of the cov matrix\n",
    "\n",
    "    volume = (4./3.) * np.pi * w[0] * w[1] * w[2]\n",
    "\n",
    "    # Make 3x 11x11 arrays\n",
    "    x, y, z = create_sphere(npts)\n",
    "\n",
    "    x = np.transpose(x)\n",
    "    y = np.transpose(y)\n",
    "    z = np.transpose(z)\n",
    "\n",
    "    # Flattern 11x11 array\n",
    "    ap = np.array([np.ravel(x), np.ravel(y), np.ravel(z)])\n",
    "\n",
    "    bp = np.dot(np.dot(v, np.diag(w)), ap) +  \\\n",
    "         np.transpose(np.tile(center, (np.shape(ap)[1], 1)))\n",
    "\n",
    "    xp = np.reshape(bp[0, :], np.shape(x))\n",
    "    yp = np.reshape(bp[1, :], np.shape(y))\n",
    "    zp = np.reshape(bp[2, :], np.shape(z))\n",
    "\n",
    "    ellipsoid = {'xp':xp, 'yp':yp, 'zp':zp, 'vol': volume}\n",
    "    print(\" --> Volume of the Gaussian Ellipsoid: {:.8e} [m^3]\".format(volume))\n",
    "\n",
    "    return ellipsoid\n",
    "\n",
    "def create_sphere(n_points=None, radius=None):\n",
    "    \"\"\"\n",
    "    Create a discrete 3D spheric surface (points)\n",
    "    Reference to create the shere:\n",
    "       https://it.mathworks.com/matlabcentral/answers/48240-surface-of-a-equation:\n",
    "       n = 100;\n",
    "        r = 1.5;\n",
    "        theta = (-n:2:n)/n*pi;\n",
    "        phi = (-n:2:n)'/n*pi/2;\n",
    "        cosphi = cos(phi); cosphi(1) = 0; cosphi(n+1) = 0;\n",
    "        sintheta = sin(theta); sintheta(1) = 0; sintheta(n+1) = 0;\n",
    "        x = r*cosphi*cos(theta);\n",
    "        y = r*cosphi*sintheta;\n",
    "        z = r*sin(phi)*ones(1,n+1);\n",
    "        surf(x,y,z)\n",
    "        xlabel('X'); ylabel('Y'); zlabel('Z')\n",
    "    \"\"\"\n",
    "    if radius is None:\n",
    "        radius = 1.0\n",
    "\n",
    "    if n_points is None:\n",
    "        n_points = 20\n",
    "\n",
    "    theta = np.matrix(np.arange(-1*n_points,n_points+1,2) / n_points * np.pi)\n",
    "    phi   = np.matrix(np.arange(-1*n_points,n_points+1,2) / n_points * np.pi / 2)\n",
    "    phi   = phi.transpose()\n",
    "\n",
    "    X = radius*np.matmul(np.cos(phi),np.cos(theta))\n",
    "    Y = radius*np.matmul(np.cos(phi),np.sin(theta))\n",
    "    Z = radius*np.matmul(np.sin(phi),np.matrix(np.ones(11)))\n",
    "\n",
    "    # Set to 0 the very small numbers\n",
    "    X[0] = 0\n",
    "    X[-1] = 0\n",
    "    Y[0] = 0\n",
    "    Y[-1] = 0\n",
    "    Y[:,0] = 0\n",
    "    Y[:,-1] = 0\n",
    "\n",
    "    return X,Y,Z\n",
    "\n",
    "def get_gaussian_ellipsoid_tetraedons(el, ee):\n",
    "    \"\"\"\n",
    "    From R. Tonini\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    xp = el['gaussian_ellipsoid']['xp']\n",
    "    yp = el['gaussian_ellipsoid']['yp']\n",
    "    zp = el['gaussian_ellipsoid']['zp'] #*-1.0\n",
    "\n",
    "    # array preparation for creating tetrahedrons\n",
    "    # This is like sss on matptf\n",
    "    sss = np.vstack([xp.flatten(), yp.flatten(), zp.flatten()]).transpose()\n",
    "    points_xyz = np.unique(sss, axis=0)\n",
    "    n_points, tmp = np.shape(points_xyz)\n",
    "\n",
    "    # lon lat conversion\n",
    "    points_ll = np.zeros((n_points, 3))\n",
    "    for i in range(n_points):\n",
    "        points_ll[i, [1, 0]] = utm.to_latlon(points_xyz[i, 0],\n",
    "                                             points_xyz[i, 1],\n",
    "                                             ee['ee_utm'][2], ee['ee_utm'][3])\n",
    "\n",
    "    points_ll[:, 2] = points_xyz[:, 2]\n",
    "\n",
    "    # Good but slower (0.0030319690704345703 <=> 0.0013072490692138672)\n",
    "    # from pyhull.delaunay import DelaunayTri\n",
    "    # tetrahedrons = np.asarray(DelaunayTri(points_ll).vertices)\n",
    "    # tetrahedron discretization (based on the points on the surface)\n",
    "    tessellation = scipy.spatial.Delaunay(points_ll)\n",
    "    tetrahedrons = tessellation.simplices\n",
    "\n",
    "    # computing barycenters\n",
    "    tetra_bar          = {}\n",
    "    tetra_bar[\"utm_x\"] = np.mean(points_xyz[tetrahedrons, 0], axis=1)\n",
    "    tetra_bar[\"utm_y\"] = np.mean(points_xyz[tetrahedrons, 1], axis=1)\n",
    "    tetra_bar[\"lon\"]   = np.mean(points_ll[tetrahedrons, 0], axis=1)\n",
    "    tetra_bar[\"lat\"]   = np.mean(points_ll[tetrahedrons, 1], axis=1)\n",
    "    tetra_bar[\"depth\"] = np.mean(points_ll[tetrahedrons, 2], axis=1)\n",
    "\n",
    "    tetra_xyz = np.column_stack((tetra_bar[\"utm_x\"],\n",
    "                                 tetra_bar[\"utm_y\"],\n",
    "                                 tetra_bar[\"depth\"]))\n",
    "\n",
    "    n_tetra = len(tetra_bar[\"lon\"])\n",
    "    print(\" --> N. Tetra in the Gaussian Ellipsoid: {0}\".format(n_tetra))\n",
    "\n",
    "    # computing tetrahedrons volume\n",
    "    volume = np.zeros((n_tetra))\n",
    "    for i in range(n_tetra):\n",
    "        mm = np.column_stack((points_xyz[tetrahedrons[i, :], :],\n",
    "                              np.array([1, 1, 1, 1])))\n",
    "        volume[i] = np.abs(np.linalg.det(mm)/6.)\n",
    "\n",
    "    volume_tot = np.sum(volume)\n",
    "    print(\" --> Volume of Tetra in the Gaussian Ellipsoid: %.8e [m^3]\" % volume_tot)\n",
    "\n",
    "    Vol_diff_perc = (el['gaussian_ellipsoid']['vol'] - volume_tot) / el['gaussian_ellipsoid']['vol']*100\n",
    "    print(\" --> Volume difference Gaussian <--> Tetra: %.2f [%%]\" % Vol_diff_perc)\n",
    "\n",
    "    el['tetra_bar']                 = tetra_bar\n",
    "    el['tetrahedrons']              = tetrahedrons\n",
    "    el['gaussian_ellipsoid_volume'] = volume_tot\n",
    "    el['volumes_elements']          = volume\n",
    "    el['tetra_xyz']                 = tetra_xyz\n",
    "\n",
    "    return el    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed96d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the step3_run.py file\n",
    "\n",
    "def compute_hazard_curves(mih, prob_scenarios, n_pois, thresholds, sigma):\n",
    "\n",
    "    n_thr = len(thresholds)\n",
    "\n",
    "    hazard_curves_pois = np.zeros((n_pois, n_thr))\n",
    "\n",
    "    # print(n_pois, n_thr, n_scen)\n",
    "    # print(mih.shape, prob_scenarios.shape)\n",
    "\n",
    "    # hazard_mode = 'lognormal'\n",
    "    for ip in range(n_pois):\n",
    "\n",
    "            mih_at_poi = mih[:,ip]\n",
    "            ind_tmp = np.array(mih_at_poi == 0)\n",
    "            mih_at_poi[ind_tmp] = 1.e-12\n",
    "\n",
    "            mu = mih_at_poi\n",
    "            mu = mu.reshape(len(mu), 1)\n",
    "\n",
    "            cond_hazard_curve_tmp = 1 - scipy.stats.lognorm.cdf(thresholds, sigma, scale=mu).transpose()\n",
    "            hazard_curves_pois[ip,:] = np.sum(prob_scenarios*cond_hazard_curve_tmp, axis=1)\n",
    "\n",
    "    # # hazard_mode = 'lognormal_v1'\n",
    "    # mih_coo = scipy.sparse.coo_array(mih)\n",
    "    # print(\"Number of non-zero elements = {}\".format(len(mih_coo.data)))\n",
    "    # df_mihs = pd.DataFrame({\"id_scen\":mih_coo.row,\"id_poi\":mih_coo.col,\"mih_value\":mih_coo.data})\n",
    "    # df_prob_scenarios = pd.DataFrame(prob_scenarios)\\\n",
    "    #                             .reset_index()\\\n",
    "    #                             .rename(columns={'index':'id_scen',0:'prob_scen'})\n",
    "    # df_mihs = df_mihs.merge(df_prob_scenarios,how='left',left_on='id_scen', right_on='id_scen')\n",
    "\n",
    "    # for ith,threshold in enumerate(thresholds[:]):\n",
    "    #     df_mihs_thr = df_mihs.copy(deep=True)\n",
    "    #     col_name = 'prob_lognorm_{}'.format(threshold)\n",
    "    #     df_mihs_thr[col_name] = 1-scipy.stats.lognorm.cdf(threshold, sigma, scale=df_mihs_thr['mih_value']).transpose()\n",
    "    #     df_mihs_thr[col_name] = df_mihs_thr[col_name]*df_mihs_thr['prob_scen']\n",
    "    #     df_mihs_thr = df_mihs_thr.groupby(by='id_poi').agg({col_name:'sum'})\n",
    "    #     hazard_curves_pois[df_mihs_thr.index,ith]=df_mihs_thr[col_name].to_numpy()\n",
    "\n",
    "    return hazard_curves_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the step5_run.py file\n",
    "\n",
    "def plot_hazard_maps(points, hmaps, event_dict, map_label):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    proj = cartopy.crs.PlateCarree()\n",
    "    #cmap = plt.cm.magma_r\n",
    "    cmap = plt.cm.jet\n",
    "    ev_lon = event_dict['lon']\n",
    "    ev_lat = event_dict['lat']\n",
    "    ev_depth = event_dict['depth']\n",
    "    ev_mag = event_dict['mag']\n",
    "    ev_place = event_dict['place']\n",
    "\n",
    "    for key, hmap in hmaps.items():           \n",
    "                \n",
    "        print(\"mapping ... {}\".format(key))\n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "        ax = plt.axes(projection=cartopy.crs.Mercator())\n",
    "        coastline = cartopy.feature.GSHHSFeature(scale='low', levels=[1])\n",
    "        #coastline = cartopy.feature.GSHHSFeature(scale='high', levels=[1])\n",
    "        ax.add_feature(coastline, edgecolor='#000000', facecolor='#cccccc', linewidth=1)\n",
    "        ax.add_feature(cartopy.feature.BORDERS.with_scale('50m'))\n",
    "        ax.add_feature(cartopy.feature.STATES.with_scale('50m'))\n",
    "        ax.add_feature(cartopy.feature.OCEAN.with_scale('50m'))\n",
    "        gl = ax.gridlines(crs=proj, draw_labels=True, linewidth=1,\n",
    "                           color=\"#ffffff\", alpha=0.5, linestyle='-')\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.bottom_labels = True\n",
    "        gl.left_labels = True\n",
    "        gl.xformatter = cartopy.mpl.gridliner.LONGITUDE_FORMATTER\n",
    "        gl.yformatter = cartopy.mpl.gridliner.LATITUDE_FORMATTER\n",
    "        gl.xlabel_style = {'size': 14}\n",
    "        gl.ylabel_style = {'size': 14}\n",
    "\n",
    "        sc = ax.scatter(points[:,0], points[:,1], c=hmap, s=17, marker=\"o\", \n",
    "                    linewidths=0.75, edgecolors=\"#000000\", label=\" \",\n",
    "                    cmap=cmap, clip_on=True,# vmin=map_min, vmax=map_max, \n",
    "                    transform=proj, zorder=10, norm=matplotlib.colors.LogNorm(vmin=0.01, vmax=50))#(vmin=map_min, vmax=map_max))\n",
    "\n",
    "        ax.plot(ev_lon, ev_lat, linewidth=0, marker='*', markersize=14, \n",
    "                #markerfacecolor='#c0bfbc', markeredgecolor='#000000', \n",
    "                markerfacecolor='white', markeredgecolor='#000000', \n",
    "                transform=proj)\n",
    "\n",
    "        cbar = plt.colorbar(sc, shrink=0.75)\n",
    "        #cbar.ax.set_yticklabels(labels=cbar.ax.get_yticklabels(), fontsize=10)\n",
    "        cbar.set_label(label=f'(m)', size=12)\n",
    "        # ax.set_title(\"Hazard map - {0}\".format(key))\n",
    "        plt.suptitle(\"Hazard map - {0}\".format(key),fontsize=24)\n",
    "        plt.title(\"Epicentral Region: {0} \\n Event parameters: Lon={1}, Lat={2}, Depth={3}; Magnitude={4}\".format(ev_place, ev_lon, ev_lat, ev_depth, str(ev_mag)[:3] ),fontsize=18)\n",
    "        ax.set_xlabel(r'Longitude ($^\\circ$)', fontsize=14)\n",
    "        ax.set_ylabel(r'Latitude ($^\\circ$)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91265843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the scenario_player.py file\n",
    "\n",
    "def run_scenario_player(event_dict, data_folder):\n",
    "    \n",
    "    # Sea Level Data\n",
    "    search_radius_sld = 5000.  # in km\n",
    "    close_services = get_closest_services(event_dict['lat'], event_dict['lon'], search_radius_sld, data_folder)\n",
    "\n",
    "    origin_time = event_dict['ot']\n",
    "    # Convert origin_time to datetime object\n",
    "    datetime_object = datetime.strptime(origin_time, '%Y-%m-%dT%H:%M:%S')\n",
    "    # Calculate the time range\n",
    "    dt1 = datetime_object - timedelta(hours=12)\n",
    "    dt2 = datetime_object + timedelta(hours=12)\n",
    "    time_range = (dt1.strftime('%Y-%m-%dT%H:%M:%S'), dt2.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "    stations_with_data = []\n",
    "    stations_without_data = []\n",
    "    sealevel_dict = dict()\n",
    "    for station in close_services:\n",
    "        ssc_id = station.get('ssc_id', 'N/A').replace(\"SSC-\",\"\")  # Get 'ssc_id', default to 'N/A' if not found; remove \"SSC-\"\n",
    "        name = station.get('name', 'N/A')      # Get 'name', default to 'N/A' if not found\n",
    "        # get data\n",
    "        data_avl, sealevel_df = fetch_sl_data(origin_time, time_range, ssc_id)\n",
    "\n",
    "        # append station information to the appropriate list depending on the data availability\n",
    "        if data_avl:\n",
    "            sealevel_df = sealevel_df.loc[sealevel_df['sec'] >= 0]\n",
    "            # max_stat = sealevel_df['sea_level'].max()\n",
    "            # if not np.isnan(max_stat): \n",
    "            try:\n",
    "                # print(f\"Station: {name} | Max {max_stat} m\")\n",
    "                tide = smooth(sec = sealevel_df['sec'].to_numpy(), sealevel = sealevel_df['sea_level'].to_numpy())\n",
    "                sealevel_df['sea_level'] = sealevel_df['sea_level'] - tide\n",
    "            \n",
    "                sealevel_dict[name] = dict()\n",
    "                sealevel_dict[name]['coords'] = (float(station['geo:lat']), float(station['geo:lon']))\n",
    "                sealevel_dict[name]['data'] = sealevel_df\n",
    "                stations_with_data.append({\"ssc_id\": ssc_id, \"name\": name})\n",
    "            # else:\n",
    "            except:\n",
    "                # TODO Valparaiso station: problem with the radar sensor (prs would work)\n",
    "                stations_without_data.append({\"ssc_id\": ssc_id, \"name\": name, \"lon\": float(station['geo:lon']), \"lat\": float(station['geo:lat'])})\n",
    "        else:\n",
    "            stations_without_data.append({\"ssc_id\": ssc_id, \"name\": name, \"lon\": float(station['geo:lon']), \"lat\": float(station['geo:lat'])})\n",
    "\n",
    "\n",
    "    # print the results\n",
    "    print(f\"Stations without data available: {len(stations_without_data)}\")\n",
    "    print(f\"Stations with data available: {len(stations_with_data)}\")\n",
    "    for station in stations_with_data:\n",
    "        print(f\"  ssc_id: {station['ssc_id']}, name: {station['name']}\")\n",
    "    # for station in stations_without_data:\n",
    "    #     print(f\"  ssc_id: {station['ssc_id']}, name: {station['name']}\")\n",
    "\n",
    "    return sealevel_dict\n",
    "\n",
    "def get_closest_services(reference_lat, reference_lon, radius, data_folder):\n",
    "    \"\"\"\n",
    "    Finds sea level stations within a specified radius.\n",
    "    \"\"\"\n",
    "    \n",
    "    sl_path = os.path.join(data_folder, 'service_sl.json')\n",
    "    try:\n",
    "        with open(sl_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {sl_path}\")\n",
    "        exit()\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON format in the file.\")\n",
    "        exit()\n",
    "\n",
    "    close_services = []\n",
    "    search_radius = radius  # in kilometers\n",
    "\n",
    "    for record in data:\n",
    "        try:\n",
    "            service_lat = float(record.get('geo:lat', 0))  # Handle missing keys\n",
    "            service_lon = float(record.get('geo:lon', 0))\n",
    "\n",
    "            distance = haversine(reference_lat, reference_lon, service_lat, service_lon)\n",
    "\n",
    "            if distance < search_radius:\n",
    "                close_services.append(record)\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"Warning: Skipping record due to invalid latitude or longitude: {record}\")\n",
    "\n",
    "    print(f\"Number of SLD stations within {search_radius} km: {len(close_services)}\")\n",
    "    return close_services\n",
    "\n",
    "\n",
    "def fetch_sl_data(origin_time, time_range, station_code):\n",
    "    \"\"\"\n",
    "    Fetches sea level data for a certain station from a web API.\n",
    "    url = \"https://www.ioc-sealevelmonitoring.org/service.php\"\n",
    "    Note: the data is not filtered (original data from a sensor).\n",
    "\n",
    "    Args:\n",
    "        time_range (tuple): Start and end times for data retrieval.\n",
    "        station_code (str): Codename of the station.\n",
    "\n",
    "    Returns:\n",
    "        data (tuple): The sea level data from the API.\n",
    "        data_avl (bool): True if data is available, False otherwise.\n",
    "        df (pd.DataFrame): A DataFrame containing sea level data\n",
    "        relative to the time of the event (origin_time) in seconds.\n",
    "        The origin_time is defined by earthquake parameters.\n",
    "        None: If an error occurs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"query\": \"data\",\n",
    "        \"format\": \"json\",\n",
    "        \"code\": station_code,\n",
    "        # \"includesensors[]\": \"prs\",\n",
    "        \"timestart\": time_range[0],\n",
    "        \"timestop\": time_range[1]\n",
    "    }\n",
    "\n",
    "    url = \"http://www.ioc-sealevelmonitoring.org/service.php\"  # Base URL\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, verify=True)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving data: {e}\")\n",
    "        data_avl = False\n",
    "        df = None\n",
    "        return data_avl, df\n",
    "\n",
    "    try:\n",
    "        data = json.loads(response.text)\n",
    "        data_avl = True\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON data: {e}\")\n",
    "        data_avl = False\n",
    "        df = None\n",
    "        return data_avl, df\n",
    "\n",
    "    if not data:\n",
    "        # print(f\"No data available for the specified time range (station: {params['code']}).\")\n",
    "        data_avl = False\n",
    "        df = None\n",
    "        return data_avl, df\n",
    "\n",
    "    dates = [item[\"stime\"] for item in data]\n",
    "    slevels = [item[\"slevel\"] for item in data]\n",
    "\n",
    "    # Convert dates to seconds since origin_time\n",
    "    # origin_time = self.fetch_eq_param()['ot']\n",
    "    seconds_since_eq = [calculate_seconds_diff(origin_time, d) for d in dates]\n",
    "\n",
    "    dates = pd.to_datetime(dates)\n",
    "    df = pd.DataFrame({\"date\": dates, \"sec\": seconds_since_eq, \"sea_level\": slevels})\n",
    "\n",
    "    # return data, data_avl, df\n",
    "    return data_avl, df\n",
    "\n",
    "def calculate_seconds_diff(time1: str, time2: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the time difference in seconds between two timestamps.\n",
    "\n",
    "    Parameters:\n",
    "    time1,time2 (str): Can be an ISO 8601 string (like \"2015-10-23T19:02:29+00:00\",\"2015-10-23 19:02:29\" ) or just a date (e.g., \"2015-10-23\").\n",
    "\n",
    "    Returns:\n",
    "    int: The difference in seconds between the two timestamps.\n",
    "\n",
    "    Raises:\n",
    "    TypeError: If unable to process naive and aware datetime properly.\n",
    "    ValueError: If timestamps are in an invalid format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the input time strings, assuming they are in ISO 8601 format\n",
    "    try:\n",
    "          # Parse the datetimes\n",
    "          dt1 = datetime.fromisoformat(time1.strip()) #remove posible leading and tailing spaces\n",
    "          dt2 = datetime.fromisoformat(time2.strip()) #remove posible leading and tailing spaces\n",
    "\n",
    "          # Make both datetimes offset-aware if they're not already\n",
    "          if dt1.tzinfo is None:\n",
    "              dt1 = dt1.replace(tzinfo=timezone.utc)  # Assume UTC if no timezone in time1\n",
    "          if dt2.tzinfo is None:\n",
    "              dt2 = dt2.replace(tzinfo=timezone.utc)  # Assume UTC if no timezone in time2\n",
    "\n",
    "          diff = dt2 - dt1\n",
    "\n",
    "          return int(diff.total_seconds())\n",
    "\n",
    "    except TypeError as e:\n",
    "        raise TypeError(\"Ensure both datetime objects are offset-aware or offset-naive.\") from e\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\"Ensure the input timestamps are in valid ISO 8601 format.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e629e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the smooth_data.py file\n",
    "\n",
    "def lowess(x, y, span, method, robust, ite):\n",
    "    \"\"\"\n",
    "    LOWESS  Smooth data using Lowess or Loess method.\n",
    "  \n",
    "    The difference between LOWESS and LOESS is that LOWESS uses a\n",
    "    linear model to do the local fitting whereas LOESS uses a\n",
    "    quadratic model to do the local fitting. Some other software\n",
    "    may not have LOWESS, instead, they use LOESS with order 1 or 2 to\n",
    "    represent these two smoothing methods.\n",
    "  \n",
    "    Reference: \n",
    "    [79] W.S.Cleveland, \"Robust Locally Weighted Regression and Smoothing\n",
    "         Scatterplots\", _J. of the American Statistical Ass._, Vol 74, No. 368 \n",
    "         (Dec.,1979), pp. 829-836.\n",
    "         http://www.math.tau.ac.il/~yekutiel/MA%20seminar/Cleveland%201979.pdf\n",
    "      \n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    span = np.floor(span)\n",
    "    span = int(min(span, n))\n",
    "    c = np.copy(y)\n",
    "    \n",
    "    if (span == 1):\n",
    "        return\n",
    "    \n",
    "    useLoess = False\n",
    "    \n",
    "    if (method == \"loess\"):\n",
    "        useLoess = True\n",
    "  \n",
    "    diffx = np.diff(x)\n",
    "  \n",
    "    ynan = np.isnan(y)-0\n",
    "    anyNans = int(np.any(ynan[:]))\n",
    "    eps = np.spacing(1)\n",
    "    seps = np.sqrt(eps)\n",
    "    theDiffs = np.hstack((1, diffx, 1))\n",
    "  \n",
    "    if (robust):\n",
    "        lbound = np.zeros((n)).astype(\"int\")\n",
    "        rbound = np.zeros((n)).astype(\"int\")\n",
    "  \n",
    "    # Compute the non-robust smooth for non-uniform x\n",
    "    for i in range(n):\n",
    "        # if x(i) and x(i-1) are equal we just use the old value.\n",
    "        if (theDiffs[i] == 0):\n",
    "          c[i] = c[i-1]\n",
    "          if (robust):\n",
    "              lbound[i] = lbound[i-1]\n",
    "              rbound[i] = rbound[i-1]\n",
    "          continue\n",
    "  \n",
    "        # Find nearest neighbours\n",
    "        idx = iKNearestNeighbours(span, i, x, 1-ynan)\n",
    "  \n",
    "        if (robust):\n",
    "            # Need to store neighborhoods for robust loop\n",
    "            lbound[i] = np.min(idx)\n",
    "            rbound[i] = np.max(idx)\n",
    "        \n",
    "        if (len(idx) == 0):\n",
    "            c[i] = np.NaN\n",
    "            continue\n",
    "  \n",
    "        x1 = x[idx]-x[i] # center around current point to improve conditioning\n",
    "        d1 = np.abs(x1)\n",
    "        y1 = y[idx]\n",
    "  \n",
    "        weight = iTricubeWeights(d1)\n",
    "        if (np.all(weight < seps)):\n",
    "            weight[:] = 1    # if all weights are 0, just skip weighting\n",
    "  \n",
    "        ones = np.ones((len(x1),1))\n",
    "        x1 = np.reshape(x1,(len(x1),1))\n",
    "        v = np.hstack((ones, x1))\n",
    "        weight = np.reshape(weight,(len(weight),1))\n",
    "  \n",
    "        if (useLoess):\n",
    "            v = np.concatenate((v,x1*x1), axis=1) # There is no significant growth here\n",
    "          \n",
    "        ones = np.ones((1,np.shape(v)[1]), dtype=int)\n",
    "        weight = np.hstack((weight,weight,weight))\n",
    "        v = weight*v\n",
    "        y1 = weight[:,0]*y1\n",
    "        if (np.shape(v)[0] == np.shape(v)[1]):\n",
    "            # Square v may give infs in the \\ solution, so force least squares\n",
    "            zeros = np.zeros((1,np.shape(v)[0]))\n",
    "            v = np.vstack((v, zeros))\n",
    "            y1 = np.hstack((y1,0))\n",
    "            b = np.linalg.lstsq(v, y1, rcond=None) # for the old behavior rcond=-1\n",
    "        else:\n",
    "            b = np.linalg.lstsq(v, y1, rcond=None) # for the old behavior rcond=-1\n",
    "        \n",
    "        c[i] = b[0][0]\n",
    "      \n",
    "    \n",
    "    # now that we have a non-robust fit, we can compute the residual and do\n",
    "    # the robust fit if required\n",
    "    maxabsyXeps = np.max(np.abs(y))*eps\n",
    "    if (robust):\n",
    "        for k in range(ite):\n",
    "            r = y-c\n",
    "            # Compute robust weights\n",
    "            rweight = iBisquareWeights(r, maxabsyXeps)\n",
    "  \n",
    "            # Find new value for each point.\n",
    "            for i in range(n):\n",
    "  \n",
    "                if (i>0 and x[i]==x[i-1]):\n",
    "                    c[i] = c[i-1]\n",
    "                    continue\n",
    "                \n",
    "                if np.isnan(c[i]): \n",
    "                    continue \n",
    "                \n",
    "                idx = range(lbound[i], rbound[i]+1)\n",
    "                 \n",
    "                if (anyNans):\n",
    "                    ciao = np.nonzero(ynan[idx]==1)[0]\n",
    "                    idx[:] = [ item for i,item in enumerate(idx) if i not in ciao ]\n",
    "                \n",
    "                # check robust weights for removed points\n",
    "                if np.any(rweight[idx] <= 0):\n",
    "                    tmprw = (rweight > 0)\n",
    "                    idx = iKNearestNeighbours(span, i, x, tmprw)\n",
    "  \n",
    "                x1 = x[idx] - x[i]\n",
    "                d1 = np.abs(x1)\n",
    "                y1 = y[idx]\n",
    "                \n",
    "                weight = iTricubeWeights(d1)\n",
    "                if (np.all(weight < seps)):\n",
    "                    weight[:] = 1    # if all weights are 0, just skip weighting\n",
    "        \n",
    "                ones = np.ones((len(x1),1))\n",
    "                x1 = np.reshape(x1,(len(x1),1))\n",
    "                v = np.hstack((ones, x1))\n",
    "                #print np.shape(v),np.shape(ones),np.shape(x1)\n",
    "  \n",
    "                if (useLoess):\n",
    "                    v = np.concatenate((v,x1*x1), axis=1) # There is no significant growth here\n",
    "                \n",
    "                \n",
    "                # Modify the weights based on x values by multiplying them by\n",
    "                # robust weights.\n",
    "                weight = weight*rweight[idx]\n",
    "                weight = np.reshape(weight, (len(weight),1))\n",
    "                weight = np.hstack((weight,weight,weight))\n",
    "                v = weight*v\n",
    "                y1 = weight[:,0]*y1\n",
    "                #print np.shape(v), np.shape(y1)\n",
    "                if (np.shape(v)[0] == np.shape(v)[1]):\n",
    "                    # Square v may give infs in the \\ solution, so force least squares\n",
    "                    zeros = np.zeros((1,np.shape(v)[0]))\n",
    "                    v = np.vstack((v, zeros))\n",
    "                    y1 = np.hstack((y1,0))\n",
    "                    b = np.linalg.lstsq(v, y1, rcond=None) # for the old behavior rcond=-1\n",
    "                else:\n",
    "                    b = np.linalg.lstsq(v, y1, rcond=None) # for the old behavior rcond=-1\n",
    "                \n",
    "                c[i] = b[0][0]\n",
    "  \n",
    "    return c\n",
    "\n",
    "\n",
    "def iBisquareWeights(r, myeps):\n",
    "    \"\"\"\n",
    "    Bi-square (robust) weight function\n",
    "    Convert residuals to weights using the bi-square weight function.\n",
    "    NOTE that this function returns the square root of the weights\n",
    "    \"\"\"\n",
    "    # Only use non-NaN residuals to compute median\n",
    "    idx = np.isnan(r)\n",
    "    idx1 = np.nonzero(idx==False)\n",
    "    idx2 = np.nonzero(idx==True)\n",
    "    # And bound the median away from zero\n",
    "    s = np.max( np.array([1e8 * myeps, np.median(np.abs(r[idx1]))]) )\n",
    "    # Covert the residuals to weights\n",
    "    delta = iBisquare( r/(6*s) )\n",
    "    # Everything with NaN residual should have zero weight\n",
    "    delta[idx2] = 0\n",
    "    return delta\n",
    "\n",
    "\n",
    "def iBisquare(x):\n",
    "    \"\"\"\n",
    "    This is this bi-square function defined at the top of the left hand\n",
    "    column of page 831 in [C79]\n",
    "    NOTE that this function returns the square root of the weights\n",
    "    \"\"\"\n",
    "    b = np.zeros( np.shape(x) )\n",
    "    idx = np.abs(x) < 1\n",
    "    idx1 = np.nonzero(idx==True)\n",
    "    b[idx1] = np.abs(1 - x[idx1]**2)\n",
    "    return b\n",
    "\n",
    "\n",
    "def iKNearestNeighbours(k, i, x, ini):\n",
    "    \"\"\"\n",
    "    Find the k points from x(in) closest to x(i)\n",
    "    \"\"\"\n",
    "    \n",
    "    if (np.shape(np.nonzero(ini))[1] <= k):\n",
    "        # If we have k points or fewer, then return them all\n",
    "        idx = np.nonzero(ini)\n",
    "    else:\n",
    "        fanculo = np.nonzero(ini)\n",
    "        # Find the distance to the k closest point\n",
    "        d = np.abs(x-x[i])\n",
    "        ds = np.sort(d[fanculo])\n",
    "        dk = ds[k]\n",
    "        # Find all points that are as close as or closer than the k closest point\n",
    "        tmp = np.nonzero(d <= dk)\n",
    "\n",
    "        close = np.zeros((len(x)))\n",
    "        close[tmp[0]] = tmp[0]+1\n",
    "    \n",
    "        tmp1 = close * ini\n",
    "        # The required indices are those points that are both close and \"in\"\n",
    "        idx = np.nonzero(tmp1)[0]\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def iTricubeWeights(d):\n",
    "    \"\"\"\n",
    "    Tri-cubic weight function\n",
    "    Convert distances into weights using tri-cubic weight function.\n",
    "    NOTE that this function returns the square-root of the weights.\n",
    " \n",
    "    Protect against divide-by-zero. This can happen if more points than the span\n",
    "    are coincident.\n",
    "    \"\"\"\n",
    "    maxD = np.max(d)\n",
    "    if (maxD > 0):\n",
    "        d = d/np.max(d)\n",
    "\n",
    "    w = (1-d**3)**(1.5)\n",
    "    return w\n",
    "\n",
    "\n",
    "def smooth(**kwargs):\n",
    "    \"\"\"  \n",
    "    SMOOTH  Smooth data.\n",
    "  \n",
    "    Note:\n",
    "      x is time in seconds\n",
    "      y is sealevel values\n",
    "      span is the width of moving average window (default value 250)\n",
    "      method is the method used\n",
    "          'rloess' is the default and the only one tested\n",
    "          'moving' and 'sgolay' methods have not been implemented\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #x = np.array(kargs[0])\n",
    "    #y = np.array(kargs[1])\n",
    "    #span = kargs[2]\n",
    "    #method = kargs[3]\n",
    "\n",
    "    x = kwargs.get('sec', None)\n",
    "    y = kwargs.get('sealevel', None)\n",
    "    span = kwargs.get('span', 100)\n",
    "    method = kwargs.get('method', 'rloess')\n",
    "\n",
    "    t = len(y)\n",
    "    if (t == 0):\n",
    "        c = y\n",
    "        print(\"error:  {0}\".format(t))\n",
    "    elif (len(x) != t):\n",
    "        print(\"error: x and y must be same length\");\n",
    "  \n",
    "    # realize span\n",
    "    if (span <= 0):\n",
    "        print(\"error: span must be positive {0}\".format(span))\n",
    "  \n",
    "    if (span < 1):\n",
    "        span = np.ceil(span*t) # percent convention\n",
    "  \n",
    "    if (not span):\n",
    "        span = 5   \n",
    "\n",
    "    idx = range(0, t)\n",
    "    \n",
    "    sortx = np.any(np.diff(np.isnan(x)) < 0)   # if NaNs are not all at end\n",
    "    if (sortx or np.any(np.diff(x) < 0)):      # sort x\n",
    "        x, idx = np.sort(x)\n",
    "        y = y[idx]\n",
    "\n",
    "    c = np.ones(len(y))*np.nan\n",
    "    ok = 1-np.isnan(x)\n",
    "\n",
    "    if (method == 'moving'):\n",
    "        c[ok] = moving(x[ok], y[ok], span)\n",
    "    \n",
    "    elif (method in ('lowess', 'loess', 'rlowess', 'rloess') ):\n",
    "        robust = 0\n",
    "        ite = 5\n",
    "        if (method[0] == 'r'):\n",
    "            robust = 1\n",
    "            method = method[1:]\n",
    "      \n",
    "        c = lowess(x, y, span, method, robust, ite)\n",
    "\n",
    "    elif (method == 'sgolay'):\n",
    "        c[ok] = sgolay(x[ok], y[ok], span, degree)\n",
    "\n",
    "    else:\n",
    "        print(\"unrecognized method\")\n",
    "  \n",
    "    c[idx] = c\n",
    "\n",
    "    return c\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the misfit.py file\n",
    "\n",
    "def find_closest_pois(sealevel_dict, pois_d, event_dict):\n",
    "    \"\"\"\n",
    "    Find closest POIs to each sea level stations\n",
    "    and return a list of indexes \n",
    "    \"\"\"\n",
    "    print(\"Finding closest POIs to each gauge\")\n",
    "    \n",
    "    npois = len(pois_d['pois_coords'])\n",
    "    gauge_pois = []\n",
    "    selected_pois_lon = []\n",
    "    selected_pois_lat = []\n",
    "    for gauge in sealevel_dict.keys():\n",
    "        lon_gauge = sealevel_dict[gauge]['coords'][1]\n",
    "        lat_gauge = sealevel_dict[gauge]['coords'][0]\n",
    "        dist=np.ones(npois)\n",
    "        for i in range(npois):\n",
    "            lon_poi = pois_d['pois_coords'][i,0]\n",
    "            lat_poi = pois_d['pois_coords'][i,1]\n",
    "            dist[i] = haversine(lon_gauge, lat_gauge, lon_poi, lat_poi)\n",
    "        idx = np.argmin(dist)\n",
    "        \n",
    "        # logger.info(\"Gauge %s (%.2f,%.2f): POI %d (%.2f,%.2f)\" % (gauge, lon_gauge, lat_gauge, pois_d['pois_index'][idx], pois_d['pois_coords'][idx,0], pois_d['pois_coords'][idx,1]))\n",
    "        print(\"Gauge %s (%.2f,%.2f): POI %d (%.2f,%.2f)\" % (gauge, lon_gauge, lat_gauge, idx, pois_d['pois_coords'][idx,0], pois_d['pois_coords'][idx,1]))\n",
    "        \n",
    "        # gauge_pois.append(pois_d['pois_index'][idx])\n",
    "        gauge_pois.append(idx)\n",
    "        selected_pois_lon.append(pois_d['pois_coords'][idx,0])\n",
    "        selected_pois_lat.append(pois_d['pois_coords'][idx,1])\n",
    "    \n",
    "    # plot_selected_pois\n",
    "    ax, proj = map_setup()\n",
    "\n",
    "    ax.plot(selected_pois_lon[:], selected_pois_lat[:], markerfacecolor='#ff0000', marker=\"o\", \n",
    "                linewidth=0, markeredgecolor=\"#000000\",\n",
    "                transform=proj, zorder=10)\n",
    "\n",
    "    for gauge in sealevel_dict.keys():\n",
    "        lon_gauge = sealevel_dict[gauge]['coords'][1]\n",
    "        lat_gauge = sealevel_dict[gauge]['coords'][0]\n",
    "        ax.plot(lon_gauge, lat_gauge, linewidth=0, marker='^', markersize=14, \n",
    "                markerfacecolor='yellow', markeredgecolor='#000000', \n",
    "                transform=proj)\n",
    "        \n",
    "    ax.plot(event_dict['lon'], event_dict['lat'], linewidth=0, marker='*', markersize=10,\n",
    "            markerfacecolor='white', markeredgecolor='#000000', transform=proj)\n",
    "\n",
    "    ax.set_xlabel(r'Longitude ($^\\circ$)', fontsize=14)\n",
    "    ax.set_ylabel(r'Latitude ($^\\circ$)', fontsize=14)\n",
    "\n",
    "\n",
    "    return gauge_pois\n",
    "\n",
    "def pick_arrival_times(gauge_data):\n",
    "        #ngauges,N,gauge_data,\\\n",
    "        #scenario_time,scenario_results,time_range,arrtime_percentage,scenario_min_height,scenario_max_height):\n",
    "    \"\"\"\n",
    "    Pick arrival times (they have to be used in the time range picking)\n",
    "    ONLY FOR DATA - removed scenarios since simulations are not available here\n",
    "    \"\"\"\n",
    "\n",
    "    # Overwrite gauge times and data so that they match the scenario time range\n",
    "    # for gauge in range(ngauges):\n",
    "    ngauges = len(gauge_data.keys())\n",
    "    gauge = 0\n",
    "    gauge_name = []\n",
    "    data_list = []\n",
    "    time_list = []\n",
    "    for key, inner_dict in gauge_data.items():\n",
    "        gauge_name.append(key)\n",
    "        df = inner_dict['data']\n",
    "        data_list.append(df['sea_level'].values)\n",
    "        time_list.append(df['sec'].values)\n",
    "        # data_list[gauge] = data_list[gauge][(time_list[gauge] >= time_range[0]) &\n",
    "        #                                                 (time_list[gauge] <= time_range[1])]\n",
    "        # time_list[gauge] = time_list[gauge][(time_list[gauge] >= time_range[0]) &\n",
    "        #                                                 (time_list[gauge] <= time_range[1])]\n",
    "        gauge = gauge + 1\n",
    "  \n",
    "    # Get PTF min/max waveheight (and indices with NaNs for the synthetic data)\n",
    "    min_waveheight, max_waveheight = get_PTF_waveheights(ngauges, data_list)\n",
    "\n",
    "    # Arrival times for the gauges\n",
    "    arrival_times = np.zeros(ngauges)\n",
    "    arrtime_percentage = 0.1  # Percentage of the maximum waveheight to trigger the arrival time\n",
    "    for gauge in range(ngauges):\n",
    "        absmax_waveheight = max(np.abs(min_waveheight[gauge]), np.abs(max_waveheight[gauge]))\n",
    "        arrtime_trigger = absmax_waveheight * arrtime_percentage\n",
    "        # tmp_data = data_list[gauge][(time_list[gauge] >= time_range[0])]\n",
    "        # arrtime_index = np.argmax(np.array(np.abs(tmp_data)) >= arrtime_trigger)\n",
    "        arrtime_index = np.argmax(np.array(np.abs(data_list[gauge])) >= arrtime_trigger)\n",
    "        arrival_times[gauge] = time_list[gauge][arrtime_index]\n",
    "        if gauge_name[gauge] in ['Antofagasta', 'Caldera', 'DART Lima', 'Iquique']:\n",
    "            arrival_times[gauge] -= 300.\n",
    "        # if gauge_name[gauge] in ['DART_32411', 'DART_32412', 'DART_43412']:\n",
    "        #     arrival_times[gauge] -= 300.\n",
    "        # elif gauge_name[gauge] in ['DART_51425', 'DART_51426', 'DART_54401']:\n",
    "        #     arrival_times[gauge] -= 600.\n",
    "\n",
    "    # Cut off gauge times and data that lie outside the arrival time range \n",
    "    gauge_cut_times = []\n",
    "    gauge_cut_data = []\n",
    "    for gauge in range(ngauges):\n",
    "        # gauge_tmp_times = time_list[gauge][time_list[gauge] >= arrival_times[gauge]]\n",
    "        # gauge_tmp_data = data_list[gauge][time_list[gauge] >= arrival_times[gauge]]\n",
    "        gauge_time_limit = arrival_times[gauge] + 7200.\n",
    "        # print(f\"Gauge {gauge_name[gauge]}, arrival time {arrival_times[gauge]}, end time {gauge_time_limit}\")  \n",
    "        gauge_tmp_times = time_list[gauge][(time_list[gauge] >= arrival_times[gauge]) & (time_list[gauge] <= gauge_time_limit)]\n",
    "        gauge_tmp_data = data_list[gauge][(time_list[gauge] >= arrival_times[gauge]) & (time_list[gauge] <= gauge_time_limit)]  \n",
    "        gauge_cut_times.append(gauge_tmp_times)\n",
    "        gauge_cut_data.append(gauge_tmp_data)\n",
    "\n",
    "    return arrival_times #, gauge_cut_data, min_waveheight, max_waveheight\n",
    "\n",
    "def get_PTF_waveheights(ngauges, gauge_data):\n",
    "    \"\"\"\n",
    "    Function to save the minimum and maximum PTF waveheight.\n",
    "    \"\"\"\n",
    "    minimum_waveheight = np.zeros(ngauges)\n",
    "    maximum_waveheight = np.zeros(ngauges)\n",
    "    for gauge in range(ngauges):\n",
    "        minimum_waveheight[gauge] = np.min(gauge_data[gauge])\n",
    "        maximum_waveheight[gauge] = np.max(gauge_data[gauge])\n",
    "        if (np.isnan(maximum_waveheight[gauge]) or np.isnan(minimum_waveheight[gauge])):\n",
    "            print(\"Gauge entry is a masked array. Setting min/max waveheight to zero.\")\n",
    "            minimum_waveheight[gauge] = 0.\n",
    "            maximum_waveheight[gauge] = 0.\n",
    "\n",
    "    return minimum_waveheight, maximum_waveheight\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
