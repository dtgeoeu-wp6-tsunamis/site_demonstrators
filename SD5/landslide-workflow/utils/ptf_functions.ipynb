{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.matlib as npm\n",
    "import utm\n",
    "import ast\n",
    "import copy\n",
    "import math\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import distributions\n",
    "from operator import itemgetter\n",
    "from numba import jit\n",
    "import cartopy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6446bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pois(pois_coords, ind, event_dict):\n",
    "\n",
    "    ev_lon = event_dict['lon']\n",
    "    ev_lat = event_dict['lat']\n",
    "    \n",
    "    proj = cartopy.crs.PlateCarree()\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    ax = plt.axes(projection=cartopy.crs.Mercator())\n",
    "\n",
    "    # coastline = cartopy.feature.GSHHSFeature(scale='low', levels=[1])\n",
    "    coastline = cartopy.feature.GSHHSFeature(scale='high', levels=[1])\n",
    "    ax.add_feature(coastline, edgecolor='#000000', facecolor='#cccccc', linewidth=1)\n",
    "    ax.add_feature(cartopy.feature.BORDERS.with_scale('50m'))\n",
    "    ax.add_feature(cartopy.feature.STATES.with_scale('50m'))\n",
    "    ax.add_feature(cartopy.feature.OCEAN.with_scale('50m'))\n",
    "    gl = ax.gridlines(crs=proj, draw_labels=True, linewidth=1,\n",
    "                        color=\"#ffffff\", alpha=0.5, linestyle='-')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.bottom_labels = True\n",
    "    gl.left_labels = True\n",
    "    gl.xformatter = cartopy.mpl.gridliner.LONGITUDE_FORMATTER\n",
    "    gl.yformatter = cartopy.mpl.gridliner.LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 14}\n",
    "    gl.ylabel_style = {'size': 14}\n",
    "\n",
    "    ax.plot(pois_coords[ind,0], pois_coords[ind,1], markerfacecolor=\"#0ba518\", marker=\"o\",\n",
    "                linewidth=0, markeredgecolor=\"#000000\",\n",
    "                transform=proj)#, zorder=10)\n",
    "\n",
    "    ax.plot(ev_lon, ev_lat, linewidth=0, marker='*', markersize=10,\n",
    "            markerfacecolor='red', markeredgecolor='#000000',\n",
    "            transform=proj)\n",
    "    \n",
    "    ax.set_extent([np.min(pois_coords[:,0]), np.max(pois_coords[:,0]), np.min(pois_coords[:,1]), np.max(pois_coords[:,1])], crs=proj)\n",
    "\n",
    "    ax.set_xlabel(r'Longitude ($^\\circ$)', fontsize=14)\n",
    "    ax.set_ylabel(r'Latitude ($^\\circ$)', fontsize=14)\n",
    "\n",
    "def plot_barycenters(bs_coords, prob, event_dict):\n",
    "\n",
    "    # fig,axs = plt.subplots(2,1,figsize=(20,20),subplot_kw={'projection': cartopy.crs.Mercator()})\n",
    "    # fig.set_tight_layout(True)\n",
    "    ev_lon = event_dict['lon']\n",
    "    ev_lat = event_dict['lat']\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    proj = cartopy.crs.PlateCarree()\n",
    " \n",
    "    ax=plt.axes(projection=cartopy.crs.Mercator())\n",
    "    coastline = cartopy.feature.GSHHSFeature(scale='high', levels=[1])\n",
    "    ax.add_feature(coastline, edgecolor='#000000', facecolor='#cccccc', linewidth=1)\n",
    "    ax.add_feature(cartopy.feature.BORDERS.with_scale('50m'))\n",
    "    ax.add_feature(cartopy.feature.STATES.with_scale('50m'))\n",
    "    ax.add_feature(cartopy.feature.OCEAN.with_scale('50m'))\n",
    "    gl = ax.gridlines(crs=proj, draw_labels=True, linewidth=1,\n",
    "                        color=\"#ffffff\", alpha=0.5, linestyle='-')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.bottom_labels = True\n",
    "    gl.left_labels = True\n",
    "    gl.xformatter = cartopy.mpl.gridliner.LONGITUDE_FORMATTER\n",
    "    gl.yformatter = cartopy.mpl.gridliner.LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 14}\n",
    "    gl.ylabel_style = {'size': 14}\n",
    " \n",
    "    lonlat_list=[list(x) for x in set(tuple(x) for x in bs_coords)]\n",
    "    nloc=len(lonlat_list)\n",
    " \n",
    "    sumprob=[]\n",
    "    for iloc in range(nloc):\n",
    "        locxy=lonlat_list[iloc]\n",
    "        isel = [i for i, x in enumerate(list(bs_coords)) if np.allclose(x, locxy)]\n",
    "        # isel = [i for i, x in enumerate(list(bs_coords)) if x == locxy]\n",
    "        #print(len(isel))\n",
    "        tmplist=[prob[i] for i in isel]\n",
    "        sumprob.append(sum(tmplist))\n",
    "    lon = [el[0] for el in lonlat_list]\n",
    "    lat = [el[1] for el in lonlat_list]\n",
    " \n",
    "    # lon = bs_coords[:,0] \n",
    "    # lat = bs_coords[:,1]\n",
    "    cmap = ax.scatter(lon,lat,s=30, transform=proj,edgecolors='k',c=sumprob,cmap=plt.cm.plasma)#, label=labels[ic], c=colors[ic])\n",
    "    cbar = plt.colorbar(cmap,ax=ax,extend='both',extendfrac='auto',aspect=30,format='%.0e',pad=0.02,\n",
    "                 label='Cumulated Probability (for each barycenter)',shrink=0.8)\n",
    " \n",
    "    ax.plot(ev_lon, ev_lat, linewidth=0, marker='*', markersize=10,\n",
    "            markerfacecolor='red', markeredgecolor='#000000',\n",
    "            transform=proj)\n",
    "     \n",
    "    ax.set_extent([np.min(lon)-0.2, np.max(lon)+0.2, np.min(lat)-0.2, np.max(lat)+0.2], crs=proj)\n",
    "\n",
    "    cbar.locator = ticker.MaxNLocator(nbins=10)\n",
    "    cbar.update_ticks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afcca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_scaling_laws.py file\n",
    "def scalinglaw_WC(**kwargs):\n",
    "    '''\n",
    "    Scaling law from Wells&Coppersmith (1994)\n",
    "    '''\n",
    "\n",
    "    mag        = kwargs.get('mag', None)\n",
    "    type_scala = kwargs.get('type_scala', None)\n",
    "\n",
    "    if (type_scala == 'M2L'):\n",
    "        a =-2.440\n",
    "        b =0.590\n",
    "        y = 10.**(a+b*mag)\n",
    "\n",
    "    elif (type_scala == 'M2W'):\n",
    "        a=-1.010\n",
    "        b=0.320\n",
    "        y = 10.**(a+b*mag)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Scaling law in scalinglaw_WC not recognized. Exit!\")\n",
    "        #print(\"Scaling law in scalinglaw_WC not recognized. Exit!\")\n",
    "        #sys.exit()\n",
    "\n",
    "    return y\n",
    "\n",
    "def scalinglaw_Murotani(**kwargs):\n",
    "    '''\n",
    "    Scaling law from Murotani et al (2013)\n",
    "    '''\n",
    "\n",
    "    mag        = kwargs.get('mag', None)\n",
    "    type_scala = kwargs.get('type_scala', None)\n",
    "\n",
    "    a    = -3.806\n",
    "    b    = 1.000\n",
    "    Area = 10**(a+b*mag)\n",
    "\n",
    "    if (type_scala == 'M2L'):\n",
    "        y = math.sqrt(2.5*Area)/2.5\n",
    "\n",
    "    elif (type_scala == 'M2W'):\n",
    "        y = math.sqrt(2.5*Area)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Scaling law in scalinglaw_Murotani not recognized. Exit!\")\n",
    "        #print(\"Scaling law in scalinglaw_Murotani not recognized. Exit!\")\n",
    "        #sys.exit()\n",
    "\n",
    "    return y\n",
    "\n",
    "def mag_to_l_BS(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 1000.0 * scalinglaw_WC(mag=mag, type_scala='M2L')\n",
    "\n",
    "    return out\n",
    "\n",
    "def mag_to_w_BS(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 1000.0 * scalinglaw_WC(mag=mag, type_scala='M2W')\n",
    "\n",
    "    return out\n",
    "\n",
    "def mag_to_l_PS(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 1000.0 * scalinglaw_Murotani(mag=mag, type_scala='M2W')\n",
    "\n",
    "    return out\n",
    "\n",
    "def correct_BS_horizontal_position(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 0.5 * mag_to_l_BS(mag=mag)\n",
    "\n",
    "    return out\n",
    "\n",
    "def correct_PS_horizontal_position(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = 0.5 * mag_to_l_PS(mag=mag)\n",
    "\n",
    "    return out\n",
    "\n",
    "def correct_BS_vertical_position(**kwargs):\n",
    "\n",
    "    mag = kwargs.get('mag', None)\n",
    "    out = math.sin(math.pi/4)*0.5 * mag_to_w_BS(mag=mag)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3634bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_load_event.py file\n",
    "def int_quake_cat2dict(json_object):\n",
    "\n",
    "    d = dict()\n",
    "\n",
    "    # Event Ids\n",
    "    try:\n",
    "        origin_id =  str(json_object['features'][0]['properties']['originid'])\n",
    "    except:\n",
    "        origin_id =  str(json_object['features'][0]['properties']['originId'])\n",
    "\n",
    "    event_id      =  str(json_object['features'][0]['properties']['eventId'])\n",
    "    # author        =  str(json_object['features'][0]['properties']['author'])\n",
    "    # version       =  str(json_object['features'][0]['properties']['version'])\n",
    "\n",
    "    # Epicenter informations\n",
    "    lon           =  float(json_object['features'][0]['geometry']['coordinates'][0])\n",
    "    lat           =  float(json_object['features'][0]['geometry']['coordinates'][1])\n",
    "    depth         =  float(json_object['features'][0]['geometry']['coordinates'][2])\n",
    "    OT            =  str(json_object['features'][0]['properties']['time'])\n",
    "    ev_type       =  str(json_object['features'][0]['properties']['type'])\n",
    "    mag_type      =  str(json_object['features'][0]['properties']['magType'])\n",
    "    place          =  str(json_object['features'][0]['properties']['place'])\n",
    "\n",
    "    #utm conversion\n",
    "    ee_utm = utm.from_latlon(lat, lon)\n",
    "\n",
    "    # Specific Mag percentiles and covariant cov_matrix\n",
    "    mag_percentiles = json_object['features'][0]['properties']['mag_percentiles']\n",
    "    cov_matrix      = json_object['features'][0]['properties']['cov_matrix']\n",
    "    \n",
    "    pos_Sigma = cov_matrix.copy() #json_string['features'][0]['properties']['cov_matrix']\n",
    "\n",
    "    cov_matrix['XX'] = float(cov_matrix['XX'])\n",
    "    cov_matrix['XY'] = float(cov_matrix['XY'])\n",
    "    cov_matrix['XZ'] = float(cov_matrix['XZ'])\n",
    "    cov_matrix['YY'] = float(cov_matrix['YY'])\n",
    "    cov_matrix['YZ'] = float(cov_matrix['YZ'])\n",
    "    cov_matrix['ZZ'] = float(cov_matrix['ZZ'])\n",
    "\n",
    "    pos_Sigma['XX']  = float(pos_Sigma['XX']) * 1e6\n",
    "    pos_Sigma['XY']  = float(pos_Sigma['XY']) * 1e6\n",
    "    pos_Sigma['XZ']  = float(pos_Sigma['XZ']) * 1e6\n",
    "    pos_Sigma['YY']  = float(pos_Sigma['YY']) * 1e6\n",
    "    pos_Sigma['YZ']  = float(pos_Sigma['YZ']) * 1e6\n",
    "    pos_Sigma['ZZ']  = float(pos_Sigma['ZZ']) * 1e6\n",
    "\n",
    "    mag_percentiles['p16']  = float(mag_percentiles['p16'])\n",
    "    mag_percentiles['p50']  = float(mag_percentiles['p50'])\n",
    "    mag_percentiles['p84']  = float(mag_percentiles['p84'])\n",
    "    mag_sigma = 0.5 * (mag_percentiles['p84'] - mag_percentiles['p16'])\n",
    "\n",
    "    d['eventid']        = event_id\n",
    "    d['originid']       = origin_id\n",
    "    d['lat']            = lat\n",
    "    d['lon']            = lon\n",
    "    d['depth']          = depth\n",
    "    d['ot']             = OT\n",
    "    d['mag']            = mag_percentiles['p50']\n",
    "    d['mag_percentiles'] = mag_percentiles\n",
    "    d['MagSigma']        = mag_sigma\n",
    "    d['type']           = ev_type\n",
    "    d['mag_type']       = mag_type\n",
    "    d['ee_utm']         = ee_utm\n",
    "    d['place']          = place\n",
    "\n",
    "    # d['version']        = \"%03d\" % (float(version))\n",
    "    # d['author']         = author\n",
    "    # d['area']           = area\n",
    "    # d['area_geo']       = area_geo\n",
    "    # d['mag']            = mag\n",
    "    # d['mag_values']     = mag_values\n",
    "    # d['mag_counts']     = mag_counts\n",
    "    # d['ct']             = creation_time\n",
    "    # d['ot_year']        = origin_year\n",
    "    # d['ot_month']       = origin_month\n",
    "    # d['ot_day']         = origin_day\n",
    "\n",
    "    d['cov_matrix']      = cov_matrix\n",
    "    d['pos_Sigma']       = pos_Sigma\n",
    "\n",
    "    d['ee_PosCovMat_2d'] = np.array([[cov_matrix['XX'], cov_matrix['XY']], \\\n",
    "                                     [cov_matrix['XY'], cov_matrix['YY']]])\n",
    "    d['PosMean_2d']      = np.array([d['ee_utm'][0], \\\n",
    "                                     d['ee_utm'][1]])\n",
    "    d['PosCovMat_3d']    = np.array([[cov_matrix['XX'], cov_matrix['XY'], cov_matrix['XZ']], \\\n",
    "                                     [cov_matrix['XY'], cov_matrix['YY'], cov_matrix['YZ']], \\\n",
    "                                     [cov_matrix['XZ'], cov_matrix['YZ'], cov_matrix['ZZ']]])\n",
    "    d['PosCovMat_3dm']    = d['PosCovMat_3d']*1000000\n",
    "    d['PosMean_3d']      = np.array([d['ee_utm'][0], \\\n",
    "                                     d['ee_utm'][1], \\\n",
    "                                     d['depth'] * 1000.0])\n",
    "\n",
    "    # d['root_name']       = str(d['ot_year']) + str(d['ot_month']) + str(d['ot_day']) + '_' + \\\n",
    "    #                        d['area']\n",
    "    #                        #str(d['eventid']) + '_' + str(d['version']) + '_' + d['area']\n",
    "\n",
    "    return d\n",
    "\n",
    "def compute_position_sigma_lat_lon(event_parameters):\n",
    "    \"\"\"\n",
    "    REFERENCE LAT = YY\n",
    "    REFERENCE LON = XX\n",
    "    \"\"\"\n",
    "\n",
    "    bs_mag_max          = 8.1\n",
    "\n",
    "    sigma               = event_parameters['sigma']\n",
    "    event_mag           = event_parameters['mag_percentiles']['p50']\n",
    "    event_mag_max       = event_parameters['mag_percentiles']['p50'] + \\\n",
    "                          event_parameters['MagSigma'] * sigma\n",
    "    event_mag_sigma     = event_parameters['MagSigma']\n",
    "\n",
    "    event_cov_xx        = event_parameters['pos_Sigma']['XX']\n",
    "    event_cov_xy        = event_parameters['pos_Sigma']['XY']\n",
    "    event_cov_yy        = event_parameters['pos_Sigma']['YY']\n",
    "\n",
    "\n",
    "    mag_to_correct      = min(bs_mag_max, event_mag_max)\n",
    "\n",
    "    delta_position_BS_h = correct_BS_horizontal_position(mag=mag_to_correct)\n",
    "    delta_position_PS_h = correct_PS_horizontal_position(mag=event_mag + sigma * event_mag_sigma)\n",
    "\n",
    "    position_BS_sigma_yy  = math.sqrt(abs(event_cov_yy)) + delta_position_BS_h\n",
    "    position_BS_sigma_xx  = math.sqrt(abs(event_cov_xx)) + delta_position_BS_h\n",
    "    position_BS_sigma_xy  = math.sqrt(abs(event_cov_xy)) + delta_position_BS_h\n",
    "\n",
    "    event_parameters['position_BS_sigma_yy'] = position_BS_sigma_yy\n",
    "    event_parameters['position_BS_sigma_xx'] = position_BS_sigma_xx\n",
    "    event_parameters['position_BS_sigma_xy'] = position_BS_sigma_xy\n",
    "\n",
    "    position_PS_sigma_yy  = math.sqrt(abs(event_cov_yy)) + delta_position_PS_h\n",
    "    position_PS_sigma_xx  = math.sqrt(abs(event_cov_xx)) + delta_position_PS_h\n",
    "    position_PS_sigma_xy  = math.sqrt(abs(event_cov_xy)) + delta_position_PS_h\n",
    "\n",
    "    event_parameters['position_PS_sigma_yy'] = position_PS_sigma_yy\n",
    "    event_parameters['position_PS_sigma_xx'] = position_PS_sigma_xx\n",
    "    event_parameters['position_PS_sigma_xy'] = position_PS_sigma_xy\n",
    "\n",
    "    return event_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b461ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_ellipsoid.py file\n",
    "def build_ellipsoid_objects(event_parameters, sigma_inn, sigma_out):\n",
    "\n",
    "    ellipse = dict()\n",
    "\n",
    "    location_ellipse_2d_BS_inn = build_location_ellipsoid_objects(event= event_parameters,\n",
    "                                                                  sigma = sigma_inn,\n",
    "                                                                  seismicity_type = 'BS')\n",
    "    ellipse['location_ellipse_2d_BS_inn'] = location_ellipse_2d_BS_inn\n",
    "\n",
    "    location_ellipse_2d_BS_out = build_location_ellipsoid_objects(event= event_parameters,\n",
    "                                                                  sigma = sigma_out,\n",
    "                                                                  seismicity_type = 'BS')\n",
    "    ellipse['location_ellipse_2d_BS_out'] = location_ellipse_2d_BS_out\n",
    "\n",
    "    location_ellipse_2d_PS_inn = build_location_ellipsoid_objects(event= event_parameters,\n",
    "                                                                  sigma = sigma_inn,\n",
    "                                                                  seismicity_type = 'PS')\n",
    "    ellipse['location_ellipse_2d_PS_inn'] = location_ellipse_2d_PS_inn\n",
    "\n",
    "    location_ellipse_2d_PS_out = build_location_ellipsoid_objects(event= event_parameters,\n",
    "                                                                  sigma = sigma_out,\n",
    "                                                                  seismicity_type = 'PS')\n",
    "    ellipse['location_ellipse_2d_PS_out'] = location_ellipse_2d_PS_out\n",
    "\n",
    "    return ellipse\n",
    "\n",
    "def build_location_ellipsoid_objects(**kwargs):\n",
    "    \"\"\"\n",
    "    From ellipsedata.m\n",
    "    % Copyright (c) 2014, Hugo Gabriel Eyherabide, Department of Mathematics\n",
    "    % and Statistics, Department of Computer Science and Helsinki Institute\n",
    "    % for Information Technology, University of Helsinki, Finland.\n",
    "    % All rights reserved.\n",
    "\n",
    "    !!!! Difference with the original matlab function !!!!\n",
    "    sigma in this python function is a float\n",
    "    sigma in matlab is a vector\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ee                 = kwargs.get('event', 'None')\n",
    "    sigma              = kwargs.get('sigma', 'None')\n",
    "    seismicity_type    = kwargs.get('seismicity_type', 'None')\n",
    "   \n",
    "    # Number of points to set the 2d ellipse\n",
    "    nr_points = 1000\n",
    "    \n",
    "    sigma = float(sigma)\n",
    "\n",
    "    # 2d Covariant matrix, eigenvalues and eignevectors\n",
    "    if(seismicity_type == 'BS'):\n",
    "       cov_matrix   = np.array([ [ee['position_BS_sigma_yy']**2, 0], [0, ee['position_BS_sigma_xx']**2] ])\n",
    "    elif(seismicity_type == 'PS'):\n",
    "       cov_matrix   = np.array([ [ee['position_PS_sigma_yy']**2, 0], [0, ee['position_PS_sigma_xx']**2] ])\n",
    "    else:\n",
    "       raise Exception('No seismicity type found. Exit')\n",
    "       #sys.exit('No seismicity type found. Exit')\n",
    "\n",
    "    # Center of the ellipse\n",
    "    center = (ee['ee_utm'][1],ee['ee_utm'][0])\n",
    "\n",
    "    PV, PD = np.linalg.eigh(cov_matrix)\n",
    "    PV = np.sqrt(np.diag(PV))\n",
    "\n",
    "    # Build points of ellipse\n",
    "    theta = np.linspace(0,2*np.pi,nr_points)\n",
    "    elpt  = np.dot(np.transpose(np.array([np.cos(theta), np.sin(theta)])) , PV)\n",
    "    elpt  = np.dot(elpt, np.transpose(PD))\n",
    "\n",
    "    # Add uncertainty\n",
    "    elpt = elpt * sigma\n",
    "\n",
    "    # shift to the center\n",
    "    elpt    = np.transpose(elpt)\n",
    "    elpt[0] = elpt[0] + center[0]\n",
    "    elpt[1] = elpt[1] + center[1]\n",
    "    elpt    = np.transpose(elpt)\n",
    "\n",
    "    return elpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_pre_load.py file\n",
    "\n",
    "def load_intensity_thresholds(data_folder):\n",
    "    \"\"\"\n",
    "    READ THRESHOLDS\n",
    "    \"\"\"\n",
    "    \n",
    "    intensity_thresholds = os.path.join(data_folder, 'intensity_thresholds.npy')\n",
    "\n",
    "    ith = np.load(intensity_thresholds, allow_pickle=True)\n",
    "    intensity_measure = ith.item().keys()\n",
    "    thresholds = ith.item()[list(intensity_measure)[0]]\n",
    "\n",
    "    return thresholds, intensity_measure\n",
    "\n",
    "\n",
    "def load_Model_Weights(data_folder):\n",
    "\n",
    "    ps_type = 1\n",
    "\n",
    "    weight_npy  = os.path.join(data_folder, 'ModelWeights.npy')\n",
    "    # empty_space = \"%10s\" % ('')\n",
    "\n",
    "    # print('Loading model weights dictionary:      <------ {}'.format(weight_npy))\n",
    "    foe = np.load(weight_npy, allow_pickle=True)\n",
    "    modelweights = foe.item()\n",
    "    # for k in modelweights.keys():\n",
    "    #     print('{} {}'.format(empty_space, k))\n",
    "\n",
    "    # select only one type of ps_weigth\n",
    "    # Only one weigth mode can be selected\n",
    "    selected_index = np.where(modelweights['BS1_Mag']['Type'] == int(ps_type))\n",
    "    modelweights['BS1_Mag']['Type'] = modelweights['BS1_Mag']['Type'][selected_index]\n",
    "    modelweights['BS1_Mag']['Wei'] = modelweights['BS1_Mag']['Wei'][selected_index]\n",
    "\n",
    "    selected_index = np.where(modelweights['PS2_Bar']['Type'] == int(ps_type))\n",
    "    modelweights['PS2_Bar']['Type'] = modelweights['PS2_Bar']['Type'][selected_index]\n",
    "    modelweights['PS2_Bar']['Wei'] = modelweights['PS2_Bar']['Wei'][selected_index]\n",
    "\n",
    "    return modelweights\n",
    "\n",
    "\n",
    "def load_region_files(Npoly, Ttype, data_folder):\n",
    "\n",
    "    focal_mechanism_root_name    = 'MeanProb_BS4_FocMech_Reg'\n",
    "    pyptf_focal_mechanism_dir    = os.path.join(data_folder, 'FocMech_PreProc')\n",
    "    region_to_ignore             = [42]\n",
    "    # region_to_ignore             = list(map(int, region_to_ignore))\n",
    "    region_list                  = [x for x in range(Npoly) if x not in region_to_ignore]\n",
    "    \n",
    "    ModelsProb_Region_files      = dict()\n",
    "\n",
    "    ## Define Region with PS now in this DICTIONARY\n",
    "    region_ps_1                  = [3,24,44,48,49]\n",
    "    region_ps_2                  = [10,16,54]\n",
    "    region_ps_3                  = [27,33,35,36]\n",
    "    region_listPs                =  [-1 for x in range(Npoly) if x not in region_to_ignore]\n",
    "    for i in region_ps_1:\n",
    "        region_listPs[i-1] = 1\n",
    "    for i in region_ps_2:\n",
    "        region_listPs[i-1] = 2\n",
    "    for i in region_ps_3:\n",
    "        region_listPs[i-1] = 3\n",
    "    ModelsProb_Region_files['region_listPs'] = region_listPs\n",
    "\n",
    "    regions_without_bs_focal_mechanism = []\n",
    "    regions_with_bs_focal_mechanism    = []\n",
    "\n",
    "    for iReg in range(len(region_list)+1):\n",
    "        if (sum(itype for itype in Ttype[iReg] if itype == 1) > 0):\n",
    "            regions_with_bs_focal_mechanism.append(iReg)\n",
    "        else:\n",
    "            regions_without_bs_focal_mechanism.append(iReg)\n",
    "\n",
    "    ModelsProb_Region_files['ModelsProb_Region_files'] = []\n",
    "\n",
    "    # print(\"Loading MeanProb_BS4_FocMech dict: <------ {}\".format(focal_mechanism_root_name))\n",
    "    for iReg in regions_with_bs_focal_mechanism:\n",
    "\n",
    "        # define files\n",
    "        filename  = focal_mechanism_root_name + '{}'.format(str(iReg+1).zfill(3)) + '.npy'\n",
    "        f_FocMech = os.path.join(pyptf_focal_mechanism_dir,filename)\n",
    "        ModelsProb_Region_files['ModelsProb_Region_files'].append(f_FocMech)\n",
    "\n",
    "    for iReg in regions_without_bs_focal_mechanism:\n",
    "        filename  = focal_mechanism_root_name + '{}'.format(str(iReg+1).zfill(3)) + '.npy'\n",
    "        f_FocMech = os.path.join(pyptf_focal_mechanism_dir,filename)\n",
    "\n",
    "        ModelsProb_Region_files['ModelsProb_Region_files'].append(f_FocMech)\n",
    "\n",
    "    ModelsProb_Region_files['ModelsProb_Region_files'].sort()\n",
    "    ModelsProb_Region_files['regions_with_bs_focal_mechanism']    = regions_with_bs_focal_mechanism\n",
    "    ModelsProb_Region_files['regions_without_bs_focal_mechanism'] = regions_without_bs_focal_mechanism\n",
    "\n",
    "    return ModelsProb_Region_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c330275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_mix_utilities.py file\n",
    "def conversion_to_utm(long, ee, PSBa):\n",
    "\n",
    "    a = utm.from_latlon(np.array(long['Discretizations']['BS-2_Position']['Val_y']), np.array(long['Discretizations']['BS-2_Position']['Val_x']), ee['ee_utm'][2])\n",
    "    long['Discretizations']['BS-2_Position']['utm_y']   = a[1]\n",
    "    long['Discretizations']['BS-2_Position']['utm_x']   = a[0]\n",
    "    long['Discretizations']['BS-2_Position']['utm_nr']  = a[2]\n",
    "    long['Discretizations']['BS-2_Position']['utm_reg'] = a[3]\n",
    "\n",
    "    a = utm.from_latlon(np.array(long['Discretizations']['PS-2_PositionArea']['Val_y']), np.array(long['Discretizations']['PS-2_PositionArea']['Val_x']), ee['ee_utm'][2])\n",
    "    long['Discretizations']['PS-2_PositionArea']['utm_y']   = a[1]\n",
    "    long['Discretizations']['PS-2_PositionArea']['utm_x']   = a[0]\n",
    "    long['Discretizations']['PS-2_PositionArea']['utm_nr']  = a[2]\n",
    "    long['Discretizations']['PS-2_PositionArea']['utm_reg'] = a[3]\n",
    "\n",
    "    for i in range(len(PSBa['BarPSperModel'])):\n",
    "        for j in range(len(PSBa['BarPSperModel'][i])):\n",
    "            #print(type(PSBa['BarPSperModel'][i][j]['pos_yy']))\n",
    "            #sys.exit()\n",
    "            if PSBa['BarPSperModel'][i][j]['pos_yy'].size < 1:\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_lat'] = np.array([])\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_lon'] = np.array([])\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_nr'] = np.array([])\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_reg'] = np.array([])\n",
    "                pass\n",
    "            else:\n",
    "                a = utm.from_latlon(np.array(PSBa['BarPSperModel'][i][j]['pos_yy']), np.array(PSBa['BarPSperModel'][i][j]['pos_xx']), ee['ee_utm'][2])\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_lat'] = a[0]\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_lon'] = a[1]\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_nr']  = a[2]\n",
    "                PSBa['BarPSperModel'][i][j]['utm_pos_reg'] = a[3]\n",
    "\n",
    "\n",
    "    return long, PSBa\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def ray_tracing_method(x,y,poly):\n",
    "\n",
    "    n = len(poly)\n",
    "    inside = False\n",
    "\n",
    "    p1x,p1y = poly[0]\n",
    "    for i in range(n+1):\n",
    "        p2x,p2y = poly[i % n]\n",
    "        if y > min(p1y,p2y):\n",
    "            if y <= max(p1y,p2y):\n",
    "                if x <= max(p1x,p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\n",
    "                    if p1x == p2x or x <= xints:\n",
    "                        inside = not inside\n",
    "        p1x,p1y = p2x,p2y\n",
    "\n",
    "    return inside\n",
    "\n",
    "def NormMultiDvec(**kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    # Here mu and sigma, already inserted into ee dictionary\n",
    "    # Coordinates in utm\n",
    "    mu = tmpmu =PosMean_3D = [EarlyEst.lonUTM,EarlyEst.latUTM,EarlyEst.Dep*1.E3]\n",
    "    Sigma = tmpCOV = EarlyEst.PosCovMat_3D = [EarlyEst.PosSigmaXX EarlyEst.PosSigmaXY EarlyEst.PosSigmaXZ; ...\n",
    "                         EarlyEst.PosSigmaXY EarlyEst.PosSigmaYY EarlyEst.PosSigmaYZ; ...\n",
    "                         EarlyEst.PosSigmaXZ EarlyEst.PosSigmaYZ EarlyEst.PosSigmaZZ];\n",
    "    mu =     np.array([ee['lon'], ee['lat'], ee['depth']*1000.0])\n",
    "    sigma =  np.array([[ee['cov_matrix']['XX'], ee['cov_matrix']['XY'], ee['cov_matrix']['XZ']], \\\n",
    "                       [ee['cov_matrix']['XY'], ee['cov_matrix']['YY'], ee['cov_matrix']['YZ']], \\\n",
    "                       [ee['cov_matrix']['XZ'], ee['cov_matrix']['YZ'], ee['cov_matrix']['ZZ']]])\n",
    "    \"\"\"\n",
    "\n",
    "    x     = kwargs.get('x', None)\n",
    "    mu    = kwargs.get('mu', None)\n",
    "    sigma = kwargs.get('sigma', None)\n",
    "\n",
    "    n = len(mu)\n",
    "\n",
    "    #mu = np.reshape(mu,(3,1))\n",
    "    mu = np.reshape(mu,(n,1))\n",
    "    t1  = (2 * math.pi)**(-1*len(mu)/2)\n",
    "    t2  = 1 / math.sqrt(np.linalg.det(sigma))\n",
    "    #c1  = npm.repmat(mu, 1, np.shape(mu)[0])\n",
    "    c1  = npm.repmat(mu, 1, len(x))\n",
    "    c11 = (x - c1.transpose()).transpose()\n",
    "    c12 = x - c1.transpose()\n",
    "\n",
    "    d  = np.linalg.lstsq(sigma, c11, rcond=None)[0]\n",
    "    e = np.dot(c12, d)\n",
    "    f = np.multiply(-0.5,np.diag(e))\n",
    "    g = np.exp(f)\n",
    "    h = t1 * t2 * g\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9771e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_lambda_bsps_load.py\n",
    "def load_lambda_BSPS(sigma, ee_d, data_folder):\n",
    "\n",
    "    d = dict()\n",
    "\n",
    "    ## Variables for lambda\n",
    "    d['lambdaBSPS']                                = {}\n",
    "    d['lambdaBSPS']['hypo_utm']                    = np.array([ee_d['ee_utm'][0], \\\n",
    "                                                               ee_d['ee_utm'][1], \\\n",
    "                                                               ee_d['depth'] ])\n",
    "    d['lambdaBSPS']['utmzone_hypo']                = ee_d['ee_utm'][2]\n",
    "    d['lambdaBSPS']['NormCov']                     = ee_d['PosCovMat_3d']\n",
    "    d['lambdaBSPS']['confid_lev']                  = norm.cdf(sigma) - norm.cdf(-1 * sigma)\n",
    "    d['lambdaBSPS']['dchi2']                       = distributions.chi2.ppf(d['lambdaBSPS']['confid_lev'], 3)\n",
    "    d['lambdaBSPS']['SD']                          = math.sqrt(d['lambdaBSPS']['dchi2'])\n",
    "    d['lambdaBSPS']['mesh']                        = get_meshes(ee_d, data_folder)\n",
    "    d['lambdaBSPS']['covariance_epicenter_volume'] = get_cov_volume(ee_d['PosCovMat_3d'], d['lambdaBSPS']['SD'])\n",
    "    d['lambdaBSPS']['npts_mw']                     = get_npts_mw(d['lambdaBSPS']['covariance_epicenter_volume'])\n",
    "    d['lambdaBSPS']['gaussian_ellipsoid']          = get_gaussian_ellipsoid_3d(ee_d, ee_d['PosCovMat_3d'], d['lambdaBSPS']['SD'], d['lambdaBSPS']['npts_mw'])\n",
    "    d['lambdaBSPS']                                = get_gaussian_ellipsoid_tetraedons(d['lambdaBSPS'], ee_d)\n",
    "\n",
    "    return d['lambdaBSPS']\n",
    "\n",
    "def get_meshes(ee, data_folder):\n",
    "\n",
    "    path  =  os.path.join(data_folder, 'mesh_files')\n",
    "    faces = ['HA_mesh_faces_x16.dat', 'CA_mesh_faces_x16.dat', 'Cyprus_mesh_faces_x16.dat']\n",
    "    nodes = ['HA_mesh_nodes_x16.dat', 'CA_mesh_nodes_x16.dat', 'Cyprus_mesh_nodes_x16.dat']\n",
    "    names = [ 'HeA', 'CaA', 'CyA']\n",
    "\n",
    "    mesh_d = dict()\n",
    "\n",
    "    for i in range(len(faces)):\n",
    "\n",
    "        f = os.path.join(path, faces[i])\n",
    "        n = os.path.join(path, nodes[i])\n",
    "\n",
    "        mesh_name                      = names[i]\n",
    "        mesh_d[mesh_name]              = dict()\n",
    "        mesh_d[mesh_name][\"faces\"]     = np.loadtxt(f)\n",
    "        mesh_d[mesh_name][\"nodes\"]     = np.loadtxt(n)\n",
    "        mesh_d[mesh_name][\"nodes_utm\"] = utm.from_latlon(mesh_d[mesh_name][\"nodes\"][:, 1],\n",
    "                                                         mesh_d[mesh_name][\"nodes\"][:, 2],\n",
    "                                                         ee['ee_utm'][2])\n",
    "\n",
    "    return mesh_d\n",
    "\n",
    "def get_cov_volume(cov_matrix, std):\n",
    "\n",
    "    w, v = np.linalg.eig(cov_matrix)\n",
    "    l_major = std*np.sqrt(w[0]) * 1000.0\n",
    "    l_inter = std*np.sqrt(w[1]) * 1000.0\n",
    "    l_minor = std*np.sqrt(w[2]) * 1000.0\n",
    "\n",
    "    volume = (4./3.) * np.pi * l_major * l_inter * l_minor\n",
    "\n",
    "    return volume\n",
    "\n",
    "def get_npts_mw(volume):\n",
    "    \"\"\"\n",
    "    Calculate the number of points to define de ellipsoide.\n",
    "    Fitting function a*x**b found by F.Romano\n",
    "    \"\"\"\n",
    "\n",
    "    a         = 0.6211\n",
    "    b         = 0.4358\n",
    "    n_tetra   = 23382 \n",
    "    vol_tetra = 3.9990e+15\n",
    "\n",
    "    npts_mw   = np.ceil(a*(volume*n_tetra/vol_tetra)**b).astype(int)\n",
    "    npts_mw   = max(10, npts_mw)\n",
    "\n",
    "    return npts_mw\n",
    "\n",
    "def get_gaussian_ellipsoid_3d(ee, cov, std, npts): \n",
    "\n",
    "    center = [ee['ee_utm'][0], ee['ee_utm'][1], ee['depth']*-1000.0]\n",
    "\n",
    "    cov = cov*1e6\n",
    "    w, v = np.linalg.eigh(cov)\n",
    "    if np.any(w < 0):\n",
    "        print('Warning: negative eigenvalues')\n",
    "        w = max(w,0)\n",
    "    w = std * np.sqrt(w)    #get std of the cov matrix\n",
    "\n",
    "    volume = (4./3.) * np.pi * w[0] * w[1] * w[2]\n",
    "\n",
    "    # Make 3x 11x11 arrays\n",
    "    x, y, z = create_sphere(npts)\n",
    "\n",
    "    x = np.transpose(x)\n",
    "    y = np.transpose(y)\n",
    "    z = np.transpose(z)\n",
    "\n",
    "    # Flattern 11x11 array\n",
    "    ap = np.array([np.ravel(x), np.ravel(y), np.ravel(z)])\n",
    "\n",
    "    bp = np.dot(np.dot(v, np.diag(w)), ap) +  \\\n",
    "         np.transpose(np.tile(center, (np.shape(ap)[1], 1)))\n",
    "\n",
    "    xp = np.reshape(bp[0, :], np.shape(x))\n",
    "    yp = np.reshape(bp[1, :], np.shape(y))\n",
    "    zp = np.reshape(bp[2, :], np.shape(z))\n",
    "\n",
    "    ellipsoid = {'xp':xp, 'yp':yp, 'zp':zp, 'vol': volume}\n",
    "    print(\" --> Volume of the Gaussian Ellipsoid: {:.8e} [m^3]\".format(volume))\n",
    "\n",
    "    return ellipsoid\n",
    "\n",
    "def create_sphere(n_points=None, radius=None):\n",
    "    \"\"\"\n",
    "    Create a discrete 3D spheric surface (points)\n",
    "    Reference to create the shere:\n",
    "       https://it.mathworks.com/matlabcentral/answers/48240-surface-of-a-equation:\n",
    "       n = 100;\n",
    "        r = 1.5;\n",
    "        theta = (-n:2:n)/n*pi;\n",
    "        phi = (-n:2:n)'/n*pi/2;\n",
    "        cosphi = cos(phi); cosphi(1) = 0; cosphi(n+1) = 0;\n",
    "        sintheta = sin(theta); sintheta(1) = 0; sintheta(n+1) = 0;\n",
    "        x = r*cosphi*cos(theta);\n",
    "        y = r*cosphi*sintheta;\n",
    "        z = r*sin(phi)*ones(1,n+1);\n",
    "        surf(x,y,z)\n",
    "        xlabel('X'); ylabel('Y'); zlabel('Z')\n",
    "    \"\"\"\n",
    "    if radius is None:\n",
    "        radius = 1.0\n",
    "\n",
    "    if n_points is None:\n",
    "        n_points = 20\n",
    "\n",
    "    theta = np.matrix(np.arange(-1*n_points,n_points+1,2) / n_points * np.pi)\n",
    "    phi   = np.matrix(np.arange(-1*n_points,n_points+1,2) / n_points * np.pi / 2)\n",
    "    phi   = phi.transpose()\n",
    "\n",
    "    X = radius*np.matmul(np.cos(phi),np.cos(theta))\n",
    "    Y = radius*np.matmul(np.cos(phi),np.sin(theta))\n",
    "    Z = radius*np.matmul(np.sin(phi),np.matrix(np.ones(11)))\n",
    "\n",
    "    # Set to 0 the very small numbers\n",
    "    X[0] = 0\n",
    "    X[-1] = 0\n",
    "    Y[0] = 0\n",
    "    Y[-1] = 0\n",
    "    Y[:,0] = 0\n",
    "    Y[:,-1] = 0\n",
    "\n",
    "    return X,Y,Z\n",
    "\n",
    "def get_gaussian_ellipsoid_tetraedons(el, ee):\n",
    "    \"\"\"\n",
    "    From R. Tonini\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    xp = el['gaussian_ellipsoid']['xp']\n",
    "    yp = el['gaussian_ellipsoid']['yp']\n",
    "    zp = el['gaussian_ellipsoid']['zp'] #*-1.0\n",
    "\n",
    "    # array preparation for creating tetrahedrons\n",
    "    # This is like sss on matptf\n",
    "    sss = np.vstack([xp.flatten(), yp.flatten(), zp.flatten()]).transpose()\n",
    "    points_xyz = np.unique(sss, axis=0)\n",
    "    n_points, tmp = np.shape(points_xyz)\n",
    "\n",
    "    # lon lat conversion\n",
    "    points_ll = np.zeros((n_points, 3))\n",
    "    for i in range(n_points):\n",
    "        points_ll[i, [1, 0]] = utm.to_latlon(points_xyz[i, 0],\n",
    "                                             points_xyz[i, 1],\n",
    "                                             ee['ee_utm'][2], ee['ee_utm'][3])\n",
    "\n",
    "    points_ll[:, 2] = points_xyz[:, 2]\n",
    "\n",
    "    # Good but slower (0.0030319690704345703 <=> 0.0013072490692138672)\n",
    "    # from pyhull.delaunay import DelaunayTri\n",
    "    # tetrahedrons = np.asarray(DelaunayTri(points_ll).vertices)\n",
    "    # tetrahedron discretization (based on the points on the surface)\n",
    "    tessellation = scipy.spatial.Delaunay(points_ll)\n",
    "    tetrahedrons = tessellation.simplices\n",
    "\n",
    "    # computing barycenters\n",
    "    tetra_bar          = {}\n",
    "    tetra_bar[\"utm_x\"] = np.mean(points_xyz[tetrahedrons, 0], axis=1)\n",
    "    tetra_bar[\"utm_y\"] = np.mean(points_xyz[tetrahedrons, 1], axis=1)\n",
    "    tetra_bar[\"lon\"]   = np.mean(points_ll[tetrahedrons, 0], axis=1)\n",
    "    tetra_bar[\"lat\"]   = np.mean(points_ll[tetrahedrons, 1], axis=1)\n",
    "    tetra_bar[\"depth\"] = np.mean(points_ll[tetrahedrons, 2], axis=1)\n",
    "\n",
    "    tetra_xyz = np.column_stack((tetra_bar[\"utm_x\"],\n",
    "                                 tetra_bar[\"utm_y\"],\n",
    "                                 tetra_bar[\"depth\"]))\n",
    "\n",
    "    n_tetra = len(tetra_bar[\"lon\"])\n",
    "    print(\" --> N. Tetra in the Gaussian Ellipsoid: {0}\".format(n_tetra))\n",
    "\n",
    "    # computing tetrahedrons volume\n",
    "    volume = np.zeros((n_tetra))\n",
    "    for i in range(n_tetra):\n",
    "        mm = np.column_stack((points_xyz[tetrahedrons[i, :], :],\n",
    "                              np.array([1, 1, 1, 1])))\n",
    "        volume[i] = np.abs(np.linalg.det(mm)/6.)\n",
    "\n",
    "    volume_tot = np.sum(volume)\n",
    "    print(\" --> Volume of Tetra in the Gaussian Ellipsoid: %.8e [m^3]\" % volume_tot)\n",
    "\n",
    "    Vol_diff_perc = (el['gaussian_ellipsoid']['vol'] - volume_tot) / el['gaussian_ellipsoid']['vol']*100\n",
    "    print(\" --> Volume difference Gaussian <--> Tetra: %.2f [%%]\" % Vol_diff_perc)\n",
    "\n",
    "    el['tetra_bar']                 = tetra_bar\n",
    "    el['tetrahedrons']              = tetrahedrons\n",
    "    el['gaussian_ellipsoid_volume'] = volume_tot\n",
    "    el['volumes_elements']          = volume\n",
    "    el['tetra_xyz']                 = tetra_xyz\n",
    "\n",
    "    return el    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b14e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_lambda_bsps_sep.py file\n",
    "\n",
    "def separation_lambda_BSPS(ee, lambda_bsps, LongTerm, mesh):\n",
    "\n",
    "    moho_ll  = np.column_stack((LongTerm['Discretizations']['BS-2_Position']['Val_x'], LongTerm['Discretizations']['BS-2_Position']['Val_y']))\n",
    "    tetra_ll = np.column_stack((lambda_bsps['tetra_bar']['lon'], lambda_bsps['tetra_bar']['lat']))\n",
    "\n",
    "    bar_depth_moho = scipy.interpolate.griddata(moho_ll,\n",
    "                                                LongTerm['Discretizations']['BS-2_Position']['DepthMoho'],\n",
    "                                                tetra_ll)\n",
    "\n",
    "    print(' --> Distance between tetra and slabs:')\n",
    "    mesh = find_tetra_index_for_ps_and_bs(ee, lambda_bsps, mesh, bar_depth_moho, LongTerm['Discretizations']['BS-2_Position']['grid_moho'])\n",
    "\n",
    "    lambda_bsps = compute_ps_bs_gaussians_general(ee, lambda_bsps, mesh, bar_depth_moho)\n",
    "\n",
    "    lambda_bsps = compute_ps_bs_gaussians_single_zone(ee, lambda_bsps, mesh)\n",
    "\n",
    "    lambda_bsps = update_lambda_bsps_dict(lambda_bsps, LongTerm['Regionalization'])\n",
    "\n",
    "    return lambda_bsps\n",
    "\n",
    "def find_tetra_index_for_ps_and_bs(ee, lambda_bsps, mesh, moho, grid_moho):\n",
    "\n",
    "    buffer = 10000\n",
    "    tt = np.empty((0,3))\n",
    "\n",
    "    # here make 1 grid moho in utm\n",
    "    grid_moho_utm = utm.from_latlon(grid_moho[:,1], grid_moho[:,0], ee['ee_utm'][2])\n",
    "    grid_moho     = np.column_stack((grid_moho_utm[0].transpose(), grid_moho_utm[1].transpose(), (1000*grid_moho[:,2]).transpose()))\n",
    "\n",
    "    for keys in mesh:\n",
    "        # Convert lat lon to utm for the baricenter\n",
    "        mesh[keys]['bari']['utm'] = utm.from_latlon(mesh[keys]['bari']['lat'], mesh[keys]['bari']['lon'], ee['ee_utm'][2])\n",
    "\n",
    "        tmp_mesh = np.column_stack((mesh[keys]['bari']['utm'][0].transpose(), \\\n",
    "                                    mesh[keys]['bari']['utm'][1].transpose(), \\\n",
    "                                    mesh[keys]['bari']['depth'].transpose()))\n",
    "        tt_mesh  = np.concatenate((tt, tmp_mesh))\n",
    "\n",
    "        mesh[keys]['d_dist'] = find_distances_tetra_mesh(tt_mesh, lambda_bsps['tetra_xyz'], buffer, moho, grid_moho)\n",
    "        \n",
    "        print('     --> Min distance from slab %s %10.3f [km]' % (mesh[keys]['name'], mesh[keys]['d_dist']['distance_min_value']/1000))\n",
    "        print('         --> Nr of PS tetra with dist.  < %4.1f [km] from slab %s : %d  (effective: %d)' % \\\n",
    "              (buffer/1000, mesh[keys]['name'], len(mesh[keys]['d_dist']['idx_less_then_buffer']), len(mesh[keys]['d_dist']['idx_less_then_buffer_effective'])))\n",
    "        print('         --> Nr of BS tetra with dist. >= %4.1f [km] from slab %s : %d  (effective: %d)' % \\\n",
    "              (buffer/1000, mesh[keys]['name'], len(mesh[keys]['d_dist']['idx_more_then_buffer']), len(mesh[keys]['d_dist']['idx_more_then_buffer_effective'])))\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def find_distances_tetra_mesh(mesh, tetra, buffer, moho, g_moho):\n",
    "\n",
    "    d = dict()\n",
    "\n",
    "    dist   = np.zeros(len(tetra))\n",
    "    m_dist = np.zeros(len(tetra))\n",
    "\n",
    "    for i in range(len(tetra)):\n",
    "        dist[i]   = np.amin(np.linalg.norm(mesh - tetra[i], axis=1))\n",
    "        m_dist[i] = np.amin(np.linalg.norm(g_moho - tetra[i], axis=1))\n",
    "\n",
    "    # Check if all below moho\n",
    "    d['tetra_in_moho']        = True #default\n",
    "\n",
    "    # Minimal distance\n",
    "    d['distances_mesh_tetra'] = dist\n",
    "    d['distance_min_value']   = np.amin(dist)\n",
    "    d['distance_min_idx']     = np.argmin(dist)\n",
    "    d['moho_d_mesh_tetra']    = m_dist\n",
    "    d['moho_d_min_value']     = np.amin(m_dist)\n",
    "    d['moho_d_min_idx']       = np.argmin(m_dist)\n",
    "\n",
    "\n",
    "    # All distances min than buffer\n",
    "    d['idx_less_then_buffer'] = np.where(dist <= buffer)[0]\n",
    "    d['idx_more_then_buffer'] = np.where(dist >  buffer)[0]\n",
    "\n",
    "    ## Questa parte qui general 'errore'!!!!\n",
    "    # select all indx below the surface for the ones into the slab (PS)\n",
    "    tmp_tetra = np.take(tetra[:,2],d['idx_less_then_buffer'])\n",
    "    d['tmp']  = np.where(tmp_tetra <= 0)[0]\n",
    "    d['idx_less_then_buffer_effective'] = d['idx_less_then_buffer'][d['tmp']]\n",
    "\n",
    "    # select all indx below the surface for the ones outside the slab (BS)\n",
    "    tmp_tetra = np.take(tetra[:,2],d['idx_more_then_buffer'])\n",
    "    d['tmp']  = np.where(tmp_tetra <= 0)[0]\n",
    "    d['idx_more_then_buffer_effective'] = d['idx_more_then_buffer'][d['tmp']]\n",
    "\n",
    "    tmp_tetra = np.take(tetra[:,2],d['idx_more_then_buffer_effective'])\n",
    "    tmp_moho  = np.take(moho, d['idx_more_then_buffer_effective'])\n",
    "    d['tmp']  = np.where((tmp_moho -1*tmp_tetra/1000) <=0)\n",
    "    d['idx_more_then_buffer_effective'] = d['idx_more_then_buffer_effective'][d['tmp']]\n",
    "\n",
    "    # Check if this is in moho\n",
    "    if(len(d['idx_more_then_buffer_effective']) == 0):\n",
    "        d['tetra_in_moho'] = False\n",
    "\n",
    "    return d\n",
    "\n",
    "def compute_ps_bs_gaussians_general(ee, lambda_bsps, mesh, bar_depth_moho):\n",
    "\n",
    "    tetra = lambda_bsps['tetra_xyz']\n",
    "    vol = lambda_bsps['volumes_elements']\n",
    "\n",
    "    hx             = ee['ee_utm'][0]\n",
    "    hy             = ee['ee_utm'][1]\n",
    "    hz             = ee['depth']* (1000.0)\n",
    "    covariance     = ee['PosCovMat_3dm']\n",
    "    xyz            = np.array([hx, hy, hz])\n",
    "\n",
    "    # first merge index\n",
    "    ps_idx    = []\n",
    "    bs_idx    = []\n",
    "    bs_ps_idx = []\n",
    "    gauss_ps_eff = np.array([])\n",
    "    gauss_bs_eff = np.array([])\n",
    "\n",
    "    #distances min\n",
    "    min_d_mesh = sys.float_info.max\n",
    "    min_d_moho = sys.float_info.max\n",
    "\n",
    "    n_tetra, _ = tetra.shape\n",
    "\n",
    "    for keys in mesh:\n",
    "        ps_idx.extend((mesh[keys]['d_dist']['idx_less_then_buffer_effective']).tolist())\n",
    "        if (mesh[keys]['d_dist']['moho_d_min_value'] <= min_d_moho):\n",
    "            min_d_moho = mesh[keys]['d_dist']['moho_d_min_value']\n",
    "        if (mesh[keys]['d_dist']['distance_min_value'] <= min_d_mesh):\n",
    "            min_d_mesh = mesh[keys]['d_dist']['distance_min_value']\n",
    "\n",
    "    ps_idx = np.array(list(set(ps_idx))).astype(int)\n",
    "    bs_idx = np.setdiff1d(np.arange(n_tetra,dtype=int),ps_idx)\n",
    "\n",
    "    tetra_dep = tetra[:,2] / 1000.\n",
    "    bs_idx_moho = np.where((tetra_dep < 0) & (bar_depth_moho-tetra_dep <= 0))[0]\n",
    "    bs_idx = np.intersect1d(bs_idx, bs_idx_moho);\n",
    "    bs_ps_idx = np.concatenate((ps_idx, bs_idx))\n",
    "\n",
    "    ps_tetra = tetra[ps_idx]\n",
    "    bs_tetra = tetra[bs_idx]\n",
    "\n",
    "    bs_ps_tetra = tetra[bs_ps_idx]\n",
    "\n",
    "    ps_tetra[:,2]    = ps_tetra[:,2]* -1\n",
    "    bs_tetra[:,2]    = bs_tetra[:,2]* -1\n",
    "    bs_ps_tetra[:,2] = bs_ps_tetra[:,2]* -1\n",
    "\n",
    "    gauss_ps_eff = scipy.stats.multivariate_normal.pdf(ps_tetra, xyz, covariance)\n",
    "    gauss_bs_eff = scipy.stats.multivariate_normal.pdf(bs_tetra, xyz, covariance)\n",
    "    gauss_bs_ps_eff = scipy.stats.multivariate_normal.pdf(bs_ps_tetra, xyz, covariance)\n",
    "\n",
    "    sum_bs_ps = np.sum(np.multiply(gauss_bs_ps_eff,vol[bs_ps_idx]))\n",
    "\n",
    "    lambda_ps = np.sum(np.multiply(gauss_ps_eff,vol[ps_idx])) / sum_bs_ps\n",
    "    lambda_bs = np.sum(np.multiply(gauss_bs_eff,vol[bs_idx])) / sum_bs_ps\n",
    "\n",
    "    lambda_bsps['lambda_ps']  = lambda_ps\n",
    "    lambda_bsps['lambda_bs']  = lambda_bs\n",
    "    lambda_bsps['gauss_ps']   = gauss_ps_eff\n",
    "    lambda_bsps['gauss_bs']   = gauss_bs_eff\n",
    "\n",
    "    print(\" --> lambda PS: {:6.4e}  Volume PS: {:10.4e} [m^3]\".format(lambda_ps, np.sum(vol[ps_idx])))\n",
    "    print(\" --> lambda BS: {:6.4e}  Volume BS: {:10.4e} [m^3]\".format(lambda_bs, np.sum(vol[bs_idx])))\n",
    "    print(\" -->                     Volume BS-PS: {:10.4e} [m^3]\".format(np.sum(vol[bs_ps_idx])))\n",
    "\n",
    "    return lambda_bsps\n",
    "\n",
    "def compute_ps_bs_gaussians_single_zone(ee, lambda_bsps, mesh):\n",
    "\n",
    "    vol = lambda_bsps['volumes_elements']\n",
    "    tetra  = lambda_bsps['tetra_xyz']\n",
    "    \n",
    "    hx             = ee['ee_utm'][0]\n",
    "    hy             = ee['ee_utm'][1]\n",
    "    hz             = ee['depth']* (1000.0)\n",
    "    covariance     = ee['PosCovMat_3dm']\n",
    "    xyz            = np.array([hx, hy, hz])\n",
    "    lambda_ps_sub  = []\n",
    "\n",
    "    if (lambda_bsps['lambda_ps'] == 0):\n",
    "\n",
    "        lambda_bsps['lambda_ps_sub']       = [0,0,0]\n",
    "        lambda_bsps['lambda_ps_on_ps_tot'] = [0,0,0] # Fixed for lambda_mix == False (PS == 0)\n",
    "        return lambda_bsps\n",
    "\n",
    "    for keys in mesh:\n",
    "\n",
    "        # first merge index\n",
    "        ps_idx    = []\n",
    "        bs_idx    = []\n",
    "\n",
    "        # first Compute general PS-BS\n",
    "        #for keys in mesh:\n",
    "        ps_idx.extend((mesh[keys]['d_dist']['idx_less_then_buffer_effective']).tolist())\n",
    "        bs_idx.extend((mesh[keys]['d_dist']['idx_more_then_buffer_effective']).tolist())\n",
    "        if(len(ps_idx) == 0):\n",
    "            lambda_ps = 0.0\n",
    "            lambda_ps_sub.append(lambda_ps)\n",
    "            print(\"     --> Single {} lambda PS: {:6.4e} Volume ps: {:10.4e} [m^3]\".format(mesh[keys]['name'], lambda_ps, np.sum(vol[ps_idx])))\n",
    "\n",
    "        else:\n",
    "            ps_idx    = set(ps_idx)\n",
    "            ps_idx    = np.array(list(ps_idx))\n",
    "            ps_tetra    = tetra[ps_idx]\n",
    "            ps_tetra[:,2]    = ps_tetra[:,2]* -1\n",
    "            gauss_ps_eff     = scipy.stats.multivariate_normal.pdf(ps_tetra, xyz, covariance)\n",
    "\n",
    "            sum_ps    = np.sum(np.multiply(gauss_ps_eff,vol[ps_idx]))\n",
    "            lambda_ps = (np.sum(np.multiply(gauss_ps_eff,vol[ps_idx])) / sum_ps) * lambda_bsps['lambda_ps']\n",
    "            lambda_ps_sub.append(lambda_ps)\n",
    "\n",
    "            print(\"     --> Single {} lambda PS: {:6.4e} Volume ps: {:10.4e} [m^3]\".format(mesh[keys]['name'], lambda_ps, np.sum(vol[ps_idx])))\n",
    "\n",
    "    lambda_bsps['lambda_ps_sub'] = lambda_ps_sub\n",
    "\n",
    "    # Define LambdsPs for each reagion on total lambda PS\n",
    "    lambda_bsps['lambda_ps_on_ps_tot'] =  np.array(lambda_bsps['lambda_ps_sub']) / lambda_bsps['lambda_ps']\n",
    "\n",
    "    return lambda_bsps\n",
    "\n",
    "def update_lambda_bsps_dict(lambda_bsps, Regionalization):\n",
    "\n",
    "    mesh_zones = {'0':'[2,23,43,47,48]', '1':'[9,15,53]', '2':'[26,32,34,35]'}\n",
    "\n",
    "    regionsPerPS    = np.empty(Regionalization['Npoly'])\n",
    "    regionsPerPS[:] = np.nan\n",
    "\n",
    "    for key in mesh_zones:\n",
    "        l = ast.literal_eval(mesh_zones[key])\n",
    "        regionsPerPS[l] = int(key)\n",
    "\n",
    "    lambda_bsps['regionsPerPS'] = regionsPerPS\n",
    "\n",
    "    return lambda_bsps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea6cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_pre_selection.py file\n",
    "\n",
    "def pre_selection_of_scenarios(sigma, ee, LongTermInfo, PSBarInfo, ellipses):\n",
    "\n",
    "    pre_selection = dict()\n",
    "    pre_selection = pre_selection_magnitudes(sigma, ee, pre_selection, LongTermInfo['Discretizations']['BS-1_Magnitude'], LongTermInfo['Discretizations']['PS-1_Magnitude'])\n",
    "\n",
    "    if (pre_selection['BS_scenarios'] == False and pre_selection['PS_scenarios'] == False):\n",
    "        print(\" --> WARNING: Event magnitude out of the discretization used in PTF. Apply Decision Matrix\")\n",
    "        return False\n",
    "    \n",
    "    if (pre_selection['BS_scenarios'] == True):\n",
    "        pre_selection = pre_selection_BS2_position(pre_selection, LongTermInfo['Discretizations']['BS-2_Position'], ellipses['location_ellipse_2d_BS_inn'], ellipses['location_ellipse_2d_BS_out'])\n",
    "    else:\n",
    "        pre_selection['BS2_Position_Selection_inn'] = np.array([])\n",
    "\n",
    "    if (pre_selection['PS_scenarios'] == True):\n",
    "        pre_selection = pre_selection_PS2_position(pre_selection, LongTermInfo['Discretizations']['PS-2_PositionArea'], ellipses['location_ellipse_2d_PS_inn'], ellipses['location_ellipse_2d_PS_out'])\n",
    "\n",
    "        pre_selection = pre_selection_Bar_PS_Model(pre_selection, PSBarInfo['BarPSperModel'], ellipses['location_ellipse_2d_PS_inn'])\n",
    "\n",
    "\n",
    "    return pre_selection\n",
    "\n",
    "def pre_selection_magnitudes(sigma, ee, pre_selection, BS_mag, PS_mag):\n",
    "\n",
    "    val_PS = np.array(PS_mag['Val'])\n",
    "    ID_PS  = list(PS_mag['ID'])\n",
    "    val_BS = np.array(BS_mag['Val'])\n",
    "    ID_BS  = list(BS_mag['ID'])\n",
    "\n",
    "    # Magnitude range given by sigma\n",
    "    min_mag = ee['mag'] - ee['MagSigma'] * sigma\n",
    "    max_mag = ee['mag'] + ee['MagSigma'] * sigma\n",
    "\n",
    "    # PS\n",
    "    if(max_mag <= val_PS[0]):\n",
    "        pre_selection['PS_scenarios'] = False\n",
    "        sel_PS_Mag_val = np.array([])\n",
    "        sel_PS_Mag_idx = (np.array([]),)\n",
    "        sel_PS_Mag_IDs = []\n",
    "\n",
    "    elif(min_mag >= val_PS[-1]):\n",
    "        pre_selection['PS_scenarios'] = True\n",
    "        sel_PS_Mag_val = np.array([val_PS[-1]])\n",
    "        sel_PS_Mag_idx = (np.where(val_PS[-1]),)\n",
    "        sel_PS_Mag_IDs = [ID_PS[-1]]\n",
    "\n",
    "    else:\n",
    "        pre_selection['PS_scenarios'] = True\n",
    "        sel_PS_Mag_val = val_PS[(val_PS >= min_mag) & (val_PS <= max_mag)]\n",
    "        sel_PS_Mag_idx = np.where((val_PS >= min_mag) & (val_PS <= max_mag))\n",
    "        # To fix if mag uncertainty is too small for val_PS element intervals\n",
    "        # Find closest magnitude\n",
    "        if(len(sel_PS_Mag_idx[0]) == 0):\n",
    "            idx = np.array((np.abs(val_PS-max_mag)).argmin())\n",
    "            sel_PS_Mag_val = np.array([val_PS[idx]])\n",
    "            sel_PS_Mag_idx = (np.array[idx],)\n",
    "            sel_PS_Mag_IDs = list(itemgetter(idx)(ID_PS))\n",
    "        else:\n",
    "            sel_PS_Mag_IDs = list(itemgetter(*sel_PS_Mag_idx[0])(ID_PS))\n",
    "\n",
    "    # BS\n",
    "    if(max_mag <= val_BS[0]):\n",
    "        pre_selection['BS_scenarios'] = False\n",
    "        sel_BS_Mag_val = np.array([])\n",
    "        sel_BS_Mag_idx = (np.array([]),)\n",
    "        sel_BS_Mag_IDs = []\n",
    "\n",
    "    elif(min_mag >= val_BS[-1]):\n",
    "        # sel_BS_Mag_val = np.array([val_BS[-1]])\n",
    "        pre_selection['BS_scenarios'] = False\n",
    "        sel_BS_Mag_val = np.array([])\n",
    "        sel_BS_Mag_idx = (np.array([]),)\n",
    "        sel_BS_Mag_IDs = []\n",
    "\n",
    "    else:\n",
    "        pre_selection['BS_scenarios'] = True\n",
    "        sel_BS_Mag_val = val_BS[(val_BS >= min_mag) & (val_BS <= max_mag)]\n",
    "        sel_BS_Mag_idx = np.where((val_BS >= min_mag) & (val_BS <= max_mag))\n",
    "        # To fix if mag uncertainty is too small for val_BS element intervals\n",
    "        # Find closest magnitude\n",
    "        if(len(sel_BS_Mag_idx[0]) == 0):\n",
    "            idx = np.array((np.abs(val_BS-max_mag)).argmin())\n",
    "            sel_BS_Mag_val = np.array([val_BS[idx]])\n",
    "            sel_BS_Mag_idx = (np.array[idx],)\n",
    "            sel_BS_Mag_IDs = list(itemgetter(idx)(ID_BS))\n",
    "        else:\n",
    "            sel_BS_Mag_IDs = list(itemgetter(*sel_BS_Mag_idx[0])(ID_BS))\n",
    "\n",
    "    pre_selection['sel_PS_Mag_val'] = sel_PS_Mag_val\n",
    "    pre_selection['sel_PS_Mag_idx'] = sel_PS_Mag_idx\n",
    "    pre_selection['sel_PS_Mag_IDs'] = sel_PS_Mag_IDs\n",
    "    pre_selection['sel_BS_Mag_val'] = sel_BS_Mag_val\n",
    "    pre_selection['sel_BS_Mag_idx'] = sel_BS_Mag_idx\n",
    "    pre_selection['sel_BS_Mag_IDs'] = sel_BS_Mag_IDs\n",
    "\n",
    "    print(\" --> BS magnitude values: {}\".format(sel_BS_Mag_val))\n",
    "    print(\" --> PS magnitude values: {}\".format(sel_PS_Mag_val))\n",
    "\n",
    "    return pre_selection\n",
    "\n",
    "def pre_selection_BS2_position(pre_selection, BS2_pos, ellipse_2d_inn, ellipse_2d_out):\n",
    "    \"\"\"\n",
    "    This function uses a ray tracing method decorated with cumba\n",
    "    \"\"\"\n",
    "\n",
    "    # https://stackoverflow.com/questions/36399381/whats-the-fastest-way-of-checking-if-a-point-is-inside-a-polygon-in-python\n",
    "    points     = zip(BS2_pos['utm_y'], BS2_pos['utm_x'])\n",
    "    inside_inn = [ray_tracing_method(point[0], point[1], ellipse_2d_inn) for point in points]\n",
    "\n",
    "    points     = zip(BS2_pos['utm_y'], BS2_pos['utm_x'])\n",
    "    inside_out = [ray_tracing_method(point[0], point[1], ellipse_2d_out) for point in points]\n",
    "\n",
    "    # Map common indices\n",
    "    # bool_array       = np.in1d(np.where(inside_out)[0], np.where(inside_inn)[0])\n",
    "    bool_array       = np.isin(np.where(inside_out)[0], np.where(inside_inn)[0])\n",
    "    common_positions = np.where(bool_array)[0]\n",
    "\n",
    "    # fill dictionary\n",
    "    pre_selection['BS2_Position_Selection_inn']    = np.where(inside_inn)[0]\n",
    "    pre_selection['BS2_Position_Selection_out']    = np.where(inside_out)[0]\n",
    "    pre_selection['BS2_Position_Selection_common'] = np.take(pre_selection['BS2_Position_Selection_out'],common_positions)\n",
    "\n",
    "    print(\" --> BS2_Position inner: {:4d} positions found\".format(len(pre_selection['BS2_Position_Selection_inn'])))\n",
    "    print(\" --> BS2_Position outer: {:4d} positions found\".format(len(pre_selection['BS2_Position_Selection_out'])))\n",
    "\n",
    "    # if len(pre_selection['BS2_Position_Selection_inn']) == 0 and len(pre_selection['BS2_Position_Selection_out']) != 0:\n",
    "    if len(pre_selection['BS2_Position_Selection_inn']) == 0:\n",
    "        pre_selection['BS_scenarios'] = False\n",
    "        print('Warning: BS are excluded since no scenarios are found in the inner positions (within sigma_inn)')\n",
    "\n",
    "    return pre_selection\n",
    "\n",
    "def pre_selection_PS2_position(pre_selection, PS2_pos, ellipse_2d_inn, ellipse_2d_out):\n",
    "    \"\"\"\n",
    "    This function uses a ray tracing method decorated with cumba\n",
    "    \"\"\"\n",
    "\n",
    "    # https://stackoverflow.com/questions/36399381/whats-the-fastest-way-of-checking-if-a-point-is-inside-a-polygon-in-python\n",
    "    points     = zip(PS2_pos['utm_y'], PS2_pos['utm_x'])\n",
    "    inside_inn = [ray_tracing_method(point[0], point[1], ellipse_2d_inn) for point in points]\n",
    "\n",
    "    points     = zip(PS2_pos['utm_y'], PS2_pos['utm_x'])\n",
    "    inside_out = [ray_tracing_method(point[0], point[1], ellipse_2d_out) for point in points]\n",
    "\n",
    "    # Map common indices\n",
    "    # bool_array       = np.in1d(np.where(inside_out)[0], np.where(inside_inn)[0])\n",
    "    bool_array       = np.isin(np.where(inside_out)[0], np.where(inside_inn)[0])\n",
    "    common_positions = np.where(bool_array)[0]\n",
    "\n",
    "    # fill dictionary\n",
    "    pre_selection['PS2_Position_Selection_inn']    = np.where(inside_inn)[0]\n",
    "    pre_selection['PS2_Position_Selection_out']    = np.where(inside_out)[0]\n",
    "    pre_selection['PS2_Position_Selection_common'] = np.take(pre_selection['PS2_Position_Selection_out'],common_positions)\n",
    "\n",
    "    print(\" --> PS2_Position inner: {:4d} positions found\".format(len(pre_selection['PS2_Position_Selection_inn'])))\n",
    "    print(\" --> PS2_Position outer: {:4d} positions found\".format(len(pre_selection['PS2_Position_Selection_out'])))\n",
    "\n",
    "    if len(pre_selection['PS2_Position_Selection_inn']) == 0:\n",
    "        pre_selection['PS_scenarios'] = False\n",
    "        print('Warning: PS are excluded since no scenarios are found in the inner positions (within sigma_inn)')\n",
    "\n",
    "    return pre_selection\n",
    "\n",
    "def pre_selection_Bar_PS_Model(pre_selection, BarPSperModel, ellipse_2d_inn):\n",
    "    \"\"\"\n",
    "    This function uses a ray tracing method decorated with numba\n",
    "    \"\"\"\n",
    "\n",
    "    Selected_PS_Mag_idx = pre_selection['sel_PS_Mag_idx'][0]\n",
    "\n",
    "    test_dict = dict()\n",
    "\n",
    "    for i1 in range(len(Selected_PS_Mag_idx)):\n",
    "        imag = Selected_PS_Mag_idx[i1]\n",
    "        for imod in range(len(BarPSperModel[imag])):\n",
    "            if('utm_pos_lat' in BarPSperModel[imag][imod]):\n",
    "                if(BarPSperModel[imag][imod]['utm_pos_lat'].size >=2):\n",
    "                    points     = zip(BarPSperModel[imag][imod]['utm_pos_lon'], BarPSperModel[imag][imod]['utm_pos_lat'])\n",
    "                    inside_inn = [ray_tracing_method(point[0], point[1], ellipse_2d_inn) for point in points]\n",
    "                elif(BarPSperModel[imag][imod]['utm_pos_lat'].size ==1 ):\n",
    "                    inside_inn = ray_tracing_method(BarPSperModel[imag][imod]['utm_pos_lon'][0], BarPSperModel[imag][imod]['utm_pos_lat'][0], ellipse_2d_inn)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                Inside_in_BarPSperModel = {'inside' : np.where(inside_inn)[0]}\n",
    "                test_dict.setdefault(imag, {})[imod] = Inside_in_BarPSperModel\n",
    "\n",
    "    pre_selection['Inside_in_BarPSperModel'] = test_dict\n",
    "\n",
    "    return pre_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e4abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_short_term.py file\n",
    "\n",
    "def short_term_probability_distribution(ee, negl_prob, LongTermInfo, PSBarInfo, lambda_bsps, pre_selection):\n",
    "\n",
    "    short_term = dict()\n",
    "    short_term['DepProbPoints'] = dict() \n",
    "    short_term['DepProbTemps'] = dict()  \n",
    "    short_term['DepProbScenes'] = dict() \n",
    "\n",
    "    x = np.size(pre_selection['sel_BS_Mag_val'])\n",
    "    y = np.size(pre_selection['BS2_Position_Selection_inn'])\n",
    "    short_term['DepProbScenesN'] = np.zeros((x,y))\n",
    "\n",
    "    x = np.size(LongTermInfo['Model_Weights']['PS2_Bar']['Wei'])\n",
    "    y = np.size(LongTermInfo['Discretizations']['PS-1_Magnitude']['Val'])\n",
    "\n",
    "    short_term['BarProb'] = [[0 for j in range(y)] for i in range(x)]\n",
    "    short_term['PS_model_YN'] = np.ones([y,x], dtype = int)\n",
    "\n",
    "    #COMPUTE INTEGRAL FOR MAGNITUDES\n",
    "    short_term = compute_distribution_for_magnitudes(LongTermInfo['Discretizations'], short_term, ee)\n",
    "\n",
    "    # Compute Short term Prob with respect PS and BS (total)\n",
    "    short_term = find_short_term_prob_for_psbs(short_term, negl_prob, lambda_bsps, len(LongTermInfo['Discretizations']['PS-1_Magnitude']['Val']), pre_selection)\n",
    "\n",
    "    if(short_term['BS_computed_YN'] == False and short_term['PS_computed_YN'] == False):\n",
    "        return False\n",
    "\n",
    "    if(short_term['BS_computed_YN'] == True and pre_selection['BS_scenarios'] == True):\n",
    "    # COMPUTE INTEGRAL FOR BS POS AND DEPTHS\n",
    "        short_term = set_grid_integration(LongTermInfo['Discretizations'], pre_selection, short_term)\n",
    "    # COMPUTE HYPOCENTRAL PROBABILITY DISTRIBUTION FOR BS, IF REQUIRED\n",
    "        short_term  = get_hypocentral_prob_for_bs(LongTermInfo['Discretizations'], pre_selection, short_term, ee)\n",
    "    else:\n",
    "        short_term['Total_BS_Scenarios'] = 0\n",
    "\n",
    "    # ##COMPUTE PS BAR PROBABILITY DISTRIBUTION FOR EACH PS MODEL, IF REQUIRED\n",
    "    # if(short_term['PS_computed_YN'] == True and pre_selection['PS_scenarios'] == True):\n",
    "    #     short_term  = get_ps_bar_probability(Discretizations  = LongTermInfo['Discretizations'],\n",
    "    #                                          Model_weight     = LongTermInfo['Model_Weights'],\n",
    "    #                                          pre_selection    = pre_selection,\n",
    "    #                                          lambda_bsps      = lambda_bsps,\n",
    "    #                                          PSBarInfo        = PSBarInfo,\n",
    "    #                                          short_term       = short_term,\n",
    "    #                                          ee               = ee)\n",
    "    # else:\n",
    "    #     short_term['Total_PS_Scenarios'] = 0\n",
    "\n",
    "    return short_term\n",
    "\n",
    "def compute_distribution_for_magnitudes(Discretizations, short_term, ee):\n",
    "\n",
    "    a     = Discretizations['PS-1_Magnitude']['Val'][0:-1]\n",
    "    b     = Discretizations['PS-1_Magnitude']['Val'][1:]\n",
    "    c     = np.add(a, b) * 0.5\n",
    "\n",
    "    lower = np.insert(c, 0, -np.inf)\n",
    "    upper = np.insert(c, c.size, np.inf)\n",
    "\n",
    "    lower_probility_norm  = norm.cdf(lower, ee['mag_percentiles']['p50'], ee['MagSigma'])\n",
    "    upper_probility_norm  = norm.cdf(upper, ee['mag_percentiles']['p50'], ee['MagSigma'])\n",
    "\n",
    "    short_term['magnitude_probability'] = np.subtract(upper_probility_norm, lower_probility_norm)\n",
    "    print(' --> Compute magnitude cumulative distribution')\n",
    "\n",
    "    return short_term\n",
    "\n",
    "def find_short_term_prob_for_psbs(short_term, negl_prob, lambda_bsps, len_PS_Mag, pre_selection):\n",
    "\n",
    "    max_BS_mag     = 8.1\n",
    "    max_PS_mag     = 9.1\n",
    "\n",
    "    short_term['BS_computed_YN'] = False\n",
    "    short_term['PS_computed_YN'] = False\n",
    "\n",
    "    vec_ps = np.ones(len_PS_Mag)  #ones or zeros? should be 1 only for mag>max_mag_BS\n",
    "    vec_bs = np.zeros(len_PS_Mag)\n",
    "\n",
    "    if (lambda_bsps['lambda_ps'] != 0.):\n",
    "        sel_RatioPSonPSTot = np.array(lambda_bsps['lambda_ps_sub']) / lambda_bsps['lambda_ps']\n",
    "    else:\n",
    "        sel_RatioPSonPSTot = np.array(lambda_bsps['lambda_ps_sub'])\n",
    "\n",
    "    pxBS = lambda_bsps['lambda_bs'] / (lambda_bsps['lambda_ps'] + lambda_bsps['lambda_bs'])\n",
    "    pxPS = 1 - pxBS\n",
    "\n",
    "    if(pre_selection['BS_scenarios'] == True):\n",
    "        for i in range(len(pre_selection['sel_BS_Mag_idx'][0])):\n",
    "\n",
    "            if (pre_selection['sel_BS_Mag_val'][i] <= max_BS_mag):\n",
    "                vec_bs[pre_selection['sel_BS_Mag_idx'][0][i]] = pxBS\n",
    "\n",
    "    if(pre_selection['PS_scenarios'] == True):\n",
    "        for i in range(len(pre_selection['sel_PS_Mag_idx'][0])):\n",
    "\n",
    "            if (pre_selection['sel_PS_Mag_val'][i] <= max_PS_mag):\n",
    "                vec_ps[pre_selection['sel_PS_Mag_idx'][0][i]] = pxPS\n",
    "\n",
    "    short_term['RatioPSonTot'] = vec_ps\n",
    "    short_term['RatioBSonTot'] = vec_bs\n",
    "    short_term['sel_RatioPSonPSTot'] = sel_RatioPSonPSTot\n",
    "\n",
    "    # Check, if probability BS/PS larger than Prob Negligible, then compute PS BS\n",
    "    tempbs = np.sum(np.multiply(short_term['magnitude_probability'][pre_selection['sel_PS_Mag_idx'][0]], pxBS))\n",
    "    tempps = np.sum(np.multiply(short_term['magnitude_probability'][pre_selection['sel_PS_Mag_idx'][0]], pxPS))\n",
    "    if(tempbs > negl_prob):\n",
    "        short_term['BS_computed_YN'] = True\n",
    "    if(tempps > negl_prob):\n",
    "        short_term['PS_computed_YN'] = True\n",
    "\n",
    "    print(' --> Negligible Probability: %.4f' % negl_prob)\n",
    "    print(' --> Probability BS = %.4e --> compute BS = %r' % (tempbs, short_term['BS_computed_YN']))\n",
    "    print(' --> Probability PS = %.4e --> compute PS = %r' % (tempps, short_term['PS_computed_YN']))\n",
    "\n",
    "    return short_term\n",
    "\n",
    "def set_grid_integration(Discretizations, pre_selection, short_term):\n",
    "\n",
    "    z_to_xyfact = 2.5\n",
    "    space_bin   = 2500.\n",
    "    space_grid  = z_to_xyfact * space_bin\n",
    "    all_depth   = np.array([])\n",
    "\n",
    "    minx = min(Discretizations['BS-2_Position']['utm_x'][pre_selection['BS2_Position_Selection_out']])\n",
    "    maxx = max(Discretizations['BS-2_Position']['utm_x'][pre_selection['BS2_Position_Selection_out']])\n",
    "    miny = min(Discretizations['BS-2_Position']['utm_y'][pre_selection['BS2_Position_Selection_out']])\n",
    "    maxy = max(Discretizations['BS-2_Position']['utm_y'][pre_selection['BS2_Position_Selection_out']])\n",
    "\n",
    "    tmp = Discretizations['BS-3_Depth']['ValVec'][pre_selection['sel_BS_Mag_idx']]\n",
    "    for i in range(0, np.shape(tmp)[0]):\n",
    "        all_depth = np.concatenate((all_depth, tmp[i][pre_selection['BS2_Position_Selection_out']]), axis=None)\n",
    "\n",
    "    all_depth_bs_3 = np.concatenate(all_depth)*1000\n",
    "    all_depth_moho = np.array(Discretizations['BS-2_Position']['DepthMoho'])[pre_selection['BS2_Position_Selection_out']]* (-1000.0)\n",
    "\n",
    "    # check if we have BS positions aligned in x or y or both\n",
    "    if (minx == maxx):\n",
    "        x_grid = np.array(minx)\n",
    "    else:\n",
    "        x_grid = np.arange(minx, maxx, space_grid)\n",
    "    if (miny == maxy):\n",
    "        y_grid = np.array(miny)\n",
    "    else:\n",
    "        y_grid = np.arange(miny, maxy, space_grid)\n",
    "\n",
    "    z_grid = np.arange(min(all_depth_bs_3), max(all_depth_moho), space_bin)\n",
    "\n",
    "    xx_2d, yy_2d = np.meshgrid(x_grid, y_grid, indexing='xy')\n",
    "    xx_2d = xx_2d.flatten('F')\n",
    "    yy_2d = yy_2d.flatten('F')\n",
    "    grid_2d = np.array([xx_2d, yy_2d])\n",
    "\n",
    "    xx_3d, yy_3d, zz_3d = np.meshgrid(x_grid, y_grid, z_grid, indexing='xy')\n",
    "    xx_3d = xx_3d.flatten('F')\n",
    "    yy_3d = yy_3d.flatten('F')\n",
    "    zz_3d = zz_3d.flatten('F')\n",
    "    grid_3d = np.array([xx_3d, yy_3d, zz_3d])\n",
    "\n",
    "    # inizialize\n",
    "    dist_2d_idx = np.zeros(len(yy_2d))\n",
    "    dist_2d_val = np.zeros(len(yy_2d))\n",
    "    dist_3d_idx = np.zeros(len(yy_3d))\n",
    "    dist_3d_val = np.zeros(len(yy_3d))\n",
    "\n",
    "    ## Make array with BS-2_Position\n",
    "    tmp = [Discretizations['BS-2_Position']['utm_y'][pre_selection['BS2_Position_Selection_out']],\n",
    "           Discretizations['BS-2_Position']['utm_x'][pre_selection['BS2_Position_Selection_out']]]\n",
    "    tmp = np.array(tmp).transpose()\n",
    "\n",
    "    # Mapping: get distances and idx\n",
    "    for i in range(len(yy_2d)):\n",
    "        a = np.linalg.norm(tmp - np.array([yy_2d[i], xx_2d[i]]), axis=1)\n",
    "        idx = np.where(a == np.amin(a))\n",
    "        dist_2d_idx[i] = idx[0][0]\n",
    "        dist_2d_val[i] = np.amin(a)\n",
    "\n",
    "    uu = []\n",
    "    for i in range(len(yy_3d)):\n",
    "        a = np.linalg.norm(tmp - np.array([yy_3d[i], xx_3d[i]]), axis=1)\n",
    "        uu.append(a)\n",
    "        idx = np.where(a == np.amin(a))\n",
    "        dist_3d_idx[i] = idx[0][0]\n",
    "        dist_3d_val[i] = np.amin(a)\n",
    "\n",
    "    short_term['grid_3d'] = grid_3d\n",
    "    short_term['grid_2d'] = grid_2d\n",
    "    short_term['dist_2d_idx'] = dist_2d_idx\n",
    "    short_term['dist_2d_val'] = dist_2d_val\n",
    "    short_term['dist_3d_idx'] = dist_3d_idx\n",
    "    short_term['dist_3d_val'] = dist_3d_val\n",
    "\n",
    "    print(' --> Set grid Integration')\n",
    "\n",
    "    return short_term\n",
    "\n",
    "def get_hypocentral_prob_for_bs(Discretizations, pre_selection, short_term, ee):\n",
    "\n",
    "    # initialize\n",
    "    bs1_mag = len(Discretizations['BS-1_Magnitude']['Val'])\n",
    "    pre_bs  = len(pre_selection['BS2_Position_Selection_inn'])\n",
    "\n",
    "    tmp_BS_scenarios_val = np.zeros((bs1_mag, pre_bs))\n",
    "    pre_selection['sel_BS_Mag_idx'] = pre_selection['sel_BS_Mag_idx'][0]\n",
    "\n",
    "    # SET SPATIAL PROBABILITY TO 1, SINCE ALREADY INCLUDED IN 3D INTEGRATION\n",
    "    short_term['PosProb'] = np.ones((len(pre_selection['sel_BS_Mag_idx']),\n",
    "                                    len(pre_selection['BS2_Position_Selection_inn'])))\n",
    "\n",
    "    for i in range(len(pre_selection['sel_BS_Mag_idx'])):\n",
    "        #get magnitude\n",
    "        v_mag   = pre_selection['sel_BS_Mag_val'][i]\n",
    "        i_mag   = pre_selection['sel_BS_Mag_idx'][i]\n",
    "\n",
    "        # Compute vertical half_width with respect the magnitude\n",
    "        v_hwidth = correct_BS_vertical_position(mag = v_mag)\n",
    "        h_hwidth = correct_BS_horizontal_position(mag = v_mag)\n",
    "\n",
    "        mu      = ee['PosMean_3d']\n",
    "        co      = copy.deepcopy(ee['PosCovMat_3dm'])\n",
    "\n",
    "        # Correct  Covariance matrix\n",
    "        co[0,0] = co[0,0] + h_hwidth**2\n",
    "        co[1,1] = co[1,1] + h_hwidth**2\n",
    "        co[2,2] = co[2,2] + v_hwidth**2\n",
    "\n",
    "        for j in range(len(pre_selection['BS2_Position_Selection_inn'])):\n",
    "            #get position\n",
    "            j_pos_inn_idx = pre_selection['BS2_Position_Selection_inn'][j]\n",
    "            # COUNT NUMBER OF SCENARIOS TO TREAT\n",
    "            try:\n",
    "                a = len(Discretizations['BS-3_Depth']['ValVec'][i_mag][j_pos_inn_idx])\n",
    "            except:\n",
    "                a = 0\n",
    "                raise Exception('!!!! Error in data: no depth defined !!!!')\n",
    "\n",
    "            b = len(Discretizations['BS-4_FocalMechanism']['ID'])\n",
    "            tmp_BS_scenarios_val[i, j] = a*b\n",
    "            j_pos_out = np.where(pre_selection['BS2_Position_Selection_out'] == j_pos_inn_idx)\n",
    "\n",
    "            # SELECT POINTS ABOVE MOHO IN THIS CELL\n",
    "            j_sel = np.where((short_term['grid_3d'][2,:]  > v_hwidth) &\n",
    "                             (short_term['dist_3d_idx']  == j_pos_out[0][0]) &\n",
    "                             (short_term['grid_3d'][2,:] <= -1000 * Discretizations['BS-2_Position']['DepthMoho'][j_pos_inn_idx] + v_hwidth))\n",
    "\n",
    "            tmp_depth = 1000 * Discretizations['BS-3_Depth']['ValVec'][i_mag][j_pos_inn_idx] + v_hwidth\n",
    "\n",
    "            b = np.array([])\n",
    "            for n in range(len(tmp_depth)):\n",
    "                a = np.linalg.norm(tmp_depth[n] - short_term['grid_3d'][2,j_sel], axis=0)\n",
    "                b = np.append(b, a, axis=0)\n",
    "\n",
    "            n = len(tmp_depth)\n",
    "            m = int(len(b)/n)\n",
    "            b = b.reshape(n,m)\n",
    "            # refDepthSel3D_val = b.min(axis=0)\n",
    "            refDepthSel3D_idx = np.where(b == np.amin(b, axis=0))[0]\n",
    "\n",
    "            # COMPUTE PROBABILITY WITHOUT INTEGRAL\n",
    "            tmp_idx = pre_selection['BS2_Position_Selection_inn'][j]\n",
    "            a = pre_selection['BS2_Position_Selection_inn']\n",
    "            x = Discretizations['BS-2_Position']['utm_x'][tmp_idx]\n",
    "            y = Discretizations['BS-2_Position']['utm_y'][tmp_idx]\n",
    "\n",
    "            tmp_pt     = npm.repmat([x,y], n,1)\n",
    "            tmp_pt     = np.append(tmp_pt, tmp_depth.reshape(n,1), axis=1)\n",
    "            tmp_grid3d = np.array([short_term['grid_3d'][0][j_sel],short_term['grid_3d'][1][j_sel],short_term['grid_3d'][2][j_sel]])\n",
    "\n",
    "            # compute probability for each depth point (depending on the size of the fault) and scenarios\n",
    "            short_term_prob_points = NormMultiDvec(x = tmp_pt, mu = mu, sigma = co) # line 536\n",
    "            short_term_prob_temps  = NormMultiDvec(x = tmp_grid3d.transpose(), mu = mu, sigma = co)\n",
    "\n",
    "            # Store short_term_prob_points\n",
    "            short_term['DepProbPoints'][i,j] = copy.deepcopy(short_term_prob_points)\n",
    "            short_term['DepProbTemps'][i,j]  = copy.deepcopy(short_term_prob_temps)\n",
    "\n",
    "            selection_sum = np.zeros(len(Discretizations['BS-3_Depth']['ValVec'][i_mag][j_pos_inn_idx]))\n",
    "\n",
    "            for k in range(len(Discretizations['BS-3_Depth']['ValVec'][i_mag][j_pos_inn_idx])):\n",
    "                selection = np.where(refDepthSel3D_idx == k)\n",
    "                selection_sum[k] = np.sum(short_term_prob_temps[selection])\n",
    "\n",
    "            short_term['DepProbScenes'][i,j] = copy.deepcopy(selection_sum)\n",
    "\n",
    "\n",
    "        keys = [z for z in short_term['DepProbScenes'] if z[0] == i]\n",
    "        vals = [short_term['DepProbScenes'][x] for x in keys]\n",
    "        NormFact = np.sum(np.hstack(vals))\n",
    "\n",
    "        keys = [z for z in short_term['DepProbPoints'] if z[0] == i]\n",
    "        vals = [short_term['DepProbPoints'][x] for x in keys]\n",
    "        NormFactPoints = np.sum(np.hstack(vals))\n",
    "\n",
    "        for p in range(len(pre_selection['BS2_Position_Selection_inn'])):\n",
    "            short_term['DepProbScenes'][i,p] = short_term['DepProbScenes'][i,p]/NormFact\n",
    "            short_term['DepProbPoints'][i,p] = short_term['DepProbPoints'][i,p]/NormFactPoints\n",
    "\n",
    "    short_term['Total_BS_Scenarios'] = tmp_BS_scenarios_val.sum()\n",
    "\n",
    "    return short_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90b5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the ptf_probability_scenarios.py file\n",
    "\n",
    "def compute_probability_scenarios(LongTermInfo, pre_selection, short_term, regions):\n",
    "\n",
    "    probability_scenarios = dict()\n",
    "    probability_scenarios['nr_ps_scenarios'] = 0\n",
    "    probability_scenarios['nr_bs_scenarios'] = 0\n",
    "    \n",
    "    if (short_term['BS_computed_YN'] == True and pre_selection['BS_scenarios'] == True):\n",
    "    \n",
    "        probability_scenarios['par_scenarios_bs']       = np.zeros( (int(short_term['Total_BS_Scenarios']), 11) )\n",
    "        probability_scenarios['prob_scenarios_bs_fact'] = np.zeros( (int(short_term['Total_BS_Scenarios']), 5) )\n",
    "        probability_scenarios = bs_probability_scenarios(short_term, pre_selection, regions, probability_scenarios, LongTermInfo['Discretizations'])\n",
    "\n",
    "        probability_scenarios['relevant_scenarios_bs'] = np.unique(probability_scenarios['par_scenarios_bs'][:,0])\n",
    "\n",
    "        print(' --> Used regions for BS : {}'.format(probability_scenarios['relevant_scenarios_bs']))\n",
    "\n",
    "        # Re-Normalize scenarios (to manage events outside nSigma, BS for large events, ...)\n",
    "        ProbScenBS = probability_scenarios['prob_scenarios_bs_fact'].prod(axis=1)\n",
    "        TotProbBS_preNorm = np.sum(ProbScenBS)\n",
    "\n",
    "        # # saving list of bs scenarios parameters\n",
    "        # if samp_mode == 'None':\n",
    "        #     save_bs_scenarios_list(par_scenarios_bs = probability_scenarios['par_scenarios_bs'],\n",
    "        #                            file_bs_list     = file_bs_list)\n",
    "    else:\n",
    "        print('No BS scenarios')\n",
    "        ProbScenBS = np.empty((0))\n",
    "        TotProbBS_preNorm = 0\n",
    "        probability_scenarios['par_scenarios_bs'] = np.empty((0, 0))\n",
    "\n",
    "    # if (short_term['PS_computed_YN'] == True):\n",
    "\n",
    "    #     probability_scenarios['par_scenarios_ps'] = np.zeros( (int(short_term['Total_PS_Scenarios']), 7) )\n",
    "    #     probability_scenarios['prob_scenarios_ps_fact'] = np.zeros( (int(short_term['Total_PS_Scenarios']), 5) )\n",
    "\n",
    "    #     probability_scenarios = ps_probability_scenarios(PSBarInfo        = PSBarInfo,\n",
    "    #                                                      short_term       = short_term,\n",
    "    #                                                      pre_selection    = pre_selection,\n",
    "    #                                                      prob_scenes      = probability_scenarios,\n",
    "    #                                                      region_ps        = regions['region_listPs'],\n",
    "    #                                                      Model_Weights    = LongTermInfo['Model_Weights'],\n",
    "    #                                                      Scenarios_PS     = Scenarios_PS,\n",
    "    #                                                      ps1_magnitude    = LongTermInfo['Discretizations']['PS-1_Magnitude'],\n",
    "    #                                                      lambda_bsps      = lambda_bsps)\n",
    "\n",
    "    #     probability_scenarios['relevant_scenarios_ps'] = np.unique(probability_scenarios['par_scenarios_ps'][:,0])\n",
    "    #     logger.info(' --> Used regions for PS : {}'.format(probability_scenarios['relevant_scenarios_ps']))\n",
    "    #     # Re-Normalize scenarios (to manage events outside nSigma, BS for large events, ...)\n",
    "    #     ProbScenPS = probability_scenarios['prob_scenarios_ps_fact'].prod(axis=1)\n",
    "    #     TotProbPS_preNorm = np.sum(ProbScenPS)\n",
    "    # else:\n",
    "    #     logger.info('No PS scenarios')\n",
    "    ProbScenPS = np.empty((0))\n",
    "    TotProbPS_preNorm = 0\n",
    "    probability_scenarios['par_scenarios_ps'] = np.empty((0, 0))\n",
    "    #     # # saving empty file\n",
    "    #     # f_list_ps = open(file_ps_list, 'w')\n",
    "    #     # f_list_ps.close()\n",
    "\n",
    "    if (probability_scenarios == False):\n",
    "        return False\n",
    "\n",
    "    TotProb_preNorm = TotProbBS_preNorm + TotProbPS_preNorm\n",
    "\n",
    "    # No scenarios bs or ps possible\n",
    "    if (TotProb_preNorm == 0):\n",
    "        return False\n",
    "\n",
    "    if (TotProb_preNorm < 1.0):\n",
    "        ProbScenBS = ProbScenBS / TotProb_preNorm\n",
    "        ProbScenPS = ProbScenPS / TotProb_preNorm\n",
    "        print(' --> Total BS scenarios probability pre-renormalization: {:.5f}'.format(TotProbBS_preNorm))\n",
    "        print(' --> Total PS scenarios probability pre-renormalization: {:.5f}'.format(TotProbPS_preNorm))\n",
    "        print('     --> Total BS and PS probabilty renormalized to 1')\n",
    "\n",
    "    probability_scenarios['ProbScenBS'] = ProbScenBS\n",
    "    probability_scenarios['ProbScenPS'] = ProbScenPS\n",
    "\n",
    "    # # saving probability of scenarios bs and ps\n",
    "    # if samp_mode == 'None':\n",
    "    #     save_probability_scenarios(prob_scenarios_bs = probability_scenarios['ProbScenBS'], \n",
    "    #                                prob_scenarios_ps = probability_scenarios['ProbScenPS'],\n",
    "    #                                workflow_dict     = workflow_dict)\n",
    " \n",
    "    # check on the nr scenarios computed into the two section. Should be identical\n",
    "    check_bs = 'OK'\n",
    "    check_ps = 'OK'\n",
    "\n",
    "    print(' --> Total_BS_Scenarios: {:7d}'.format(probability_scenarios['nr_bs_scenarios']))\n",
    "    print(' --> Total_PS_Scenarios: {:7d}'.format(probability_scenarios['nr_ps_scenarios']))\n",
    " \n",
    "    if (probability_scenarios['nr_bs_scenarios'] != short_term['Total_BS_Scenarios']):\n",
    "        check_bs = 'WARNING'\n",
    "        print(' --> Check Nr BS scenarios: {:7d} <--> {} --> {}'.format(probability_scenarios['nr_bs_scenarios'], short_term['Total_BS_Scenarios'], check_bs))\n",
    "    # if (probability_scenarios['nr_ps_scenarios'] != short_term['Total_PS_Scenarios']):\n",
    "    #     check_ps = 'WARNING'\n",
    "    #     print(' --> Check Nr PS scenarios: {:7d} <--> {} --> {}'.format(probability_scenarios['nr_ps_scenarios'], short_term['Total_PS_Scenarios'], check_ps))\n",
    "\n",
    "    return probability_scenarios\n",
    "\n",
    "def bs_probability_scenarios(short_term, pre_selection, region_files, prob_scenes, Discretizations):\n",
    "\n",
    "    empty_scenarios = [37,38,39,41,43,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,82,83,84,85,86,87,88,89,90,92,100,101,102,103,104,105,108,109]\n",
    "\n",
    "    region_info = dict()\n",
    "    regions_nr = []\n",
    "\n",
    "    iScenBS = 0\n",
    "    sel_mag = len(pre_selection['sel_BS_Mag_idx'])\n",
    "    bs2_pos = len(pre_selection['BS2_Position_Selection_common'])\n",
    "    foc_ids = len(Discretizations['BS-4_FocalMechanism']['ID'])\n",
    "\n",
    "    for i1 in range(sel_mag):\n",
    "        imag = pre_selection['sel_BS_Mag_idx'][i1]\n",
    "\n",
    "        for i2 in range(bs2_pos):\n",
    "            ipos = pre_selection['BS2_Position_Selection_common'][i2]\n",
    "            ireg = Discretizations['BS-2_Position']['Region'][ipos]\n",
    "            \n",
    "            if ireg in empty_scenarios:\n",
    "                continue\n",
    "\n",
    "            if (ireg not in regions_nr):\n",
    "                region_info = load_region_infos(ireg, region_info, region_files)\n",
    "\n",
    "                regions_nr.append(ireg)\n",
    "\n",
    "            RegMeanProb_BS4 = region_info[ireg]['BS4_FocMech_MeanProb_valNorm']\n",
    "            if(RegMeanProb_BS4.size == 0):\n",
    "                 print(' --> WARNING: region info %d is empty!!!' % (ireg) )\n",
    "\n",
    "            ipos_reg = np.where(region_info[ireg]['BS4_FocMech_iPosInRegion'] == ipos+1)[1]\n",
    "            tmpProbAngles = RegMeanProb_BS4[ipos_reg[0]]\n",
    "\n",
    "            len_depth_valvec = len(Discretizations['BS-3_Depth']['ValVec'][imag][ipos])\n",
    "\n",
    "            # I3 (depth) AND I4 (angles) ENUMERATE ALL RELEVANT SCENARIOS FOR EACH MAG AND POS (Equivalent to compute_scenarios_prefixes)\n",
    "            for i3 in range(len_depth_valvec):\n",
    "\n",
    "                for i4 in range(foc_ids):\n",
    "                    mag               = Discretizations['BS-1_Magnitude']['Val'][imag]\n",
    "                    lon, lat          = Discretizations['BS-2_Position']['Val'][ipos].split()\n",
    "                    depth             = Discretizations['BS-3_Depth']['ValVec'][imag][ipos][i3]\n",
    "                    strike, dip, rake = Discretizations['BS-4_FocalMechanism']['Val'][i4].split()\n",
    "                    area              = Discretizations['BS-5_Area']['ValArea'][ireg-1, imag, i4]\n",
    "                    length            = Discretizations['BS-5_Area']['ValLen'][ireg-1, imag, i4]\n",
    "                    slip              = Discretizations['BS-6_Slip']['Val'][ireg-1, imag, i4]\n",
    "\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][0]  = int(ireg)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][1]  = float(mag)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][2]  = float(lon)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][3]  = float(lat)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][4]  = float(depth)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][5]  = float(strike)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][6]  = float(dip)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][7]  = float(rake)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][8]  = float(length)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][9]  = float(area)\n",
    "                    prob_scenes['par_scenarios_bs'][iScenBS][10] = float(slip)\n",
    "\n",
    "                    prob_scenes['prob_scenarios_bs_fact'][iScenBS][0] = short_term['magnitude_probability'][imag]\n",
    "                    prob_scenes['prob_scenarios_bs_fact'][iScenBS][1] = short_term['PosProb'][i1, i2]\n",
    "                    prob_scenes['prob_scenarios_bs_fact'][iScenBS][2] = short_term['RatioBSonTot'][imag]\n",
    "                    prob_scenes['prob_scenarios_bs_fact'][iScenBS][3] = short_term['DepProbScenes'][i1, i2][i3]\n",
    "                    prob_scenes['prob_scenarios_bs_fact'][iScenBS][4] = tmpProbAngles[i4]\n",
    "\n",
    "                    iScenBS = iScenBS + 1\n",
    "\n",
    "    prob_scenes['par_scenarios_bs'] = prob_scenes['par_scenarios_bs'][:iScenBS,:]\n",
    "    prob_scenes['prob_scenarios_bs_fact'] = prob_scenes['prob_scenarios_bs_fact'][:iScenBS,:]\n",
    "    prob_scenes['nr_bs_scenarios'] = np.shape(prob_scenes['prob_scenarios_bs_fact'])[0]\n",
    "\n",
    "    return prob_scenes\n",
    "\n",
    "def load_region_infos(ireg, region_info, files):\n",
    "\n",
    "    info = np.load(files['ModelsProb_Region_files'][ireg-1], allow_pickle=True).item()\n",
    "    region_info[ireg] = info\n",
    "    return region_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed96d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the step3_run.py file\n",
    "\n",
    "def compute_hazard_curves(mih, prob_scenarios, n_pois, thresholds, sigma):\n",
    "\n",
    "    n_thr = len(thresholds)\n",
    "\n",
    "    hazard_curves_pois = np.zeros((n_pois, n_thr))\n",
    "\n",
    "    # print(n_pois, n_thr, n_scen)\n",
    "    # print(mih.shape, prob_scenarios.shape)\n",
    "\n",
    "    # hazard_mode = 'lognormal'\n",
    "    for ip in range(n_pois):\n",
    "\n",
    "            mih_at_poi = mih[:,ip]\n",
    "            ind_tmp = np.array(mih_at_poi == 0)\n",
    "            mih_at_poi[ind_tmp] = 1.e-12\n",
    "\n",
    "            mu = mih_at_poi\n",
    "            mu = mu.reshape(len(mu), 1)\n",
    "\n",
    "            cond_hazard_curve_tmp = 1 - scipy.stats.lognorm.cdf(thresholds, sigma, scale=mu).transpose()\n",
    "            hazard_curves_pois[ip,:] = np.sum(prob_scenarios*cond_hazard_curve_tmp, axis=1)\n",
    "\n",
    "    # # hazard_mode = 'lognormal_v1'\n",
    "    # mih_coo = scipy.sparse.coo_array(mih)\n",
    "    # print(\"Number of non-zero elements = {}\".format(len(mih_coo.data)))\n",
    "    # df_mihs = pd.DataFrame({\"id_scen\":mih_coo.row,\"id_poi\":mih_coo.col,\"mih_value\":mih_coo.data})\n",
    "    # df_prob_scenarios = pd.DataFrame(prob_scenarios)\\\n",
    "    #                             .reset_index()\\\n",
    "    #                             .rename(columns={'index':'id_scen',0:'prob_scen'})\n",
    "    # df_mihs = df_mihs.merge(df_prob_scenarios,how='left',left_on='id_scen', right_on='id_scen')\n",
    "\n",
    "    # for ith,threshold in enumerate(thresholds[:]):\n",
    "    #     df_mihs_thr = df_mihs.copy(deep=True)\n",
    "    #     col_name = 'prob_lognorm_{}'.format(threshold)\n",
    "    #     df_mihs_thr[col_name] = 1-scipy.stats.lognorm.cdf(threshold, sigma, scale=df_mihs_thr['mih_value']).transpose()\n",
    "    #     df_mihs_thr[col_name] = df_mihs_thr[col_name]*df_mihs_thr['prob_scen']\n",
    "    #     df_mihs_thr = df_mihs_thr.groupby(by='id_poi').agg({col_name:'sum'})\n",
    "    #     hazard_curves_pois[df_mihs_thr.index,ith]=df_mihs_thr[col_name].to_numpy()\n",
    "\n",
    "    return hazard_curves_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from the step5_run.py file\n",
    "\n",
    "def plot_hazard_maps(points, hmaps, event_dict, map_label):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    proj = cartopy.crs.PlateCarree()\n",
    "    #cmap = plt.cm.magma_r\n",
    "    cmap = plt.cm.jet\n",
    "    ev_lon = event_dict['lon']\n",
    "    ev_lat = event_dict['lat']\n",
    "    ev_depth = event_dict['depth']\n",
    "    ev_mag = event_dict['mag']\n",
    "    ev_place = event_dict['place']\n",
    "\n",
    "    for key, hmap in hmaps.items():           \n",
    "                \n",
    "        print(\"mapping ... {}\".format(key))\n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "        ax = plt.axes(projection=cartopy.crs.Mercator())\n",
    "        coastline = cartopy.feature.GSHHSFeature(scale='low', levels=[1])\n",
    "        #coastline = cartopy.feature.GSHHSFeature(scale='high', levels=[1])\n",
    "        ax.add_feature(coastline, edgecolor='#000000', facecolor='#cccccc', linewidth=1)\n",
    "        ax.add_feature(cartopy.feature.BORDERS.with_scale('50m'))\n",
    "        ax.add_feature(cartopy.feature.STATES.with_scale('50m'))\n",
    "        ax.add_feature(cartopy.feature.OCEAN.with_scale('50m'))\n",
    "        gl = ax.gridlines(crs=proj, draw_labels=True, linewidth=1,\n",
    "                           color=\"#ffffff\", alpha=0.5, linestyle='-')\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.bottom_labels = True\n",
    "        gl.left_labels = True\n",
    "        gl.xformatter = cartopy.mpl.gridliner.LONGITUDE_FORMATTER\n",
    "        gl.yformatter = cartopy.mpl.gridliner.LATITUDE_FORMATTER\n",
    "        gl.xlabel_style = {'size': 14}\n",
    "        gl.ylabel_style = {'size': 14}\n",
    "\n",
    "        sc = ax.scatter(points[:,0], points[:,1], c=hmap, s=17, marker=\"o\", \n",
    "                    linewidths=0.75, edgecolors=\"#000000\", label=\" \",\n",
    "                    cmap=cmap, clip_on=True,# vmin=map_min, vmax=map_max, \n",
    "                    transform=proj, zorder=10, norm=matplotlib.colors.LogNorm(vmin=0.01, vmax=50))#(vmin=map_min, vmax=map_max))\n",
    "\n",
    "        ax.plot(ev_lon, ev_lat, linewidth=0, marker='*', markersize=14, \n",
    "                #markerfacecolor='#c0bfbc', markeredgecolor='#000000', \n",
    "                markerfacecolor='white', markeredgecolor='#000000', \n",
    "                transform=proj)\n",
    "\n",
    "        cbar = plt.colorbar(sc, shrink=0.75)\n",
    "        #cbar.ax.set_yticklabels(labels=cbar.ax.get_yticklabels(), fontsize=10)\n",
    "        cbar.set_label(label=f'(m)', size=12)\n",
    "        # ax.set_title(\"Hazard map - {0}\".format(key))\n",
    "        plt.suptitle(\"Hazard map - {0}\".format(key),fontsize=24)\n",
    "        plt.title(\"Epicentral Region: {0} \\n Event parameters: Lon={1}, Lat={2}, Depth={3}; Magnitude={4}\".format(ev_place, ev_lon, ev_lat, ev_depth, str(ev_mag)[:3] ),fontsize=18)\n",
    "        ax.set_xlabel(r'Longitude ($^\\circ$)', fontsize=14)\n",
    "        ax.set_ylabel(r'Latitude ($^\\circ$)', fontsize=14)\n",
    "        \n",
    "def plot_hazard_curve(ind, thresholds, hc):\n",
    "\n",
    "    fig = plt.figure(figsize=(20,14))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_facecolor('#cccccc')\n",
    "    ax.grid(True, color='#ffffff')\n",
    "\n",
    "    for poi_index in ind:\n",
    "\n",
    "        ax.plot(thresholds, hc[poi_index,:], label=poi_index,\n",
    "                 alpha=1, # color='#1a5fb4',\n",
    "                 linestyle=\"solid\", linewidth=3)\n",
    "\n",
    "\n",
    "    #ax.legend(loc=\"lower left\", ncol=2, bbox_to_anchor=(0., 1.0), frameon=False)7.574120044708252sec\n",
    "    ax.legend(loc=\"upper right\", ncol=1, bbox_to_anchor=(1.0, 1.0), frameon=True)\n",
    "    # ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    # ax.set_xlim(1e-2, 100)\n",
    "    ax.set_xlim(0, 20)\n",
    "    ax.set_ylim(1e-5, 5)\n",
    "    ax.set_xlabel(r'MIH (m)')\n",
    "    ax.set_ylabel(r'PoE (50 yrs)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
