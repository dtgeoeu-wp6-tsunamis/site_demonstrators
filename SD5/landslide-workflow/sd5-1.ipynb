{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afea6259",
   "metadata": {},
   "source": [
    "**SD5 - 1908 Messina event: a test case for adding landslide sources to the PTF**   \n",
    "On December 28, 1908, an earthquake of an estimated magnitude Mw of 7.1 occurred near Messina, offshore East Sicily. The earthquake triggered a tsunami of several meters and caused at least 60,000 fatalities. The exact cause of the 1908 tsunami is not fully understood, and both earthquake and landslide source contributions provide alternative or complementary explanations.   \n",
    "   \n",
    "In this notebook, we use the 1908 Messina event as a test case for the mini-workflow that embeds landslide sources into the PTF.   \n",
    "Before running the workflow, we precomputed the possible landslide release volumes in the area of interest, ran the numerical simulations of landslide dynamics for each release volume with the code BingClaw, followed by the numerical simulations of tsunami propagation with the code Tsunami-HySEA.    \n",
    "    \n",
    "The mini-workflow in this notebook follows these steps:\n",
    "1. Run the first step of the PTF that generates an ensemble of earthquake scenarios and associated probabilities\n",
    "2. Compute the peak ground acceleration (PGA) for each earthquake scenario (shakemaps)\n",
    "3. Compute the probabilities of each precomputed release volume based on the PGA and the probability of each earthquake scenario\n",
    "4. Retrieve the maximum inundation heights (MIH) from the precomputed simulations database, aggregate the probabilities, and visualize the results.\n",
    "    \n",
    "*See the README file for info on requirements to run the notebook and where to download the input files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18861009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import interp1d\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import rasterio\n",
    "\n",
    "import ewricagm.gm_mapping as GMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set paths of input and output directories\n",
    "# path of the main input folder (downloaded from the Simulation Data Lake)\n",
    "input_folder = r\"P:\\2022\\02\\20220296\\Delivery-Result\\input-landslide-workflow-sd5\\\\\"\n",
    "# input_folder = \"/mnt/P/2022/02/20220296/Delivery-Result/input-landslide-workflow-sd5\"\n",
    "# ptf input data folder\n",
    "data_folder = os.path.join(input_folder, \"ptf-input\")  \n",
    "# precomputed database input folder\n",
    "database_folder = os.path.join(input_folder, \"precomputed-database\")\n",
    "# output folder\n",
    "output_folder = 'output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load external functions\n",
    "%run ./utils/ptf_functions.ipynb\n",
    "%run ./utils/ptf_functions_sampling.ipynb\n",
    "%run ./utils/workflow_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb54d5d",
   "metadata": {},
   "source": [
    "### **1. Run step 1 of the PTF**   \n",
    "This step create an ensemble of earthquake scenarios with different source parameters and associated probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input parameters for the PTF\n",
    "event_file_name = '1908_1228_messina.json'\n",
    "workdir = './'\n",
    "sigma = 1.1\n",
    "sigma_inn = sigma\n",
    "sigma_out = sigma + 0.5\n",
    "negligible_prob = 2*norm.cdf(-1. * sigma)\n",
    "sampling_nscenarios = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the event file and build the event dictionary\n",
    "event_file = os.path.join(data_folder, event_file_name)\n",
    "s = open(event_file, 'r').read()\n",
    "jsn_object = eval(s)\n",
    "\n",
    "event_dict = int_quake_cat2dict(json_object=jsn_object)\n",
    "\n",
    "event_dict['event_file'] = event_file\n",
    "event_dict['sigma'] = sigma\n",
    "\n",
    "event_dict = compute_position_sigma_lat_lon(event_dict)\n",
    "print(event_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load file with coordinates of Points of Interests (POIs) and plot POIs locations\n",
    "pois = np.load(os.path.join(data_folder,'pois_messina_ptf.npy'), allow_pickle=True).item()\n",
    "n_pois = len(pois['pois_coords'])\n",
    "print(f'Total number of POIs: {n_pois}')\n",
    "\n",
    "pois_coords = pois['pois_coords']\n",
    "pois_index = np.arange(n_pois)\n",
    "plot_pois(pois_coords, pois_index, event_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load input data from long term database\n",
    "thresholds, intensity_measure = load_intensity_thresholds(data_folder)\n",
    "\n",
    "LongTermInfo = dict()\n",
    "LongTermInfo['Regionalization'] = np.load(os.path.join(data_folder, 'regionalization.npy'), allow_pickle=True).item()\n",
    "LongTermInfo['Discretizations'] = np.load(os.path.join(data_folder,  'discretizations.npy'), allow_pickle=True).item()\n",
    "LongTermInfo['Model_Weights'] =  load_Model_Weights(data_folder)\n",
    "Region_files = load_region_files(LongTermInfo['Regionalization']['Npoly'], LongTermInfo['Regionalization']['Ttypes'], data_folder)\n",
    "Mesh = np.load(os.path.join(data_folder,'mesh_files', 'slab_meshes_mediterranean.npy'), allow_pickle=True).item()\n",
    "PSBarInfo = np.load(os.path.join(data_folder, 'PSBarInfo.npy'), allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find BS and PS scenarios from long term database that are relevant for this event and compute the probability of each scenario\n",
    "ellipses = build_ellipsoid_objects(event_dict, sigma_inn, sigma_out)\n",
    "LongTermInfo, PSBarInfo = conversion_to_utm(LongTermInfo, event_dict, PSBarInfo)\n",
    "\n",
    "# Separation BS/PS\n",
    "lambda_bsps = load_lambda_BSPS(sigma, event_dict, data_folder)\n",
    "lambda_bsps = separation_lambda_BSPS(event_dict, lambda_bsps, LongTermInfo, Mesh)\n",
    "\n",
    "# Pre-selection of scenarios\n",
    "pre_selection = pre_selection_of_scenarios(sigma, event_dict, LongTermInfo, PSBarInfo, ellipses)\n",
    "\n",
    "# Compute probabilities\n",
    "short_term_probability  = short_term_probability_distribution(event_dict, negligible_prob, LongTermInfo, PSBarInfo, lambda_bsps, pre_selection)\n",
    "probability_scenarios = compute_probability_scenarios(LongTermInfo, pre_selection, short_term_probability, Region_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sampling the ensemble to reduce the number of scenarios and update the probabilities (optional)\n",
    "sampled_ensemble_SDE = compute_ensemble_sampling_SDE(LongTermInfo     = LongTermInfo,                                                                                                     negligible_prob  = negligible_prob,                                                                                                  pre_selection    = pre_selection,                                                                                                    regions          = Region_files,\n",
    "                                                     short_term       = short_term_probability,\n",
    "                                                     proba_scenarios  = probability_scenarios,\n",
    "                                                     samp_scen        = sampling_nscenarios,\n",
    "                                                     samp_type        = 'LH')\n",
    "\n",
    "probability_scenarios = sampled_ensemble_SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save scenario lists (parameters and probabilities)\n",
    "file_bs_list = os.path.join(output_folder, 'step1_BS_scenarios_list.txt')\n",
    "file_bs_prob = os.path.join(output_folder, 'step1_BS_scenarios_prob.npy')\n",
    "\n",
    "par_scenarios_bs = probability_scenarios['par_scenarios_bs']\n",
    "len_scenarios_bs, len_pars = par_scenarios_bs.shape\n",
    "\n",
    "fmt = \"{:f}\".format\n",
    "with open(file_bs_list, 'w') as f_list_bs:\n",
    "    for ic in range(len_scenarios_bs):\n",
    "        pars = \" \".join(fmt(par_scenarios_bs[ic, item]) for item in range(1, len_pars))\n",
    "        ireg = \"{:.0f}\".format(par_scenarios_bs[ic, 0])\n",
    "        f_list_bs.write(\"{:s} {:s} {:s}\\n\".format(str(ic+1), ireg, pars))\n",
    "\n",
    "prob_scenarios_bs = probability_scenarios['ProbScenBS']\n",
    "np.save(file_bs_prob, prob_scenarios_bs)\n",
    "\n",
    "print('BS scenarios saved in: {}'.format(file_bs_list))\n",
    "print('BS probabilities saved in: {}'.format(file_bs_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd1b43",
   "metadata": {},
   "source": [
    "### **2. Create shakemaps for each PTF scenario**    \n",
    "This step uses the earthquake source parameters from the list of scenarios generated by the step 1 of the PTF to compute a shakemap for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c589d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert the BS scenarios list into the required *.yaml file\n",
    "path_to_csv = 'output/step1_BS_scenarios_list.txt'\n",
    "yaml_path=convert_csv_to_yaml(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run neural network to calculate the shakemaps\n",
    "print('\\nCalculating shapefunctions NN_MT:')\n",
    "directory = os.path.dirname(yaml_path)\n",
    "ensemblefile = yaml_path\n",
    "config_path = 'ewricagm/configs/config_nn_mt.yaml'\n",
    "GMmap.gm_mapping_read(ensemblefile, directory, config_path=config_path)\n",
    "print('Finished calculating NN_MT.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract Peak Ground Acceleration (Z_pga, H_pga) from the resulting JSON file and save output\n",
    "# Load the JSON data from the file\n",
    "with open(f'{directory}/predicted_data_NN.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the required fields\n",
    "extracted_data = []\n",
    "# Iterate over the values of the dictionary\n",
    "for item_value in data.values():\n",
    "    extracted_data.append({\n",
    "        \"lat\": item_value.get(\"lat\"),\n",
    "        \"lon\": item_value.get(\"lon\"),\n",
    "        \"Z_pga\": item_value.get(\"Z_pga\"),\n",
    "        \"H_pga\": item_value.get(\"H_pga\")\n",
    "    })\n",
    "\n",
    "# Write the extracted data to a new JSON file\n",
    "with open(f'{directory}/H_Z_pda_data_log10_G.json', 'w') as f:\n",
    "    json.dump(extracted_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82e2c7",
   "metadata": {},
   "source": [
    "**Plot shakemaps** (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  data from the JSON file\n",
    "with open(f'{directory}/H_Z_pda_data_log10_G.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the maximum index based on the data\n",
    "max_index = len(data[0]['Z_pga']) - 1 if data and data[0].get('Z_pga') else 0\n",
    "# Create a list of options for the dropdown menu\n",
    "scenario_options = [(f'Scenario {i}', i) for i in range(max_index + 1)]\n",
    "# Create an interactive dropdown for selecting the scenario index\n",
    "index_dropdown = widgets.Dropdown(\n",
    "    options=scenario_options,\n",
    "    value=0,\n",
    "    description='Index:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Link the dropdown to the plotting function\n",
    "widgets.interactive(lambda index_to_plot: plot_pga_shakemaps(data, index_to_plot), index_to_plot=index_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b669fb",
   "metadata": {},
   "source": [
    "### **3. Compute the probabilities of the landslide+tsunami scenarios**    \n",
    "This step uses the shakemaps to compute the probability that each volume from the pre-computed database has to fail and produce a landslide and a subsequent tsunami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For importing from rvsampler\n",
    "#sys.path.append('/home/erlend/github/site_demonstrators/SD5/landslide-workflow/precomputed-database/release-volume-sampler/src')\n",
    "to_append = os.path.join(\"precomputed-database\",\"release-volume-sampler\",\"src\")\n",
    "sys.path.append(to_append)\n",
    "rundir = os.path.join(output_folder,\"rvsampler-rundir\")\n",
    "print(f\"Rundir is {rundir}\")\n",
    "# Copy database generated by preparational step to output folder.\n",
    "if not os.path.exists(rundir):\n",
    "    shutil.copytree(os.path.join(input_folder, \"rvsampler-rundir-messina-20250806\"), rundir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb354c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up operational steps.\n",
    "import subprocess\n",
    "\n",
    "# subprocess.run([\n",
    "#     \"python3\",\n",
    "#     \"/home/erlend/github/site_demonstrators/SD5/landslide-workflow/precomputed-database/release-volume-sampler/src/clean_operational.py\",\n",
    "#     \"--rundir\", rundir\n",
    "# ], check=True)\n",
    "\n",
    "%run ./precomputed-database/release-volume-sampler/src/clean_operational.py --rundir {rundir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db569231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvsampler.shakemaps_reader import ShakemapsReader\n",
    "\n",
    "# Preprocess shakemaps\n",
    "shakemaps_filename = os.path.join(output_folder,\"H_Z_pda_data_log10_G.json\")\n",
    "bathymetry_filename = os.path.join(rundir, \"bathy_truncated.tif\")\n",
    "\n",
    "shakemaps_reader = ShakemapsReader(\n",
    "    shakemaps_filename=shakemaps_filename,\n",
    "    rundir=rundir,\n",
    ")\n",
    "\n",
    "with rasterio.open(bathymetry_filename) as src:\n",
    "    bounds = src.bounds\n",
    "    profile = src.profile.copy()\n",
    "\n",
    "shakemaps_reader.write_shakemaps_to_rasters(\n",
    "    profile=profile, \n",
    "    bounds=bounds,\n",
    "    samples=None, # None (reads all samples) or specify a list of samples to read, e.g., [0,3]\n",
    "    interpolation_method='linear', # “linear”, “nearest”, “slinear”, “cubic”, “quintic” and “pchip”\n",
    ")\n",
    "shakemaps_reader.completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9687f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Displacement probabilities\n",
    "from rvsampler.displacements import DisplacementProbabilityAggregator\n",
    "from rvsampler.database_handler import VolumeDatabaseHandler\n",
    "\n",
    "displacements_exceedance_params = {\n",
    "    \"cumulative_dir\": os.path.join(rundir, \"displacements\"),\n",
    "    \"outfile_name\": \"exceedance_displacement.npz\"\n",
    "}\n",
    "dpa = DisplacementProbabilityAggregator(rundir, magnitude=7)\n",
    "dpa.compute_probabilities_by_sample(displacement_threshold=5., nr_of_pga_thresholds=100)\n",
    "dpa.completed()\n",
    "# Write probabilities to database\n",
    "displacements_dir = os.path.join(rundir, \"displacements\")\n",
    "\n",
    "with VolumeDatabaseHandler(rundir) as volumes_db:\n",
    "    for fname in os.listdir(displacements_dir):\n",
    "        if fname.startswith(\"sample_\") and os.path.isdir(os.path.join(displacements_dir, fname)):\n",
    "            sample_nr = fname.split(\"_\")[-1]\n",
    "            column_name = f\"p_shake_{sample_nr}\"\n",
    "            volumes_db.assign_probabilities_to_seed_triangles(\n",
    "                displacement_dir=os.path.join(displacements_dir, fname),\n",
    "                table_filename=\"exceedance_displacement.npz\",\n",
    "                column_name=column_name\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de61822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster release probabilities\n",
    "from rvsampler.aggregate import ProbabilityAggregator\n",
    "\n",
    "pag = ProbabilityAggregator(rundir)\n",
    "result = pag.compute_cluster_release_probabilities()\n",
    "pag.plot_cluster_probability_heatmap(save_fig=True);\n",
    "pag.completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdefa5b",
   "metadata": {},
   "source": [
    "### **4. Results and visualisation**   \n",
    "This step extracts the Maximum Inundation Heights (MIHs) from the pre-computed database of tsunamis simulations, uses the probabilities from step 3 to compute the exceedance probability of the MIH at each POI, and plots the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b00619",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load necessary files\n",
    "# Load MIH values from precomputed database (mih shape: number of landslide-tsunami scenarios, number of POIs)\n",
    "mih_all_file = os.path.join(database_folder,\"MIH_all.npy\")\n",
    "MIH_all = np.load(mih_all_file)\n",
    "\n",
    "# Load PTF scenarios' probabilities (ptf_prob shape: number of PTF scenarios)\n",
    "ptf_prob_file = os.path.join(output_folder,\"step1_BS_scenarios_prob.npy\")\n",
    "ptf_prob = np.load(ptf_prob_file)\n",
    "\n",
    "# Load matrix of probabilities \n",
    "cluster_prob_file = os.path.join(rundir, \"aggregation\", \"cluster_release_probabilities.npz\")\n",
    "cluster_prob = np.load(cluster_prob_file,allow_pickle=True)\n",
    "\n",
    "# Load cluster_prob into a DataFrame with scenarios as columns and clusters as index\n",
    "cluster_release_probabilities = pd.DataFrame(cluster_prob['probabilities'], index=cluster_prob['clusters'], columns=cluster_prob['scenarios'])\n",
    "cluster_release_probabilities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the exceedance probability of the MIH at each POI and extract MIH values for specific percentiles\n",
    "# Sort MIH_all for each POI (column-wise)\n",
    "MIH_all_sorted = np.sort(MIH_all, axis=0)\n",
    "\n",
    "# Define thresholds and percentiles for exceedance probability calculation\n",
    "thresholds = np.linspace(0, 6, 100)  # e.g., 0 to 6 m\n",
    "percentiles1 = np.array([0.01, 0.05, 0.15, 0.85, 0.95, 0.99, 1])\n",
    "percentiles = 1-percentiles1\n",
    "\n",
    "npois = MIH_all.shape[1]\n",
    "MIH_perc_weighted = np.zeros((npois, len(percentiles)))\n",
    "MIH_perc_equal = np.zeros((npois, len(percentiles)))\n",
    "for poi in range(0,npois):\n",
    "    \n",
    "    ## Same weight\n",
    "    cdf = 1 - np.arange(1, len(MIH_all_sorted[:,poi])+1) / len(MIH_all_sorted[:,poi])\n",
    "    inverse_cdf = interp1d(cdf, MIH_all_sorted[:,poi], kind='nearest', bounds_error=False, fill_value=\"extrapolate\")\n",
    "    perc_v = inverse_cdf(percentiles)\n",
    "    # Get MIH for specific percentiles\n",
    "    MIH_perc_equal[poi,:] = np.array([inverse_cdf(p) for p in percentiles])\n",
    "    \n",
    "    ## Weighted by cluster release probabilities\n",
    "    exceedance_probs = exceedance_probability(cluster_release_probabilities.values, MIH_all[:,poi], thresholds, ptf_prob)\n",
    "    # Get MIH for specific percentiles\n",
    "    for i,p in enumerate(percentiles):\n",
    "        if len(thresholds[exceedance_probs>p])>0:\n",
    "            MIH_perc_weighted[poi,i] = thresholds[exceedance_probs>p][-1]\n",
    "        else:\n",
    "            MIH_perc_weighted[poi,i] = 0.\n",
    "\n",
    "print(f\"Extracted MIH values for percentiles {percentiles1} considering probabilities ('weighted') and without probabilities ('equal')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd28bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save results to output files\n",
    "file_MIH_percentiles_equal = os.path.join(output_folder, \"MIH_percentiles_equal.txt\")\n",
    "file_MIH_percentiles_weighted = os.path.join(output_folder, \"MIH_percentiles_weighted.txt\")\n",
    "np.savetxt(file_MIH_percentiles_equal, MIH_perc_equal, fmt='%.1f', header=str(percentiles), comments='')\n",
    "np.savetxt(file_MIH_percentiles_weighted, MIH_perc_weighted, fmt='%.1f', header=str(percentiles), comments='')\n",
    "print(f\"Saved results in {file_MIH_percentiles_weighted} and {file_MIH_percentiles_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a613e",
   "metadata": {},
   "source": [
    "**Plot results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load necessary file for plotting\n",
    "# Load POIs coordinates (POIs used in the precomputed database of landslide-tsunami simulations)\n",
    "pois_file = os.path.join(database_folder,\"messina_tms_HySEA.txt\")\n",
    "pois_coords = np.loadtxt(pois_file, skiprows=1)\n",
    "\n",
    "# Load runup data (data from https://www.ngdc.noaa.gov/hazel/view/hazards/tsunami/related-runups/1373)\n",
    "file_runup_data = os.path.join(database_folder,\"messina_runups_data.npz\")\n",
    "runup_data = np.load(file_runup_data)\n",
    "\n",
    "# Load MIH values for specific percentiles\n",
    "file_MIH_percentiles_equal = os.path.join(output_folder, \"MIH_percentiles_equal.txt\")\n",
    "file_MIH_percentiles_weighted = os.path.join(output_folder, \"MIH_percentiles_weighted.txt\")\n",
    "MIH_perc_weighted = np.loadtxt(file_MIH_percentiles_weighted, skiprows=1)\n",
    "MIH_perc_equal = np.loadtxt(file_MIH_percentiles_equal, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d54cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot results\n",
    "pois_to_plot = np.arange(50, 100, 1) # POIs 50 to 100 are along the East coast of Sicily\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "plot_mih_percentiles(pois_coords, runup_data, MIH_perc_equal, pois_to_plot, ax, 'Equal weight')\n",
    "plot_mih_percentiles(pois_coords, runup_data, MIH_perc_weighted, pois_to_plot, ax2,'Weighted by cluster release probabilities')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4696867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
